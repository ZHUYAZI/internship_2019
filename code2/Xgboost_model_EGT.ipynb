{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package xgboost:\n",
      "\n",
      "NAME\n",
      "    xgboost - XGBoost: eXtreme Gradient Boosting library.\n",
      "\n",
      "DESCRIPTION\n",
      "    Contributors: https://github.com/dmlc/xgboost/blob/master/CONTRIBUTORS.md\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    callback\n",
      "    compat\n",
      "    core\n",
      "    libpath\n",
      "    plotting\n",
      "    rabit\n",
      "    sklearn\n",
      "    training\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        xgboost.core.Booster\n",
      "        xgboost.core.DMatrix\n",
      "    sklearn.base.BaseEstimator(builtins.object)\n",
      "        xgboost.sklearn.XGBModel\n",
      "            xgboost.sklearn.XGBClassifier(xgboost.sklearn.XGBModel, sklearn.base.ClassifierMixin)\n",
      "                xgboost.sklearn.XGBRFClassifier\n",
      "            xgboost.sklearn.XGBRanker\n",
      "            xgboost.sklearn.XGBRegressor(xgboost.sklearn.XGBModel, sklearn.base.RegressorMixin)\n",
      "                xgboost.sklearn.XGBRFRegressor\n",
      "    \n",
      "    class Booster(builtins.object)\n",
      "     |  Booster(params=None, cache=(), model_file=None)\n",
      "     |  \n",
      "     |  A Booster of XGBoost.\n",
      "     |  \n",
      "     |  Booster is the model of xgboost, that contains low level routines for\n",
      "     |  training, prediction and evaluation.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __copy__(self)\n",
      "     |  \n",
      "     |  __deepcopy__(self, _)\n",
      "     |  \n",
      "     |  __del__(self)\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __init__(self, params=None, cache=(), model_file=None)\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      params : dict\n",
      "     |          Parameters for boosters.\n",
      "     |      cache : list\n",
      "     |          List of cache items.\n",
      "     |      model_file : string\n",
      "     |          Path to the model file.\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  attr(self, key)\n",
      "     |      Get attribute string from the Booster.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      key : str\n",
      "     |          The key to get attribute from.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      value : str\n",
      "     |          The attribute value of the key, returns None if attribute do not exist.\n",
      "     |  \n",
      "     |  attributes(self)\n",
      "     |      Get attributes stored in the Booster as a dictionary.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      result : dictionary of  attribute_name: attribute_value pairs of strings.\n",
      "     |          Returns an empty dict if there's no attributes.\n",
      "     |  \n",
      "     |  boost(self, dtrain, grad, hess)\n",
      "     |      Boost the booster for one iteration, with customized gradient\n",
      "     |      statistics.  Like :func:`xgboost.core.Booster.update`, this\n",
      "     |      function should not be called directly by users.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      dtrain : DMatrix\n",
      "     |          The training DMatrix.\n",
      "     |      grad : list\n",
      "     |          The first order of gradient.\n",
      "     |      hess : list\n",
      "     |          The second order of gradient.\n",
      "     |  \n",
      "     |  copy(self)\n",
      "     |      Copy the booster object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      booster: `Booster`\n",
      "     |          a copied booster model\n",
      "     |  \n",
      "     |  dump_model(self, fout, fmap='', with_stats=False, dump_format='text')\n",
      "     |      Dump model into a text or JSON file.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fout : string\n",
      "     |          Output file name.\n",
      "     |      fmap : string, optional\n",
      "     |          Name of the file containing feature map names.\n",
      "     |      with_stats : bool, optional\n",
      "     |          Controls whether the split statistics are output.\n",
      "     |      dump_format : string, optional\n",
      "     |          Format of model dump file. Can be 'text' or 'json'.\n",
      "     |  \n",
      "     |  eval(self, data, name='eval', iteration=0)\n",
      "     |      Evaluate the model on mat.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : DMatrix\n",
      "     |          The dmatrix storing the input.\n",
      "     |      \n",
      "     |      name : str, optional\n",
      "     |          The name of the dataset.\n",
      "     |      \n",
      "     |      iteration : int, optional\n",
      "     |          The current iteration number.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      result: str\n",
      "     |          Evaluation result string.\n",
      "     |  \n",
      "     |  eval_set(self, evals, iteration=0, feval=None)\n",
      "     |      Evaluate a set of data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      evals : list of tuples (DMatrix, string)\n",
      "     |          List of items to be evaluated.\n",
      "     |      iteration : int\n",
      "     |          Current iteration.\n",
      "     |      feval : function\n",
      "     |          Custom evaluation function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      result: str\n",
      "     |          Evaluation result string.\n",
      "     |  \n",
      "     |  get_dump(self, fmap='', with_stats=False, dump_format='text')\n",
      "     |      Returns the model dump as a list of strings.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fmap : string, optional\n",
      "     |          Name of the file containing feature map names.\n",
      "     |      with_stats : bool, optional\n",
      "     |          Controls whether the split statistics are output.\n",
      "     |      dump_format : string, optional\n",
      "     |          Format of model dump. Can be 'text' or 'json'.\n",
      "     |  \n",
      "     |  get_fscore(self, fmap='')\n",
      "     |      Get feature importance of each feature.\n",
      "     |      \n",
      "     |      .. note:: Feature importance is defined only for tree boosters\n",
      "     |      \n",
      "     |          Feature importance is only defined when the decision tree model is chosen as base\n",
      "     |          learner (`booster=gbtree`). It is not defined for other base learner types, such\n",
      "     |          as linear learners (`booster=gblinear`).\n",
      "     |      \n",
      "     |      .. note:: Zero-importance features will not be included\n",
      "     |      \n",
      "     |         Keep in mind that this function does not include zero-importance feature, i.e.\n",
      "     |         those features that have not been used in any split conditions.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fmap: str (optional)\n",
      "     |         The name of feature map file\n",
      "     |  \n",
      "     |  get_score(self, fmap='', importance_type='weight')\n",
      "     |      Get feature importance of each feature.\n",
      "     |      Importance type can be defined as:\n",
      "     |      \n",
      "     |      * 'weight': the number of times a feature is used to split the data across all trees.\n",
      "     |      * 'gain': the average gain across all splits the feature is used in.\n",
      "     |      * 'cover': the average coverage across all splits the feature is used in.\n",
      "     |      * 'total_gain': the total gain across all splits the feature is used in.\n",
      "     |      * 'total_cover': the total coverage across all splits the feature is used in.\n",
      "     |      \n",
      "     |      .. note:: Feature importance is defined only for tree boosters\n",
      "     |      \n",
      "     |          Feature importance is only defined when the decision tree model is chosen as base\n",
      "     |          learner (`booster=gbtree`). It is not defined for other base learner types, such\n",
      "     |          as linear learners (`booster=gblinear`).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fmap: str (optional)\n",
      "     |         The name of feature map file.\n",
      "     |      importance_type: str, default 'weight'\n",
      "     |          One of the importance types defined above.\n",
      "     |  \n",
      "     |  get_split_value_histogram(self, feature, fmap='', bins=None, as_pandas=True)\n",
      "     |      Get split value histogram of a feature\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      feature: str\n",
      "     |          The name of the feature.\n",
      "     |      fmap: str (optional)\n",
      "     |          The name of feature map file.\n",
      "     |      bin: int, default None\n",
      "     |          The maximum number of bins.\n",
      "     |          Number of bins equals number of unique split values n_unique,\n",
      "     |          if bins == None or bins > n_unique.\n",
      "     |      as_pandas: bool, default True\n",
      "     |          Return pd.DataFrame when pandas is installed.\n",
      "     |          If False or pandas is not installed, return numpy ndarray.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      a histogram of used splitting values for the specified feature\n",
      "     |      either as numpy array or pandas DataFrame.\n",
      "     |  \n",
      "     |  load_model(self, fname)\n",
      "     |      Load the model from a file.\n",
      "     |      \n",
      "     |      The model is loaded from an XGBoost internal binary format which is\n",
      "     |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      "     |      the Python Booster object (such as feature_names) will not be loaded.\n",
      "     |      To preserve all attributes, pickle the Booster object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string or a memory buffer\n",
      "     |          Input file name or memory buffer(see also save_raw)\n",
      "     |  \n",
      "     |  load_rabit_checkpoint(self)\n",
      "     |      Initialize the model by load from rabit checkpoint.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      version: integer\n",
      "     |          The version number of the model.\n",
      "     |  \n",
      "     |  predict(self, data, output_margin=False, ntree_limit=0, pred_leaf=False, pred_contribs=False, approx_contribs=False, pred_interactions=False, validate_features=True)\n",
      "     |      Predict with data.\n",
      "     |      \n",
      "     |      .. note:: This function is not thread safe.\n",
      "     |      \n",
      "     |        For each booster object, predict can only be called from one thread.\n",
      "     |        If you want to run prediction using multiple thread, call ``bst.copy()`` to make copies\n",
      "     |        of model object and then call ``predict()``.\n",
      "     |      \n",
      "     |      .. note:: Using ``predict()`` with DART booster\n",
      "     |      \n",
      "     |        If the booster object is DART type, ``predict()`` will perform dropouts, i.e. only\n",
      "     |        some of the trees will be evaluated. This will produce incorrect results if ``data`` is\n",
      "     |        not the training data. To obtain correct results on test sets, set ``ntree_limit`` to\n",
      "     |        a nonzero value, e.g.\n",
      "     |      \n",
      "     |        .. code-block:: python\n",
      "     |      \n",
      "     |          preds = bst.predict(dtest, ntree_limit=num_round)\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : DMatrix\n",
      "     |          The dmatrix storing the input.\n",
      "     |      \n",
      "     |      output_margin : bool\n",
      "     |          Whether to output the raw untransformed margin value.\n",
      "     |      \n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to 0 (use all trees).\n",
      "     |      \n",
      "     |      pred_leaf : bool\n",
      "     |          When this option is on, the output will be a matrix of (nsample, ntrees)\n",
      "     |          with each record indicating the predicted leaf index of each sample in each tree.\n",
      "     |          Note that the leaf index of a tree is unique per tree, so you may find leaf 1\n",
      "     |          in both tree 1 and tree 0.\n",
      "     |      \n",
      "     |      pred_contribs : bool\n",
      "     |          When this is True the output will be a matrix of size (nsample, nfeats + 1)\n",
      "     |          with each record indicating the feature contributions (SHAP values) for that\n",
      "     |          prediction. The sum of all feature contributions is equal to the raw untransformed\n",
      "     |          margin value of the prediction. Note the final column is the bias term.\n",
      "     |      \n",
      "     |      approx_contribs : bool\n",
      "     |          Approximate the contributions of each feature\n",
      "     |      \n",
      "     |      pred_interactions : bool\n",
      "     |          When this is True the output will be a matrix of size (nsample, nfeats + 1, nfeats + 1)\n",
      "     |          indicating the SHAP interaction values for each pair of features. The sum of each\n",
      "     |          row (or column) of the interaction values equals the corresponding SHAP value (from\n",
      "     |          pred_contribs), and the sum of the entire matrix equals the raw untransformed margin\n",
      "     |          value of the prediction. Note the last row and column correspond to the bias term.\n",
      "     |      \n",
      "     |      validate_features : bool\n",
      "     |          When this is True, validate that the Booster's and data's feature_names are identical.\n",
      "     |          Otherwise, it is assumed that the feature_names are the same.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      prediction : numpy array\n",
      "     |  \n",
      "     |  save_model(self, fname)\n",
      "     |      Save the model to a file.\n",
      "     |      \n",
      "     |      The model is saved in an XGBoost internal binary format which is\n",
      "     |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      "     |      the Python Booster object (such as feature_names) will not be saved.\n",
      "     |      To preserve all attributes, pickle the Booster object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string\n",
      "     |          Output file name\n",
      "     |  \n",
      "     |  save_rabit_checkpoint(self)\n",
      "     |      Save the current booster to rabit checkpoint.\n",
      "     |  \n",
      "     |  save_raw(self)\n",
      "     |      Save the model to a in memory buffer representation\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      a in memory buffer representation of the model\n",
      "     |  \n",
      "     |  set_attr(self, **kwargs)\n",
      "     |      Set the attribute of the Booster.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **kwargs\n",
      "     |          The attributes to set. Setting a value to None deletes an attribute.\n",
      "     |  \n",
      "     |  set_param(self, params, value=None)\n",
      "     |      Set parameters into the Booster.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      params: dict/list/str\n",
      "     |         list of key,value pairs, dict of key to value or simply str key\n",
      "     |      value: optional\n",
      "     |         value of the specified parameter, when params is str key\n",
      "     |  \n",
      "     |  trees_to_dataframe(self, fmap='')\n",
      "     |      Parse a boosted tree model text dump into a pandas DataFrame structure.\n",
      "     |      \n",
      "     |      This feature is only defined when the decision tree model is chosen as base\n",
      "     |      learner (`booster in {gbtree, dart}`). It is not defined for other base learner\n",
      "     |      types, such as linear learners (`booster=gblinear`).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fmap: str (optional)\n",
      "     |         The name of feature map file.\n",
      "     |  \n",
      "     |  update(self, dtrain, iteration, fobj=None)\n",
      "     |      Update for one iteration, with objective function calculated\n",
      "     |      internally.  This function should not be called directly by users.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      dtrain : DMatrix\n",
      "     |          Training data.\n",
      "     |      iteration : int\n",
      "     |          Current iteration number.\n",
      "     |      fobj : function\n",
      "     |          Customized objective function.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  feature_names = None\n",
      "    \n",
      "    class DMatrix(builtins.object)\n",
      "     |  DMatrix(data, label=None, missing=None, weight=None, silent=False, feature_names=None, feature_types=None, nthread=None)\n",
      "     |  \n",
      "     |  Data Matrix used in XGBoost.\n",
      "     |  \n",
      "     |  DMatrix is a internal data structure that used by XGBoost\n",
      "     |  which is optimized for both memory efficiency and training speed.\n",
      "     |  You can construct DMatrix from numpy.arrays\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __del__(self)\n",
      "     |  \n",
      "     |  __init__(self, data, label=None, missing=None, weight=None, silent=False, feature_names=None, feature_types=None, nthread=None)\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : string/numpy.array/scipy.sparse/pd.DataFrame/dt.Frame\n",
      "     |          Data source of DMatrix.\n",
      "     |          When data is string type, it represents the path libsvm format txt file,\n",
      "     |          or binary file that xgboost can read from.\n",
      "     |      label : list or numpy 1-D array, optional\n",
      "     |          Label of the training data.\n",
      "     |      missing : float, optional\n",
      "     |          Value in the data which needs to be present as a missing value. If\n",
      "     |          None, defaults to np.nan.\n",
      "     |      weight : list or numpy 1-D array , optional\n",
      "     |          Weight for each instance.\n",
      "     |      \n",
      "     |          .. note:: For ranking task, weights are per-group.\n",
      "     |      \n",
      "     |              In ranking task, one weight is assigned to each group (not each data\n",
      "     |              point). This is because we only care about the relative ordering of\n",
      "     |              data points within each group, so it doesn't make sense to assign\n",
      "     |              weights to individual data points.\n",
      "     |      \n",
      "     |      silent : boolean, optional\n",
      "     |          Whether print messages during construction\n",
      "     |      feature_names : list, optional\n",
      "     |          Set names for features.\n",
      "     |      feature_types : list, optional\n",
      "     |          Set types for features.\n",
      "     |      nthread : integer, optional\n",
      "     |          Number of threads to use for loading data from numpy array. If -1,\n",
      "     |          uses maximum threads available on the system.\n",
      "     |  \n",
      "     |  get_base_margin(self)\n",
      "     |      Get the base margin of the DMatrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      base_margin : float\n",
      "     |  \n",
      "     |  get_float_info(self, field)\n",
      "     |      Get float property from the DMatrix.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      field: str\n",
      "     |          The field name of the information\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      info : array\n",
      "     |          a numpy array of float information of the data\n",
      "     |  \n",
      "     |  get_label(self)\n",
      "     |      Get the label of the DMatrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      label : array\n",
      "     |  \n",
      "     |  get_uint_info(self, field)\n",
      "     |      Get unsigned integer property from the DMatrix.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      field: str\n",
      "     |          The field name of the information\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      info : array\n",
      "     |          a numpy array of unsigned integer information of the data\n",
      "     |  \n",
      "     |  get_weight(self)\n",
      "     |      Get the weight of the DMatrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      weight : array\n",
      "     |  \n",
      "     |  num_col(self)\n",
      "     |      Get the number of columns (features) in the DMatrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      number of columns : int\n",
      "     |  \n",
      "     |  num_row(self)\n",
      "     |      Get the number of rows in the DMatrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      number of rows : int\n",
      "     |  \n",
      "     |  save_binary(self, fname, silent=True)\n",
      "     |      Save DMatrix to an XGBoost buffer.  Saved binary can be later loaded\n",
      "     |      by providing the path to :py:func:`xgboost.DMatrix` as input.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string\n",
      "     |          Name of the output buffer file.\n",
      "     |      silent : bool (optional; default: True)\n",
      "     |          If set, the output is suppressed.\n",
      "     |  \n",
      "     |  set_base_margin(self, margin)\n",
      "     |      Set base margin of booster to start from.\n",
      "     |      \n",
      "     |      This can be used to specify a prediction value of\n",
      "     |      existing model to be base_margin\n",
      "     |      However, remember margin is needed, instead of transformed prediction\n",
      "     |      e.g. for logistic regression: need to put in value before logistic transformation\n",
      "     |      see also example/demo.py\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      margin: array like\n",
      "     |          Prediction margin of each datapoint\n",
      "     |  \n",
      "     |  set_float_info(self, field, data)\n",
      "     |      Set float type property into the DMatrix.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      field: str\n",
      "     |          The field name of the information\n",
      "     |      \n",
      "     |      data: numpy array\n",
      "     |          The array of data to be set\n",
      "     |  \n",
      "     |  set_float_info_npy2d(self, field, data)\n",
      "     |      Set float type property into the DMatrix\n",
      "     |         for numpy 2d array input\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      field: str\n",
      "     |          The field name of the information\n",
      "     |      \n",
      "     |      data: numpy array\n",
      "     |          The array of data to be set\n",
      "     |  \n",
      "     |  set_group(self, group)\n",
      "     |      Set group size of DMatrix (used for ranking).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      group : array like\n",
      "     |          Group size of each group\n",
      "     |  \n",
      "     |  set_label(self, label)\n",
      "     |      Set label of dmatrix\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      label: array like\n",
      "     |          The label information to be set into DMatrix\n",
      "     |  \n",
      "     |  set_label_npy2d(self, label)\n",
      "     |      Set label of dmatrix\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      label: array like\n",
      "     |          The label information to be set into DMatrix\n",
      "     |          from numpy 2D array\n",
      "     |  \n",
      "     |  set_uint_info(self, field, data)\n",
      "     |      Set uint type property into the DMatrix.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      field: str\n",
      "     |          The field name of the information\n",
      "     |      \n",
      "     |      data: numpy array\n",
      "     |          The array of data to be set\n",
      "     |  \n",
      "     |  set_weight(self, weight)\n",
      "     |      Set weight of each instance.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      weight : array like\n",
      "     |          Weight for each data point\n",
      "     |      \n",
      "     |          .. note:: For ranking task, weights are per-group.\n",
      "     |      \n",
      "     |              In ranking task, one weight is assigned to each group (not each data\n",
      "     |              point). This is because we only care about the relative ordering of\n",
      "     |              data points within each group, so it doesn't make sense to assign\n",
      "     |              weights to individual data points.\n",
      "     |  \n",
      "     |  set_weight_npy2d(self, weight)\n",
      "     |      Set weight of each instance\n",
      "     |          for numpy 2D array\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      weight : array like\n",
      "     |          Weight for each data point in numpy 2D array\n",
      "     |      \n",
      "     |          .. note:: For ranking task, weights are per-group.\n",
      "     |      \n",
      "     |              In ranking task, one weight is assigned to each group (not each data\n",
      "     |              point). This is because we only care about the relative ordering of\n",
      "     |              data points within each group, so it doesn't make sense to assign\n",
      "     |              weights to individual data points.\n",
      "     |  \n",
      "     |  slice(self, rindex)\n",
      "     |      Slice the DMatrix and return a new DMatrix that only contains `rindex`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      rindex : list\n",
      "     |          List of indices to be selected.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      res : DMatrix\n",
      "     |          A new DMatrix containing only selected indices.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  feature_names\n",
      "     |      Get feature names (column labels).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_names : list or None\n",
      "     |  \n",
      "     |  feature_types\n",
      "     |      Get feature types (column types).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_types : list or None\n",
      "    \n",
      "    class XGBClassifier(XGBModel, sklearn.base.ClassifierMixin)\n",
      "     |  XGBClassifier(max_depth=3, learning_rate=0.1, n_estimators=100, verbosity=1, silent=None, objective='binary:logistic', booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, **kwargs)\n",
      "     |  \n",
      "     |  Implementation of the scikit-learn API for XGBoost classification.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  max_depth : int\n",
      "     |      Maximum tree depth for base learners.\n",
      "     |  learning_rate : float\n",
      "     |      Boosting learning rate (xgb's \"eta\")\n",
      "     |  n_estimators : int\n",
      "     |      Number of trees to fit.\n",
      "     |  verbosity : int\n",
      "     |      The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
      "     |  silent : boolean\n",
      "     |      Whether to print messages while running boosting. Deprecated. Use verbosity instead.\n",
      "     |  objective : string or callable\n",
      "     |      Specify the learning task and the corresponding learning objective or\n",
      "     |      a custom objective function to be used (see note below).\n",
      "     |  booster: string\n",
      "     |      Specify which booster to use: gbtree, gblinear or dart.\n",
      "     |  nthread : int\n",
      "     |      Number of parallel threads used to run xgboost.  (Deprecated, please use ``n_jobs``)\n",
      "     |  n_jobs : int\n",
      "     |      Number of parallel threads used to run xgboost.  (replaces ``nthread``)\n",
      "     |  gamma : float\n",
      "     |      Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
      "     |  min_child_weight : int\n",
      "     |      Minimum sum of instance weight(hessian) needed in a child.\n",
      "     |  max_delta_step : int\n",
      "     |      Maximum delta step we allow each tree's weight estimation to be.\n",
      "     |  subsample : float\n",
      "     |      Subsample ratio of the training instance.\n",
      "     |  colsample_bytree : float\n",
      "     |      Subsample ratio of columns when constructing each tree.\n",
      "     |  colsample_bylevel : float\n",
      "     |      Subsample ratio of columns for each level.\n",
      "     |  colsample_bynode : float\n",
      "     |      Subsample ratio of columns for each split.\n",
      "     |  reg_alpha : float (xgb's alpha)\n",
      "     |      L1 regularization term on weights\n",
      "     |  reg_lambda : float (xgb's lambda)\n",
      "     |      L2 regularization term on weights\n",
      "     |  scale_pos_weight : float\n",
      "     |      Balancing of positive and negative weights.\n",
      "     |  base_score:\n",
      "     |      The initial prediction score of all instances, global bias.\n",
      "     |  seed : int\n",
      "     |      Random number seed.  (Deprecated, please use random_state)\n",
      "     |  random_state : int\n",
      "     |      Random number seed.  (replaces seed)\n",
      "     |  missing : float, optional\n",
      "     |      Value in the data which needs to be present as a missing value. If\n",
      "     |      None, defaults to np.nan.\n",
      "     |  importance_type: string, default \"gain\"\n",
      "     |      The feature importance type for the feature_importances_ property: either \"gain\",\n",
      "     |      \"weight\", \"cover\", \"total_gain\" or \"total_cover\".\n",
      "     |  \\*\\*kwargs : dict, optional\n",
      "     |      Keyword arguments for XGBoost Booster object.  Full documentation of parameters can\n",
      "     |      be found here: https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
      "     |      Attempting to set a parameter via the constructor args and \\*\\*kwargs dict simultaneously\n",
      "     |      will result in a TypeError.\n",
      "     |  \n",
      "     |      .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      "     |  \n",
      "     |          \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee that parameters\n",
      "     |          passed via this argument will interact properly with scikit-learn.\n",
      "     |  \n",
      "     |  Note\n",
      "     |  ----\n",
      "     |  A custom objective function can be provided for the ``objective``\n",
      "     |  parameter. In this case, it should have the signature\n",
      "     |  ``objective(y_true, y_pred) -> grad, hess``:\n",
      "     |  \n",
      "     |  y_true: array_like of shape [n_samples]\n",
      "     |      The target values\n",
      "     |  y_pred: array_like of shape [n_samples]\n",
      "     |      The predicted values\n",
      "     |  \n",
      "     |  grad: array_like of shape [n_samples]\n",
      "     |      The value of the gradient for each sample point.\n",
      "     |  hess: array_like of shape [n_samples]\n",
      "     |      The value of the second derivative for each sample point\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      XGBClassifier\n",
      "     |      XGBModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, max_depth=3, learning_rate=0.1, n_estimators=100, verbosity=1, silent=None, objective='binary:logistic', booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  evals_result(self)\n",
      "     |      Return the evaluation results.\n",
      "     |      \n",
      "     |      If **eval_set** is passed to the `fit` function, you can call\n",
      "     |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.\n",
      "     |      When **eval_metric** is also passed to the `fit` function, the\n",
      "     |      **evals_result** will contain the **eval_metrics** passed to the `fit` function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      evals_result : dictionary\n",
      "     |      \n",
      "     |      Example\n",
      "     |      -------\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n",
      "     |      \n",
      "     |          clf = xgb.XGBClassifier(**param_dist)\n",
      "     |      \n",
      "     |          clf.fit(X_train, y_train,\n",
      "     |                  eval_set=[(X_train, y_train), (X_test, y_test)],\n",
      "     |                  eval_metric='logloss',\n",
      "     |                  verbose=True)\n",
      "     |      \n",
      "     |          evals_result = clf.evals_result()\n",
      "     |      \n",
      "     |      The variable **evals_result** will contain\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      "     |          'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None, sample_weight_eval_set=None, callbacks=None)\n",
      "     |      Fit gradient boosting classifier\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like\n",
      "     |          Feature matrix\n",
      "     |      y : array_like\n",
      "     |          Labels\n",
      "     |      sample_weight : array_like\n",
      "     |          Weight for each instance\n",
      "     |      eval_set : list, optional\n",
      "     |          A list of (X, y) pairs to use as a validation set for\n",
      "     |          early-stopping\n",
      "     |      sample_weight_eval_set : list, optional\n",
      "     |          A list of the form [L_1, L_2, ..., L_n], where each L_i is a list of\n",
      "     |          instance weights on the i-th validation set.\n",
      "     |      eval_metric : str, callable, optional\n",
      "     |          If a str, should be a built-in evaluation metric to use. See\n",
      "     |          doc/parameter.rst. If callable, a custom evaluation metric. The call\n",
      "     |          signature is func(y_predicted, y_true) where y_true will be a\n",
      "     |          DMatrix object such that you may need to call the get_label\n",
      "     |          method. It must return a str, value pair where the str is a name\n",
      "     |          for the evaluation and value is the value of the evaluation\n",
      "     |          function. This objective is always minimized.\n",
      "     |      early_stopping_rounds : int, optional\n",
      "     |          Activates early stopping. Validation error needs to decrease at\n",
      "     |          least every <early_stopping_rounds> round(s) to continue training.\n",
      "     |          Requires at least one item in evals. If there's more than one,\n",
      "     |          will use the last. If early stopping occurs, the model will have\n",
      "     |          three additional fields: bst.best_score, bst.best_iteration and\n",
      "     |          bst.best_ntree_limit (bst.best_ntree_limit is the ntree_limit parameter\n",
      "     |          default value in predict method if not any other value is specified).\n",
      "     |          (Use bst.best_ntree_limit to get the correct value if num_parallel_tree\n",
      "     |          and/or num_class appears in the parameters)\n",
      "     |      verbose : bool\n",
      "     |          If `verbose` and an evaluation set is used, writes the evaluation\n",
      "     |          metric measured on the validation set to stderr.\n",
      "     |      xgb_model : str\n",
      "     |          file name of stored xgb model or 'Booster' instance Xgb model to be\n",
      "     |          loaded before training (allows training continuation).\n",
      "     |      callbacks : list of callback functions\n",
      "     |          List of callback functions that are applied at end of each iteration.\n",
      "     |          It is possible to use predefined callbacks by using :ref:`callback_api`.\n",
      "     |          Example:\n",
      "     |      \n",
      "     |          .. code-block:: python\n",
      "     |      \n",
      "     |              [xgb.callback.reset_learning_rate(custom_rates)]\n",
      "     |  \n",
      "     |  predict(self, data, output_margin=False, ntree_limit=None, validate_features=True)\n",
      "     |      Predict with `data`.\n",
      "     |      \n",
      "     |      .. note:: This function is not thread safe.\n",
      "     |      \n",
      "     |        For each booster object, predict can only be called from one thread.\n",
      "     |        If you want to run prediction using multiple thread, call ``xgb.copy()`` to make copies\n",
      "     |        of model object and then call ``predict()``.\n",
      "     |      \n",
      "     |      .. note:: Using ``predict()`` with DART booster\n",
      "     |      \n",
      "     |        If the booster object is DART type, ``predict()`` will perform dropouts, i.e. only\n",
      "     |        some of the trees will be evaluated. This will produce incorrect results if ``data`` is\n",
      "     |        not the training data. To obtain correct results on test sets, set ``ntree_limit`` to\n",
      "     |        a nonzero value, e.g.\n",
      "     |      \n",
      "     |        .. code-block:: python\n",
      "     |      \n",
      "     |          preds = bst.predict(dtest, ntree_limit=num_round)\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : DMatrix\n",
      "     |          The dmatrix storing the input.\n",
      "     |      output_margin : bool\n",
      "     |          Whether to output the raw untransformed margin value.\n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to best_ntree_limit if defined\n",
      "     |          (i.e. it has been trained with early stopping), otherwise 0 (use all trees).\n",
      "     |      validate_features : bool\n",
      "     |          When this is True, validate that the Booster's and data's feature_names are identical.\n",
      "     |          Otherwise, it is assumed that the feature_names are the same.\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      prediction : numpy array\n",
      "     |  \n",
      "     |  predict_proba(self, data, ntree_limit=None, validate_features=True)\n",
      "     |      Predict the probability of each `data` example being of a given class.\n",
      "     |      \n",
      "     |      .. note:: This function is not thread safe\n",
      "     |      \n",
      "     |          For each booster object, predict can only be called from one thread.\n",
      "     |          If you want to run prediction using multiple thread, call ``xgb.copy()`` to make copies\n",
      "     |          of model object and then call predict\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : DMatrix\n",
      "     |          The dmatrix storing the input.\n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to best_ntree_limit if defined\n",
      "     |          (i.e. it has been trained with early stopping), otherwise 0 (use all trees).\n",
      "     |      validate_features : bool\n",
      "     |          When this is True, validate that the Booster's and data's feature_names are identical.\n",
      "     |          Otherwise, it is assumed that the feature_names are the same.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      prediction : numpy array\n",
      "     |          a numpy array with the probability of each data example being of a given class.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from XGBModel:\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  apply(self, X, ntree_limit=0)\n",
      "     |      Return the predicted leaf every tree for each sample.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like, shape=[n_samples, n_features]\n",
      "     |          Input features matrix.\n",
      "     |      \n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to 0 (use all trees).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      "     |          For each datapoint x in X and for each tree, return the index of the\n",
      "     |          leaf x ends up in. Leaves are numbered within\n",
      "     |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      "     |  \n",
      "     |  get_booster(self)\n",
      "     |      Get the underlying xgboost Booster of this model.\n",
      "     |      \n",
      "     |      This will raise an exception when fit was not called\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      booster : a xgboost booster of underlying model\n",
      "     |  \n",
      "     |  get_num_boosting_rounds(self)\n",
      "     |      Gets the number of xgboost boosting rounds.\n",
      "     |  \n",
      "     |  get_params(self, deep=False)\n",
      "     |      Get parameters.\n",
      "     |  \n",
      "     |  get_xgb_params(self)\n",
      "     |      Get xgboost type parameters.\n",
      "     |  \n",
      "     |  load_model(self, fname)\n",
      "     |      Load the model from a file.\n",
      "     |      \n",
      "     |      The model is loaded from an XGBoost internal binary format which is\n",
      "     |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      "     |      the Python Booster object (such as feature names) will not be loaded.\n",
      "     |      Label encodings (text labels to numeric labels) will be also lost.\n",
      "     |      **If you are using only the Python interface, we recommend pickling the\n",
      "     |      model object for best results.**\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string or a memory buffer\n",
      "     |          Input file name or memory buffer(see also save_raw)\n",
      "     |  \n",
      "     |  save_model(self, fname)\n",
      "     |      Save the model to a file.\n",
      "     |      \n",
      "     |      The model is saved in an XGBoost internal binary format which is\n",
      "     |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      "     |      the Python Booster object (such as feature names) will not be loaded.\n",
      "     |      Label encodings (text labels to numeric labels) will be also lost.\n",
      "     |      **If you are using only the Python interface, we recommend pickling the\n",
      "     |      model object for best results.**\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string\n",
      "     |          Output file name\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      Modification of the sklearn method to allow unknown kwargs. This allows using\n",
      "     |      the full range of xgboost parameters that are not defined as member variables\n",
      "     |      in sklearn grid search.\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from XGBModel:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |      Coefficients property\n",
      "     |      \n",
      "     |      .. note:: Coefficients are defined only for linear learners\n",
      "     |      \n",
      "     |          Coefficients are only defined when the linear model is chosen as base\n",
      "     |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      "     |          as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Feature importances property\n",
      "     |      \n",
      "     |      .. note:: Feature importance is defined only for tree boosters\n",
      "     |      \n",
      "     |          Feature importance is only defined when the decision tree model is chosen as base\n",
      "     |          learner (`booster=gbtree`). It is not defined for other base learner types, such\n",
      "     |          as linear learners (`booster=gblinear`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array of shape ``[n_features]``\n",
      "     |  \n",
      "     |  intercept_\n",
      "     |      Intercept (bias) property\n",
      "     |      \n",
      "     |      .. note:: Intercept is defined only for linear learners\n",
      "     |      \n",
      "     |          Intercept (bias) is only defined when the linear model is chosen as base\n",
      "     |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      "     |          as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "    \n",
      "    class XGBModel(sklearn.base.BaseEstimator)\n",
      "     |  XGBModel(max_depth=3, learning_rate=0.1, n_estimators=100, verbosity=1, silent=None, objective='reg:linear', booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, importance_type='gain', **kwargs)\n",
      "     |  \n",
      "     |  Implementation of the Scikit-Learn API for XGBoost.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  max_depth : int\n",
      "     |      Maximum tree depth for base learners.\n",
      "     |  learning_rate : float\n",
      "     |      Boosting learning rate (xgb's \"eta\")\n",
      "     |  n_estimators : int\n",
      "     |      Number of trees to fit.\n",
      "     |  verbosity : int\n",
      "     |      The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
      "     |  silent : boolean\n",
      "     |      Whether to print messages while running boosting. Deprecated. Use verbosity instead.\n",
      "     |  objective : string or callable\n",
      "     |      Specify the learning task and the corresponding learning objective or\n",
      "     |      a custom objective function to be used (see note below).\n",
      "     |  booster: string\n",
      "     |      Specify which booster to use: gbtree, gblinear or dart.\n",
      "     |  nthread : int\n",
      "     |      Number of parallel threads used to run xgboost.  (Deprecated, please use ``n_jobs``)\n",
      "     |  n_jobs : int\n",
      "     |      Number of parallel threads used to run xgboost.  (replaces ``nthread``)\n",
      "     |  gamma : float\n",
      "     |      Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
      "     |  min_child_weight : int\n",
      "     |      Minimum sum of instance weight(hessian) needed in a child.\n",
      "     |  max_delta_step : int\n",
      "     |      Maximum delta step we allow each tree's weight estimation to be.\n",
      "     |  subsample : float\n",
      "     |      Subsample ratio of the training instance.\n",
      "     |  colsample_bytree : float\n",
      "     |      Subsample ratio of columns when constructing each tree.\n",
      "     |  colsample_bylevel : float\n",
      "     |      Subsample ratio of columns for each level.\n",
      "     |  colsample_bynode : float\n",
      "     |      Subsample ratio of columns for each split.\n",
      "     |  reg_alpha : float (xgb's alpha)\n",
      "     |      L1 regularization term on weights\n",
      "     |  reg_lambda : float (xgb's lambda)\n",
      "     |      L2 regularization term on weights\n",
      "     |  scale_pos_weight : float\n",
      "     |      Balancing of positive and negative weights.\n",
      "     |  base_score:\n",
      "     |      The initial prediction score of all instances, global bias.\n",
      "     |  seed : int\n",
      "     |      Random number seed.  (Deprecated, please use random_state)\n",
      "     |  random_state : int\n",
      "     |      Random number seed.  (replaces seed)\n",
      "     |  missing : float, optional\n",
      "     |      Value in the data which needs to be present as a missing value. If\n",
      "     |      None, defaults to np.nan.\n",
      "     |  importance_type: string, default \"gain\"\n",
      "     |      The feature importance type for the feature_importances_ property: either \"gain\",\n",
      "     |      \"weight\", \"cover\", \"total_gain\" or \"total_cover\".\n",
      "     |  \\*\\*kwargs : dict, optional\n",
      "     |      Keyword arguments for XGBoost Booster object.  Full documentation of parameters can\n",
      "     |      be found here: https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
      "     |      Attempting to set a parameter via the constructor args and \\*\\*kwargs dict simultaneously\n",
      "     |      will result in a TypeError.\n",
      "     |  \n",
      "     |      .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      "     |  \n",
      "     |          \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee that parameters\n",
      "     |          passed via this argument will interact properly with scikit-learn.\n",
      "     |  \n",
      "     |  Note\n",
      "     |  ----\n",
      "     |  A custom objective function can be provided for the ``objective``\n",
      "     |  parameter. In this case, it should have the signature\n",
      "     |  ``objective(y_true, y_pred) -> grad, hess``:\n",
      "     |  \n",
      "     |  y_true: array_like of shape [n_samples]\n",
      "     |      The target values\n",
      "     |  y_pred: array_like of shape [n_samples]\n",
      "     |      The predicted values\n",
      "     |  \n",
      "     |  grad: array_like of shape [n_samples]\n",
      "     |      The value of the gradient for each sample point.\n",
      "     |  hess: array_like of shape [n_samples]\n",
      "     |      The value of the second derivative for each sample point\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      XGBModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, max_depth=3, learning_rate=0.1, n_estimators=100, verbosity=1, silent=None, objective='reg:linear', booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, importance_type='gain', **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  apply(self, X, ntree_limit=0)\n",
      "     |      Return the predicted leaf every tree for each sample.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like, shape=[n_samples, n_features]\n",
      "     |          Input features matrix.\n",
      "     |      \n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to 0 (use all trees).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      "     |          For each datapoint x in X and for each tree, return the index of the\n",
      "     |          leaf x ends up in. Leaves are numbered within\n",
      "     |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      "     |  \n",
      "     |  evals_result(self)\n",
      "     |      Return the evaluation results.\n",
      "     |      \n",
      "     |      If **eval_set** is passed to the `fit` function, you can call\n",
      "     |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.\n",
      "     |      When **eval_metric** is also passed to the `fit` function, the\n",
      "     |      **evals_result** will contain the **eval_metrics** passed to the `fit` function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      evals_result : dictionary\n",
      "     |      \n",
      "     |      Example\n",
      "     |      -------\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n",
      "     |      \n",
      "     |          clf = xgb.XGBModel(**param_dist)\n",
      "     |      \n",
      "     |          clf.fit(X_train, y_train,\n",
      "     |                  eval_set=[(X_train, y_train), (X_test, y_test)],\n",
      "     |                  eval_metric='logloss',\n",
      "     |                  verbose=True)\n",
      "     |      \n",
      "     |          evals_result = clf.evals_result()\n",
      "     |      \n",
      "     |      The variable **evals_result** will contain:\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      "     |          'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None, sample_weight_eval_set=None, callbacks=None)\n",
      "     |      Fit the gradient boosting model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like\n",
      "     |          Feature matrix\n",
      "     |      y : array_like\n",
      "     |          Labels\n",
      "     |      sample_weight : array_like\n",
      "     |          instance weights\n",
      "     |      eval_set : list, optional\n",
      "     |          A list of (X, y) tuple pairs to use as a validation set for\n",
      "     |          early-stopping\n",
      "     |      sample_weight_eval_set : list, optional\n",
      "     |          A list of the form [L_1, L_2, ..., L_n], where each L_i is a list of\n",
      "     |          instance weights on the i-th validation set.\n",
      "     |      eval_metric : str, callable, optional\n",
      "     |          If a str, should be a built-in evaluation metric to use. See\n",
      "     |          doc/parameter.rst. If callable, a custom evaluation metric. The call\n",
      "     |          signature is func(y_predicted, y_true) where y_true will be a\n",
      "     |          DMatrix object such that you may need to call the get_label\n",
      "     |          method. It must return a str, value pair where the str is a name\n",
      "     |          for the evaluation and value is the value of the evaluation\n",
      "     |          function. This objective is always minimized.\n",
      "     |      early_stopping_rounds : int\n",
      "     |          Activates early stopping. Validation error needs to decrease at\n",
      "     |          least every <early_stopping_rounds> round(s) to continue training.\n",
      "     |          Requires at least one item in evals.  If there's more than one,\n",
      "     |          will use the last. Returns the model from the last iteration\n",
      "     |          (not the best one). If early stopping occurs, the model will\n",
      "     |          have three additional fields: bst.best_score, bst.best_iteration\n",
      "     |          and bst.best_ntree_limit.\n",
      "     |          (Use bst.best_ntree_limit to get the correct value if num_parallel_tree\n",
      "     |          and/or num_class appears in the parameters)\n",
      "     |      verbose : bool\n",
      "     |          If `verbose` and an evaluation set is used, writes the evaluation\n",
      "     |          metric measured on the validation set to stderr.\n",
      "     |      xgb_model : str\n",
      "     |          file name of stored xgb model or 'Booster' instance Xgb model to be\n",
      "     |          loaded before training (allows training continuation).\n",
      "     |      callbacks : list of callback functions\n",
      "     |          List of callback functions that are applied at end of each iteration.\n",
      "     |          It is possible to use predefined callbacks by using :ref:`callback_api`.\n",
      "     |          Example:\n",
      "     |      \n",
      "     |          .. code-block:: python\n",
      "     |      \n",
      "     |              [xgb.callback.reset_learning_rate(custom_rates)]\n",
      "     |  \n",
      "     |  get_booster(self)\n",
      "     |      Get the underlying xgboost Booster of this model.\n",
      "     |      \n",
      "     |      This will raise an exception when fit was not called\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      booster : a xgboost booster of underlying model\n",
      "     |  \n",
      "     |  get_num_boosting_rounds(self)\n",
      "     |      Gets the number of xgboost boosting rounds.\n",
      "     |  \n",
      "     |  get_params(self, deep=False)\n",
      "     |      Get parameters.\n",
      "     |  \n",
      "     |  get_xgb_params(self)\n",
      "     |      Get xgboost type parameters.\n",
      "     |  \n",
      "     |  load_model(self, fname)\n",
      "     |      Load the model from a file.\n",
      "     |      \n",
      "     |      The model is loaded from an XGBoost internal binary format which is\n",
      "     |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      "     |      the Python Booster object (such as feature names) will not be loaded.\n",
      "     |      Label encodings (text labels to numeric labels) will be also lost.\n",
      "     |      **If you are using only the Python interface, we recommend pickling the\n",
      "     |      model object for best results.**\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string or a memory buffer\n",
      "     |          Input file name or memory buffer(see also save_raw)\n",
      "     |  \n",
      "     |  predict(self, data, output_margin=False, ntree_limit=None, validate_features=True)\n",
      "     |      Predict with `data`.\n",
      "     |      \n",
      "     |      .. note:: This function is not thread safe.\n",
      "     |      \n",
      "     |        For each booster object, predict can only be called from one thread.\n",
      "     |        If you want to run prediction using multiple thread, call ``xgb.copy()`` to make copies\n",
      "     |        of model object and then call ``predict()``.\n",
      "     |      \n",
      "     |      .. note:: Using ``predict()`` with DART booster\n",
      "     |      \n",
      "     |        If the booster object is DART type, ``predict()`` will perform dropouts, i.e. only\n",
      "     |        some of the trees will be evaluated. This will produce incorrect results if ``data`` is\n",
      "     |        not the training data. To obtain correct results on test sets, set ``ntree_limit`` to\n",
      "     |        a nonzero value, e.g.\n",
      "     |      \n",
      "     |        .. code-block:: python\n",
      "     |      \n",
      "     |          preds = bst.predict(dtest, ntree_limit=num_round)\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : DMatrix\n",
      "     |          The dmatrix storing the input.\n",
      "     |      output_margin : bool\n",
      "     |          Whether to output the raw untransformed margin value.\n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to best_ntree_limit if defined\n",
      "     |          (i.e. it has been trained with early stopping), otherwise 0 (use all trees).\n",
      "     |      validate_features : bool\n",
      "     |          When this is True, validate that the Booster's and data's feature_names are identical.\n",
      "     |          Otherwise, it is assumed that the feature_names are the same.\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      prediction : numpy array\n",
      "     |  \n",
      "     |  save_model(self, fname)\n",
      "     |      Save the model to a file.\n",
      "     |      \n",
      "     |      The model is saved in an XGBoost internal binary format which is\n",
      "     |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      "     |      the Python Booster object (such as feature names) will not be loaded.\n",
      "     |      Label encodings (text labels to numeric labels) will be also lost.\n",
      "     |      **If you are using only the Python interface, we recommend pickling the\n",
      "     |      model object for best results.**\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string\n",
      "     |          Output file name\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      Modification of the sklearn method to allow unknown kwargs. This allows using\n",
      "     |      the full range of xgboost parameters that are not defined as member variables\n",
      "     |      in sklearn grid search.\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |      Coefficients property\n",
      "     |      \n",
      "     |      .. note:: Coefficients are defined only for linear learners\n",
      "     |      \n",
      "     |          Coefficients are only defined when the linear model is chosen as base\n",
      "     |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      "     |          as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Feature importances property\n",
      "     |      \n",
      "     |      .. note:: Feature importance is defined only for tree boosters\n",
      "     |      \n",
      "     |          Feature importance is only defined when the decision tree model is chosen as base\n",
      "     |          learner (`booster=gbtree`). It is not defined for other base learner types, such\n",
      "     |          as linear learners (`booster=gblinear`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array of shape ``[n_features]``\n",
      "     |  \n",
      "     |  intercept_\n",
      "     |      Intercept (bias) property\n",
      "     |      \n",
      "     |      .. note:: Intercept is defined only for linear learners\n",
      "     |      \n",
      "     |          Intercept (bias) is only defined when the linear model is chosen as base\n",
      "     |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      "     |          as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class XGBRFClassifier(XGBClassifier)\n",
      "     |  XGBRFClassifier(max_depth=3, learning_rate=1, n_estimators=100, verbosity=1, silent=None, objective='binary:logistic', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=0.8, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=0.8, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, **kwargs)\n",
      "     |  \n",
      "     |  Experimental implementation of the scikit-learn API for XGBoost random forest classification.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  max_depth : int\n",
      "     |      Maximum tree depth for base learners.\n",
      "     |  learning_rate : float\n",
      "     |      Boosting learning rate (xgb's \"eta\")\n",
      "     |  n_estimators : int\n",
      "     |      Number of trees to fit.\n",
      "     |  verbosity : int\n",
      "     |      The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
      "     |  silent : boolean\n",
      "     |      Whether to print messages while running boosting. Deprecated. Use verbosity instead.\n",
      "     |  objective : string or callable\n",
      "     |      Specify the learning task and the corresponding learning objective or\n",
      "     |      a custom objective function to be used (see note below).\n",
      "     |  booster: string\n",
      "     |      Specify which booster to use: gbtree, gblinear or dart.\n",
      "     |  nthread : int\n",
      "     |      Number of parallel threads used to run xgboost.  (Deprecated, please use ``n_jobs``)\n",
      "     |  n_jobs : int\n",
      "     |      Number of parallel threads used to run xgboost.  (replaces ``nthread``)\n",
      "     |  gamma : float\n",
      "     |      Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
      "     |  min_child_weight : int\n",
      "     |      Minimum sum of instance weight(hessian) needed in a child.\n",
      "     |  max_delta_step : int\n",
      "     |      Maximum delta step we allow each tree's weight estimation to be.\n",
      "     |  subsample : float\n",
      "     |      Subsample ratio of the training instance.\n",
      "     |  colsample_bytree : float\n",
      "     |      Subsample ratio of columns when constructing each tree.\n",
      "     |  colsample_bylevel : float\n",
      "     |      Subsample ratio of columns for each level.\n",
      "     |  colsample_bynode : float\n",
      "     |      Subsample ratio of columns for each split.\n",
      "     |  reg_alpha : float (xgb's alpha)\n",
      "     |      L1 regularization term on weights\n",
      "     |  reg_lambda : float (xgb's lambda)\n",
      "     |      L2 regularization term on weights\n",
      "     |  scale_pos_weight : float\n",
      "     |      Balancing of positive and negative weights.\n",
      "     |  base_score:\n",
      "     |      The initial prediction score of all instances, global bias.\n",
      "     |  seed : int\n",
      "     |      Random number seed.  (Deprecated, please use random_state)\n",
      "     |  random_state : int\n",
      "     |      Random number seed.  (replaces seed)\n",
      "     |  missing : float, optional\n",
      "     |      Value in the data which needs to be present as a missing value. If\n",
      "     |      None, defaults to np.nan.\n",
      "     |  importance_type: string, default \"gain\"\n",
      "     |      The feature importance type for the feature_importances_ property: either \"gain\",\n",
      "     |      \"weight\", \"cover\", \"total_gain\" or \"total_cover\".\n",
      "     |  \\*\\*kwargs : dict, optional\n",
      "     |      Keyword arguments for XGBoost Booster object.  Full documentation of parameters can\n",
      "     |      be found here: https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
      "     |      Attempting to set a parameter via the constructor args and \\*\\*kwargs dict simultaneously\n",
      "     |      will result in a TypeError.\n",
      "     |  \n",
      "     |      .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      "     |  \n",
      "     |          \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee that parameters\n",
      "     |          passed via this argument will interact properly with scikit-learn.\n",
      "     |  \n",
      "     |  Note\n",
      "     |  ----\n",
      "     |  A custom objective function can be provided for the ``objective``\n",
      "     |  parameter. In this case, it should have the signature\n",
      "     |  ``objective(y_true, y_pred) -> grad, hess``:\n",
      "     |  \n",
      "     |  y_true: array_like of shape [n_samples]\n",
      "     |      The target values\n",
      "     |  y_pred: array_like of shape [n_samples]\n",
      "     |      The predicted values\n",
      "     |  \n",
      "     |  grad: array_like of shape [n_samples]\n",
      "     |      The value of the gradient for each sample point.\n",
      "     |  hess: array_like of shape [n_samples]\n",
      "     |      The value of the second derivative for each sample point\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      XGBRFClassifier\n",
      "     |      XGBClassifier\n",
      "     |      XGBModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, max_depth=3, learning_rate=1, n_estimators=100, verbosity=1, silent=None, objective='binary:logistic', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=0.8, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=0.8, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_num_boosting_rounds(self)\n",
      "     |      Gets the number of xgboost boosting rounds.\n",
      "     |  \n",
      "     |  get_xgb_params(self)\n",
      "     |      Get xgboost type parameters.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from XGBClassifier:\n",
      "     |  \n",
      "     |  evals_result(self)\n",
      "     |      Return the evaluation results.\n",
      "     |      \n",
      "     |      If **eval_set** is passed to the `fit` function, you can call\n",
      "     |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.\n",
      "     |      When **eval_metric** is also passed to the `fit` function, the\n",
      "     |      **evals_result** will contain the **eval_metrics** passed to the `fit` function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      evals_result : dictionary\n",
      "     |      \n",
      "     |      Example\n",
      "     |      -------\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n",
      "     |      \n",
      "     |          clf = xgb.XGBClassifier(**param_dist)\n",
      "     |      \n",
      "     |          clf.fit(X_train, y_train,\n",
      "     |                  eval_set=[(X_train, y_train), (X_test, y_test)],\n",
      "     |                  eval_metric='logloss',\n",
      "     |                  verbose=True)\n",
      "     |      \n",
      "     |          evals_result = clf.evals_result()\n",
      "     |      \n",
      "     |      The variable **evals_result** will contain\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      "     |          'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None, sample_weight_eval_set=None, callbacks=None)\n",
      "     |      Fit gradient boosting classifier\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like\n",
      "     |          Feature matrix\n",
      "     |      y : array_like\n",
      "     |          Labels\n",
      "     |      sample_weight : array_like\n",
      "     |          Weight for each instance\n",
      "     |      eval_set : list, optional\n",
      "     |          A list of (X, y) pairs to use as a validation set for\n",
      "     |          early-stopping\n",
      "     |      sample_weight_eval_set : list, optional\n",
      "     |          A list of the form [L_1, L_2, ..., L_n], where each L_i is a list of\n",
      "     |          instance weights on the i-th validation set.\n",
      "     |      eval_metric : str, callable, optional\n",
      "     |          If a str, should be a built-in evaluation metric to use. See\n",
      "     |          doc/parameter.rst. If callable, a custom evaluation metric. The call\n",
      "     |          signature is func(y_predicted, y_true) where y_true will be a\n",
      "     |          DMatrix object such that you may need to call the get_label\n",
      "     |          method. It must return a str, value pair where the str is a name\n",
      "     |          for the evaluation and value is the value of the evaluation\n",
      "     |          function. This objective is always minimized.\n",
      "     |      early_stopping_rounds : int, optional\n",
      "     |          Activates early stopping. Validation error needs to decrease at\n",
      "     |          least every <early_stopping_rounds> round(s) to continue training.\n",
      "     |          Requires at least one item in evals. If there's more than one,\n",
      "     |          will use the last. If early stopping occurs, the model will have\n",
      "     |          three additional fields: bst.best_score, bst.best_iteration and\n",
      "     |          bst.best_ntree_limit (bst.best_ntree_limit is the ntree_limit parameter\n",
      "     |          default value in predict method if not any other value is specified).\n",
      "     |          (Use bst.best_ntree_limit to get the correct value if num_parallel_tree\n",
      "     |          and/or num_class appears in the parameters)\n",
      "     |      verbose : bool\n",
      "     |          If `verbose` and an evaluation set is used, writes the evaluation\n",
      "     |          metric measured on the validation set to stderr.\n",
      "     |      xgb_model : str\n",
      "     |          file name of stored xgb model or 'Booster' instance Xgb model to be\n",
      "     |          loaded before training (allows training continuation).\n",
      "     |      callbacks : list of callback functions\n",
      "     |          List of callback functions that are applied at end of each iteration.\n",
      "     |          It is possible to use predefined callbacks by using :ref:`callback_api`.\n",
      "     |          Example:\n",
      "     |      \n",
      "     |          .. code-block:: python\n",
      "     |      \n",
      "     |              [xgb.callback.reset_learning_rate(custom_rates)]\n",
      "     |  \n",
      "     |  predict(self, data, output_margin=False, ntree_limit=None, validate_features=True)\n",
      "     |      Predict with `data`.\n",
      "     |      \n",
      "     |      .. note:: This function is not thread safe.\n",
      "     |      \n",
      "     |        For each booster object, predict can only be called from one thread.\n",
      "     |        If you want to run prediction using multiple thread, call ``xgb.copy()`` to make copies\n",
      "     |        of model object and then call ``predict()``.\n",
      "     |      \n",
      "     |      .. note:: Using ``predict()`` with DART booster\n",
      "     |      \n",
      "     |        If the booster object is DART type, ``predict()`` will perform dropouts, i.e. only\n",
      "     |        some of the trees will be evaluated. This will produce incorrect results if ``data`` is\n",
      "     |        not the training data. To obtain correct results on test sets, set ``ntree_limit`` to\n",
      "     |        a nonzero value, e.g.\n",
      "     |      \n",
      "     |        .. code-block:: python\n",
      "     |      \n",
      "     |          preds = bst.predict(dtest, ntree_limit=num_round)\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : DMatrix\n",
      "     |          The dmatrix storing the input.\n",
      "     |      output_margin : bool\n",
      "     |          Whether to output the raw untransformed margin value.\n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to best_ntree_limit if defined\n",
      "     |          (i.e. it has been trained with early stopping), otherwise 0 (use all trees).\n",
      "     |      validate_features : bool\n",
      "     |          When this is True, validate that the Booster's and data's feature_names are identical.\n",
      "     |          Otherwise, it is assumed that the feature_names are the same.\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      prediction : numpy array\n",
      "     |  \n",
      "     |  predict_proba(self, data, ntree_limit=None, validate_features=True)\n",
      "     |      Predict the probability of each `data` example being of a given class.\n",
      "     |      \n",
      "     |      .. note:: This function is not thread safe\n",
      "     |      \n",
      "     |          For each booster object, predict can only be called from one thread.\n",
      "     |          If you want to run prediction using multiple thread, call ``xgb.copy()`` to make copies\n",
      "     |          of model object and then call predict\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : DMatrix\n",
      "     |          The dmatrix storing the input.\n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to best_ntree_limit if defined\n",
      "     |          (i.e. it has been trained with early stopping), otherwise 0 (use all trees).\n",
      "     |      validate_features : bool\n",
      "     |          When this is True, validate that the Booster's and data's feature_names are identical.\n",
      "     |          Otherwise, it is assumed that the feature_names are the same.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      prediction : numpy array\n",
      "     |          a numpy array with the probability of each data example being of a given class.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from XGBModel:\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  apply(self, X, ntree_limit=0)\n",
      "     |      Return the predicted leaf every tree for each sample.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like, shape=[n_samples, n_features]\n",
      "     |          Input features matrix.\n",
      "     |      \n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to 0 (use all trees).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      "     |          For each datapoint x in X and for each tree, return the index of the\n",
      "     |          leaf x ends up in. Leaves are numbered within\n",
      "     |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      "     |  \n",
      "     |  get_booster(self)\n",
      "     |      Get the underlying xgboost Booster of this model.\n",
      "     |      \n",
      "     |      This will raise an exception when fit was not called\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      booster : a xgboost booster of underlying model\n",
      "     |  \n",
      "     |  get_params(self, deep=False)\n",
      "     |      Get parameters.\n",
      "     |  \n",
      "     |  load_model(self, fname)\n",
      "     |      Load the model from a file.\n",
      "     |      \n",
      "     |      The model is loaded from an XGBoost internal binary format which is\n",
      "     |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      "     |      the Python Booster object (such as feature names) will not be loaded.\n",
      "     |      Label encodings (text labels to numeric labels) will be also lost.\n",
      "     |      **If you are using only the Python interface, we recommend pickling the\n",
      "     |      model object for best results.**\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string or a memory buffer\n",
      "     |          Input file name or memory buffer(see also save_raw)\n",
      "     |  \n",
      "     |  save_model(self, fname)\n",
      "     |      Save the model to a file.\n",
      "     |      \n",
      "     |      The model is saved in an XGBoost internal binary format which is\n",
      "     |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      "     |      the Python Booster object (such as feature names) will not be loaded.\n",
      "     |      Label encodings (text labels to numeric labels) will be also lost.\n",
      "     |      **If you are using only the Python interface, we recommend pickling the\n",
      "     |      model object for best results.**\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string\n",
      "     |          Output file name\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      Modification of the sklearn method to allow unknown kwargs. This allows using\n",
      "     |      the full range of xgboost parameters that are not defined as member variables\n",
      "     |      in sklearn grid search.\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from XGBModel:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |      Coefficients property\n",
      "     |      \n",
      "     |      .. note:: Coefficients are defined only for linear learners\n",
      "     |      \n",
      "     |          Coefficients are only defined when the linear model is chosen as base\n",
      "     |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      "     |          as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Feature importances property\n",
      "     |      \n",
      "     |      .. note:: Feature importance is defined only for tree boosters\n",
      "     |      \n",
      "     |          Feature importance is only defined when the decision tree model is chosen as base\n",
      "     |          learner (`booster=gbtree`). It is not defined for other base learner types, such\n",
      "     |          as linear learners (`booster=gblinear`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array of shape ``[n_features]``\n",
      "     |  \n",
      "     |  intercept_\n",
      "     |      Intercept (bias) property\n",
      "     |      \n",
      "     |      .. note:: Intercept is defined only for linear learners\n",
      "     |      \n",
      "     |          Intercept (bias) is only defined when the linear model is chosen as base\n",
      "     |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      "     |          as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "    \n",
      "    class XGBRFRegressor(XGBRegressor)\n",
      "     |  XGBRFRegressor(max_depth=3, learning_rate=1, n_estimators=100, verbosity=1, silent=None, objective='reg:linear', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=0.8, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=0.8, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, **kwargs)\n",
      "     |  \n",
      "     |  Experimental implementation of the scikit-learn API for XGBoost random forest regression.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  max_depth : int\n",
      "     |      Maximum tree depth for base learners.\n",
      "     |  learning_rate : float\n",
      "     |      Boosting learning rate (xgb's \"eta\")\n",
      "     |  n_estimators : int\n",
      "     |      Number of trees to fit.\n",
      "     |  verbosity : int\n",
      "     |      The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
      "     |  silent : boolean\n",
      "     |      Whether to print messages while running boosting. Deprecated. Use verbosity instead.\n",
      "     |  objective : string or callable\n",
      "     |      Specify the learning task and the corresponding learning objective or\n",
      "     |      a custom objective function to be used (see note below).\n",
      "     |  booster: string\n",
      "     |      Specify which booster to use: gbtree, gblinear or dart.\n",
      "     |  nthread : int\n",
      "     |      Number of parallel threads used to run xgboost.  (Deprecated, please use ``n_jobs``)\n",
      "     |  n_jobs : int\n",
      "     |      Number of parallel threads used to run xgboost.  (replaces ``nthread``)\n",
      "     |  gamma : float\n",
      "     |      Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
      "     |  min_child_weight : int\n",
      "     |      Minimum sum of instance weight(hessian) needed in a child.\n",
      "     |  max_delta_step : int\n",
      "     |      Maximum delta step we allow each tree's weight estimation to be.\n",
      "     |  subsample : float\n",
      "     |      Subsample ratio of the training instance.\n",
      "     |  colsample_bytree : float\n",
      "     |      Subsample ratio of columns when constructing each tree.\n",
      "     |  colsample_bylevel : float\n",
      "     |      Subsample ratio of columns for each level.\n",
      "     |  colsample_bynode : float\n",
      "     |      Subsample ratio of columns for each split.\n",
      "     |  reg_alpha : float (xgb's alpha)\n",
      "     |      L1 regularization term on weights\n",
      "     |  reg_lambda : float (xgb's lambda)\n",
      "     |      L2 regularization term on weights\n",
      "     |  scale_pos_weight : float\n",
      "     |      Balancing of positive and negative weights.\n",
      "     |  base_score:\n",
      "     |      The initial prediction score of all instances, global bias.\n",
      "     |  seed : int\n",
      "     |      Random number seed.  (Deprecated, please use random_state)\n",
      "     |  random_state : int\n",
      "     |      Random number seed.  (replaces seed)\n",
      "     |  missing : float, optional\n",
      "     |      Value in the data which needs to be present as a missing value. If\n",
      "     |      None, defaults to np.nan.\n",
      "     |  importance_type: string, default \"gain\"\n",
      "     |      The feature importance type for the feature_importances_ property: either \"gain\",\n",
      "     |      \"weight\", \"cover\", \"total_gain\" or \"total_cover\".\n",
      "     |  \\*\\*kwargs : dict, optional\n",
      "     |      Keyword arguments for XGBoost Booster object.  Full documentation of parameters can\n",
      "     |      be found here: https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
      "     |      Attempting to set a parameter via the constructor args and \\*\\*kwargs dict simultaneously\n",
      "     |      will result in a TypeError.\n",
      "     |  \n",
      "     |      .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      "     |  \n",
      "     |          \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee that parameters\n",
      "     |          passed via this argument will interact properly with scikit-learn.\n",
      "     |  \n",
      "     |  Note\n",
      "     |  ----\n",
      "     |  A custom objective function can be provided for the ``objective``\n",
      "     |  parameter. In this case, it should have the signature\n",
      "     |  ``objective(y_true, y_pred) -> grad, hess``:\n",
      "     |  \n",
      "     |  y_true: array_like of shape [n_samples]\n",
      "     |      The target values\n",
      "     |  y_pred: array_like of shape [n_samples]\n",
      "     |      The predicted values\n",
      "     |  \n",
      "     |  grad: array_like of shape [n_samples]\n",
      "     |      The value of the gradient for each sample point.\n",
      "     |  hess: array_like of shape [n_samples]\n",
      "     |      The value of the second derivative for each sample point\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      XGBRFRegressor\n",
      "     |      XGBRegressor\n",
      "     |      XGBModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, max_depth=3, learning_rate=1, n_estimators=100, verbosity=1, silent=None, objective='reg:linear', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=0.8, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=0.8, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_num_boosting_rounds(self)\n",
      "     |      Gets the number of xgboost boosting rounds.\n",
      "     |  \n",
      "     |  get_xgb_params(self)\n",
      "     |      Get xgboost type parameters.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from XGBModel:\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  apply(self, X, ntree_limit=0)\n",
      "     |      Return the predicted leaf every tree for each sample.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like, shape=[n_samples, n_features]\n",
      "     |          Input features matrix.\n",
      "     |      \n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to 0 (use all trees).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      "     |          For each datapoint x in X and for each tree, return the index of the\n",
      "     |          leaf x ends up in. Leaves are numbered within\n",
      "     |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      "     |  \n",
      "     |  evals_result(self)\n",
      "     |      Return the evaluation results.\n",
      "     |      \n",
      "     |      If **eval_set** is passed to the `fit` function, you can call\n",
      "     |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.\n",
      "     |      When **eval_metric** is also passed to the `fit` function, the\n",
      "     |      **evals_result** will contain the **eval_metrics** passed to the `fit` function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      evals_result : dictionary\n",
      "     |      \n",
      "     |      Example\n",
      "     |      -------\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n",
      "     |      \n",
      "     |          clf = xgb.XGBModel(**param_dist)\n",
      "     |      \n",
      "     |          clf.fit(X_train, y_train,\n",
      "     |                  eval_set=[(X_train, y_train), (X_test, y_test)],\n",
      "     |                  eval_metric='logloss',\n",
      "     |                  verbose=True)\n",
      "     |      \n",
      "     |          evals_result = clf.evals_result()\n",
      "     |      \n",
      "     |      The variable **evals_result** will contain:\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      "     |          'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None, sample_weight_eval_set=None, callbacks=None)\n",
      "     |      Fit the gradient boosting model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like\n",
      "     |          Feature matrix\n",
      "     |      y : array_like\n",
      "     |          Labels\n",
      "     |      sample_weight : array_like\n",
      "     |          instance weights\n",
      "     |      eval_set : list, optional\n",
      "     |          A list of (X, y) tuple pairs to use as a validation set for\n",
      "     |          early-stopping\n",
      "     |      sample_weight_eval_set : list, optional\n",
      "     |          A list of the form [L_1, L_2, ..., L_n], where each L_i is a list of\n",
      "     |          instance weights on the i-th validation set.\n",
      "     |      eval_metric : str, callable, optional\n",
      "     |          If a str, should be a built-in evaluation metric to use. See\n",
      "     |          doc/parameter.rst. If callable, a custom evaluation metric. The call\n",
      "     |          signature is func(y_predicted, y_true) where y_true will be a\n",
      "     |          DMatrix object such that you may need to call the get_label\n",
      "     |          method. It must return a str, value pair where the str is a name\n",
      "     |          for the evaluation and value is the value of the evaluation\n",
      "     |          function. This objective is always minimized.\n",
      "     |      early_stopping_rounds : int\n",
      "     |          Activates early stopping. Validation error needs to decrease at\n",
      "     |          least every <early_stopping_rounds> round(s) to continue training.\n",
      "     |          Requires at least one item in evals.  If there's more than one,\n",
      "     |          will use the last. Returns the model from the last iteration\n",
      "     |          (not the best one). If early stopping occurs, the model will\n",
      "     |          have three additional fields: bst.best_score, bst.best_iteration\n",
      "     |          and bst.best_ntree_limit.\n",
      "     |          (Use bst.best_ntree_limit to get the correct value if num_parallel_tree\n",
      "     |          and/or num_class appears in the parameters)\n",
      "     |      verbose : bool\n",
      "     |          If `verbose` and an evaluation set is used, writes the evaluation\n",
      "     |          metric measured on the validation set to stderr.\n",
      "     |      xgb_model : str\n",
      "     |          file name of stored xgb model or 'Booster' instance Xgb model to be\n",
      "     |          loaded before training (allows training continuation).\n",
      "     |      callbacks : list of callback functions\n",
      "     |          List of callback functions that are applied at end of each iteration.\n",
      "     |          It is possible to use predefined callbacks by using :ref:`callback_api`.\n",
      "     |          Example:\n",
      "     |      \n",
      "     |          .. code-block:: python\n",
      "     |      \n",
      "     |              [xgb.callback.reset_learning_rate(custom_rates)]\n",
      "     |  \n",
      "     |  get_booster(self)\n",
      "     |      Get the underlying xgboost Booster of this model.\n",
      "     |      \n",
      "     |      This will raise an exception when fit was not called\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      booster : a xgboost booster of underlying model\n",
      "     |  \n",
      "     |  get_params(self, deep=False)\n",
      "     |      Get parameters.\n",
      "     |  \n",
      "     |  load_model(self, fname)\n",
      "     |      Load the model from a file.\n",
      "     |      \n",
      "     |      The model is loaded from an XGBoost internal binary format which is\n",
      "     |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      "     |      the Python Booster object (such as feature names) will not be loaded.\n",
      "     |      Label encodings (text labels to numeric labels) will be also lost.\n",
      "     |      **If you are using only the Python interface, we recommend pickling the\n",
      "     |      model object for best results.**\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string or a memory buffer\n",
      "     |          Input file name or memory buffer(see also save_raw)\n",
      "     |  \n",
      "     |  predict(self, data, output_margin=False, ntree_limit=None, validate_features=True)\n",
      "     |      Predict with `data`.\n",
      "     |      \n",
      "     |      .. note:: This function is not thread safe.\n",
      "     |      \n",
      "     |        For each booster object, predict can only be called from one thread.\n",
      "     |        If you want to run prediction using multiple thread, call ``xgb.copy()`` to make copies\n",
      "     |        of model object and then call ``predict()``.\n",
      "     |      \n",
      "     |      .. note:: Using ``predict()`` with DART booster\n",
      "     |      \n",
      "     |        If the booster object is DART type, ``predict()`` will perform dropouts, i.e. only\n",
      "     |        some of the trees will be evaluated. This will produce incorrect results if ``data`` is\n",
      "     |        not the training data. To obtain correct results on test sets, set ``ntree_limit`` to\n",
      "     |        a nonzero value, e.g.\n",
      "     |      \n",
      "     |        .. code-block:: python\n",
      "     |      \n",
      "     |          preds = bst.predict(dtest, ntree_limit=num_round)\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : DMatrix\n",
      "     |          The dmatrix storing the input.\n",
      "     |      output_margin : bool\n",
      "     |          Whether to output the raw untransformed margin value.\n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to best_ntree_limit if defined\n",
      "     |          (i.e. it has been trained with early stopping), otherwise 0 (use all trees).\n",
      "     |      validate_features : bool\n",
      "     |          When this is True, validate that the Booster's and data's feature_names are identical.\n",
      "     |          Otherwise, it is assumed that the feature_names are the same.\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      prediction : numpy array\n",
      "     |  \n",
      "     |  save_model(self, fname)\n",
      "     |      Save the model to a file.\n",
      "     |      \n",
      "     |      The model is saved in an XGBoost internal binary format which is\n",
      "     |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      "     |      the Python Booster object (such as feature names) will not be loaded.\n",
      "     |      Label encodings (text labels to numeric labels) will be also lost.\n",
      "     |      **If you are using only the Python interface, we recommend pickling the\n",
      "     |      model object for best results.**\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string\n",
      "     |          Output file name\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      Modification of the sklearn method to allow unknown kwargs. This allows using\n",
      "     |      the full range of xgboost parameters that are not defined as member variables\n",
      "     |      in sklearn grid search.\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from XGBModel:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |      Coefficients property\n",
      "     |      \n",
      "     |      .. note:: Coefficients are defined only for linear learners\n",
      "     |      \n",
      "     |          Coefficients are only defined when the linear model is chosen as base\n",
      "     |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      "     |          as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Feature importances property\n",
      "     |      \n",
      "     |      .. note:: Feature importance is defined only for tree boosters\n",
      "     |      \n",
      "     |          Feature importance is only defined when the decision tree model is chosen as base\n",
      "     |          learner (`booster=gbtree`). It is not defined for other base learner types, such\n",
      "     |          as linear learners (`booster=gblinear`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array of shape ``[n_features]``\n",
      "     |  \n",
      "     |  intercept_\n",
      "     |      Intercept (bias) property\n",
      "     |      \n",
      "     |      .. note:: Intercept is defined only for linear learners\n",
      "     |      \n",
      "     |          Intercept (bias) is only defined when the linear model is chosen as base\n",
      "     |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      "     |          as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a\n",
      "     |          precomputed kernel matrix instead, shape = (n_samples,\n",
      "     |          n_samples_fitted], where n_samples_fitted is the number of\n",
      "     |          samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The R2 score used when calling ``score`` on a regressor will use\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with `metrics.r2_score`. This will influence the ``score`` method of\n",
      "     |      all the multioutput regressors (except for\n",
      "     |      `multioutput.MultiOutputRegressor`). To specify the default value\n",
      "     |      manually and avoid the warning, please either call `metrics.r2_score`\n",
      "     |      directly or make a custom scorer with `metrics.make_scorer` (the\n",
      "     |      built-in scorer ``'r2'`` uses ``multioutput='uniform_average'``).\n",
      "    \n",
      "    class XGBRanker(XGBModel)\n",
      "     |  XGBRanker(max_depth=3, learning_rate=0.1, n_estimators=100, verbosity=1, silent=None, objective='rank:pairwise', booster='gbtree', n_jobs=-1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, **kwargs)\n",
      "     |  \n",
      "     |  Implementation of the Scikit-Learn API for XGBoost Ranking.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  max_depth : int\n",
      "     |      Maximum tree depth for base learners.\n",
      "     |  learning_rate : float\n",
      "     |      Boosting learning rate (xgb's \"eta\")\n",
      "     |  n_estimators : int\n",
      "     |      Number of boosted trees to fit.\n",
      "     |  verbosity : int\n",
      "     |      The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
      "     |  silent : boolean\n",
      "     |      Whether to print messages while running boosting. Deprecated. Use verbosity instead.\n",
      "     |  objective : string\n",
      "     |      Specify the learning task and the corresponding learning objective.\n",
      "     |      The objective name must start with \"rank:\".\n",
      "     |  booster: string\n",
      "     |      Specify which booster to use: gbtree, gblinear or dart.\n",
      "     |  nthread : int\n",
      "     |      Number of parallel threads used to run xgboost.  (Deprecated, please use ``n_jobs``)\n",
      "     |  n_jobs : int\n",
      "     |      Number of parallel threads used to run xgboost.  (replaces ``nthread``)\n",
      "     |  gamma : float\n",
      "     |      Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
      "     |  min_child_weight : int\n",
      "     |      Minimum sum of instance weight(hessian) needed in a child.\n",
      "     |  max_delta_step : int\n",
      "     |      Maximum delta step we allow each tree's weight estimation to be.\n",
      "     |  subsample : float\n",
      "     |      Subsample ratio of the training instance.\n",
      "     |  colsample_bytree : float\n",
      "     |      Subsample ratio of columns when constructing each tree.\n",
      "     |  colsample_bylevel : float\n",
      "     |      Subsample ratio of columns for each level.\n",
      "     |  colsample_bynode : float\n",
      "     |      Subsample ratio of columns for each split.\n",
      "     |  reg_alpha : float (xgb's alpha)\n",
      "     |      L1 regularization term on weights\n",
      "     |  reg_lambda : float (xgb's lambda)\n",
      "     |      L2 regularization term on weights\n",
      "     |  scale_pos_weight : float\n",
      "     |      Balancing of positive and negative weights.\n",
      "     |  base_score:\n",
      "     |      The initial prediction score of all instances, global bias.\n",
      "     |  seed : int\n",
      "     |      Random number seed.  (Deprecated, please use random_state)\n",
      "     |  random_state : int\n",
      "     |      Random number seed.  (replaces seed)\n",
      "     |  missing : float, optional\n",
      "     |      Value in the data which needs to be present as a missing value. If\n",
      "     |      None, defaults to np.nan.\n",
      "     |  \\*\\*kwargs : dict, optional\n",
      "     |      Keyword arguments for XGBoost Booster object.  Full documentation of parameters can\n",
      "     |      be found here: https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
      "     |      Attempting to set a parameter via the constructor args and \\*\\*kwargs dict\n",
      "     |      simultaneously will result in a TypeError.\n",
      "     |  \n",
      "     |      .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      "     |  \n",
      "     |          \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee that parameters\n",
      "     |          passed via this argument will interact properly with scikit-learn.\n",
      "     |  \n",
      "     |  Note\n",
      "     |  ----\n",
      "     |  A custom objective function is currently not supported by XGBRanker.\n",
      "     |  \n",
      "     |  Note\n",
      "     |  ----\n",
      "     |  Group information is required for ranking tasks.\n",
      "     |  \n",
      "     |  Before fitting the model, your data need to be sorted by group. When\n",
      "     |  fitting the model, you need to provide an additional array that\n",
      "     |  contains the size of each group.\n",
      "     |  \n",
      "     |  For example, if your original data look like:\n",
      "     |  \n",
      "     |  +-------+-----------+---------------+\n",
      "     |  |   qid |   label   |   features    |\n",
      "     |  +-------+-----------+---------------+\n",
      "     |  |   1   |   0       |   x_1         |\n",
      "     |  +-------+-----------+---------------+\n",
      "     |  |   1   |   1       |   x_2         |\n",
      "     |  +-------+-----------+---------------+\n",
      "     |  |   1   |   0       |   x_3         |\n",
      "     |  +-------+-----------+---------------+\n",
      "     |  |   2   |   0       |   x_4         |\n",
      "     |  +-------+-----------+---------------+\n",
      "     |  |   2   |   1       |   x_5         |\n",
      "     |  +-------+-----------+---------------+\n",
      "     |  |   2   |   1       |   x_6         |\n",
      "     |  +-------+-----------+---------------+\n",
      "     |  |   2   |   1       |   x_7         |\n",
      "     |  +-------+-----------+---------------+\n",
      "     |  \n",
      "     |  then your group array should be ``[3, 4]``.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      XGBRanker\n",
      "     |      XGBModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, max_depth=3, learning_rate=0.1, n_estimators=100, verbosity=1, silent=None, objective='rank:pairwise', booster='gbtree', n_jobs=-1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, group, sample_weight=None, eval_set=None, sample_weight_eval_set=None, eval_group=None, eval_metric=None, early_stopping_rounds=None, verbose=False, xgb_model=None, callbacks=None)\n",
      "     |      Fit the gradient boosting model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like\n",
      "     |          Feature matrix\n",
      "     |      y : array_like\n",
      "     |          Labels\n",
      "     |      group : array_like\n",
      "     |          group size of training data\n",
      "     |      sample_weight : array_like\n",
      "     |          group weights\n",
      "     |      \n",
      "     |          .. note:: Weights are per-group for ranking tasks\n",
      "     |      \n",
      "     |              In ranking task, one weight is assigned to each group (not each data\n",
      "     |              point). This is because we only care about the relative ordering of\n",
      "     |              data points within each group, so it doesn't make sense to assign\n",
      "     |              weights to individual data points.\n",
      "     |      \n",
      "     |      eval_set : list, optional\n",
      "     |          A list of (X, y) tuple pairs to use as a validation set for\n",
      "     |          early-stopping\n",
      "     |      sample_weight_eval_set : list, optional\n",
      "     |          A list of the form [L_1, L_2, ..., L_n], where each L_i is a list of\n",
      "     |          group weights on the i-th validation set.\n",
      "     |      \n",
      "     |          .. note:: Weights are per-group for ranking tasks\n",
      "     |      \n",
      "     |              In ranking task, one weight is assigned to each group (not each data\n",
      "     |              point). This is because we only care about the relative ordering of\n",
      "     |              data points within each group, so it doesn't make sense to assign\n",
      "     |              weights to individual data points.\n",
      "     |      \n",
      "     |      eval_group : list of arrays, optional\n",
      "     |          A list that contains the group size corresponds to each\n",
      "     |          (X, y) pair in eval_set\n",
      "     |      eval_metric : str, callable, optional\n",
      "     |          If a str, should be a built-in evaluation metric to use. See\n",
      "     |          doc/parameter.rst. If callable, a custom evaluation metric. The call\n",
      "     |          signature is func(y_predicted, y_true) where y_true will be a\n",
      "     |          DMatrix object such that you may need to call the get_label\n",
      "     |          method. It must return a str, value pair where the str is a name\n",
      "     |          for the evaluation and value is the value of the evaluation\n",
      "     |          function. This objective is always minimized.\n",
      "     |      early_stopping_rounds : int\n",
      "     |          Activates early stopping. Validation error needs to decrease at\n",
      "     |          least every <early_stopping_rounds> round(s) to continue training.\n",
      "     |          Requires at least one item in evals.  If there's more than one,\n",
      "     |          will use the last. Returns the model from the last iteration\n",
      "     |          (not the best one). If early stopping occurs, the model will\n",
      "     |          have three additional fields: bst.best_score, bst.best_iteration\n",
      "     |          and bst.best_ntree_limit.\n",
      "     |          (Use bst.best_ntree_limit to get the correct value if num_parallel_tree\n",
      "     |          and/or num_class appears in the parameters)\n",
      "     |      verbose : bool\n",
      "     |          If `verbose` and an evaluation set is used, writes the evaluation\n",
      "     |          metric measured on the validation set to stderr.\n",
      "     |      xgb_model : str\n",
      "     |          file name of stored xgb model or 'Booster' instance Xgb model to be\n",
      "     |          loaded before training (allows training continuation).\n",
      "     |      callbacks : list of callback functions\n",
      "     |          List of callback functions that are applied at end of each iteration.\n",
      "     |          It is possible to use predefined callbacks by using :ref:`callback_api`.\n",
      "     |          Example:\n",
      "     |      \n",
      "     |          .. code-block:: python\n",
      "     |      \n",
      "     |              [xgb.callback.reset_learning_rate(custom_rates)]\n",
      "     |  \n",
      "     |  predict(self, data, output_margin=False, ntree_limit=0, validate_features=True)\n",
      "     |      Predict with `data`.\n",
      "     |      \n",
      "     |      .. note:: This function is not thread safe.\n",
      "     |      \n",
      "     |        For each booster object, predict can only be called from one thread.\n",
      "     |        If you want to run prediction using multiple thread, call ``xgb.copy()`` to make copies\n",
      "     |        of model object and then call ``predict()``.\n",
      "     |      \n",
      "     |      .. note:: Using ``predict()`` with DART booster\n",
      "     |      \n",
      "     |        If the booster object is DART type, ``predict()`` will perform dropouts, i.e. only\n",
      "     |        some of the trees will be evaluated. This will produce incorrect results if ``data`` is\n",
      "     |        not the training data. To obtain correct results on test sets, set ``ntree_limit`` to\n",
      "     |        a nonzero value, e.g.\n",
      "     |      \n",
      "     |        .. code-block:: python\n",
      "     |      \n",
      "     |          preds = bst.predict(dtest, ntree_limit=num_round)\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : DMatrix\n",
      "     |          The dmatrix storing the input.\n",
      "     |      output_margin : bool\n",
      "     |          Whether to output the raw untransformed margin value.\n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to best_ntree_limit if defined\n",
      "     |          (i.e. it has been trained with early stopping), otherwise 0 (use all trees).\n",
      "     |      validate_features : bool\n",
      "     |          When this is True, validate that the Booster's and data's feature_names are identical.\n",
      "     |          Otherwise, it is assumed that the feature_names are the same.\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      prediction : numpy array\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from XGBModel:\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  apply(self, X, ntree_limit=0)\n",
      "     |      Return the predicted leaf every tree for each sample.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like, shape=[n_samples, n_features]\n",
      "     |          Input features matrix.\n",
      "     |      \n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to 0 (use all trees).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      "     |          For each datapoint x in X and for each tree, return the index of the\n",
      "     |          leaf x ends up in. Leaves are numbered within\n",
      "     |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      "     |  \n",
      "     |  evals_result(self)\n",
      "     |      Return the evaluation results.\n",
      "     |      \n",
      "     |      If **eval_set** is passed to the `fit` function, you can call\n",
      "     |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.\n",
      "     |      When **eval_metric** is also passed to the `fit` function, the\n",
      "     |      **evals_result** will contain the **eval_metrics** passed to the `fit` function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      evals_result : dictionary\n",
      "     |      \n",
      "     |      Example\n",
      "     |      -------\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n",
      "     |      \n",
      "     |          clf = xgb.XGBModel(**param_dist)\n",
      "     |      \n",
      "     |          clf.fit(X_train, y_train,\n",
      "     |                  eval_set=[(X_train, y_train), (X_test, y_test)],\n",
      "     |                  eval_metric='logloss',\n",
      "     |                  verbose=True)\n",
      "     |      \n",
      "     |          evals_result = clf.evals_result()\n",
      "     |      \n",
      "     |      The variable **evals_result** will contain:\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      "     |          'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      "     |  \n",
      "     |  get_booster(self)\n",
      "     |      Get the underlying xgboost Booster of this model.\n",
      "     |      \n",
      "     |      This will raise an exception when fit was not called\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      booster : a xgboost booster of underlying model\n",
      "     |  \n",
      "     |  get_num_boosting_rounds(self)\n",
      "     |      Gets the number of xgboost boosting rounds.\n",
      "     |  \n",
      "     |  get_params(self, deep=False)\n",
      "     |      Get parameters.\n",
      "     |  \n",
      "     |  get_xgb_params(self)\n",
      "     |      Get xgboost type parameters.\n",
      "     |  \n",
      "     |  load_model(self, fname)\n",
      "     |      Load the model from a file.\n",
      "     |      \n",
      "     |      The model is loaded from an XGBoost internal binary format which is\n",
      "     |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      "     |      the Python Booster object (such as feature names) will not be loaded.\n",
      "     |      Label encodings (text labels to numeric labels) will be also lost.\n",
      "     |      **If you are using only the Python interface, we recommend pickling the\n",
      "     |      model object for best results.**\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string or a memory buffer\n",
      "     |          Input file name or memory buffer(see also save_raw)\n",
      "     |  \n",
      "     |  save_model(self, fname)\n",
      "     |      Save the model to a file.\n",
      "     |      \n",
      "     |      The model is saved in an XGBoost internal binary format which is\n",
      "     |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      "     |      the Python Booster object (such as feature names) will not be loaded.\n",
      "     |      Label encodings (text labels to numeric labels) will be also lost.\n",
      "     |      **If you are using only the Python interface, we recommend pickling the\n",
      "     |      model object for best results.**\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string\n",
      "     |          Output file name\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      Modification of the sklearn method to allow unknown kwargs. This allows using\n",
      "     |      the full range of xgboost parameters that are not defined as member variables\n",
      "     |      in sklearn grid search.\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from XGBModel:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |      Coefficients property\n",
      "     |      \n",
      "     |      .. note:: Coefficients are defined only for linear learners\n",
      "     |      \n",
      "     |          Coefficients are only defined when the linear model is chosen as base\n",
      "     |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      "     |          as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Feature importances property\n",
      "     |      \n",
      "     |      .. note:: Feature importance is defined only for tree boosters\n",
      "     |      \n",
      "     |          Feature importance is only defined when the decision tree model is chosen as base\n",
      "     |          learner (`booster=gbtree`). It is not defined for other base learner types, such\n",
      "     |          as linear learners (`booster=gblinear`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array of shape ``[n_features]``\n",
      "     |  \n",
      "     |  intercept_\n",
      "     |      Intercept (bias) property\n",
      "     |      \n",
      "     |      .. note:: Intercept is defined only for linear learners\n",
      "     |      \n",
      "     |          Intercept (bias) is only defined when the linear model is chosen as base\n",
      "     |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      "     |          as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class XGBRegressor(XGBModel, sklearn.base.RegressorMixin)\n",
      "     |  XGBRegressor(max_depth=3, learning_rate=0.1, n_estimators=100, verbosity=1, silent=None, objective='reg:linear', booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, importance_type='gain', **kwargs)\n",
      "     |  \n",
      "     |  Implementation of the scikit-learn API for XGBoost regression.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  max_depth : int\n",
      "     |      Maximum tree depth for base learners.\n",
      "     |  learning_rate : float\n",
      "     |      Boosting learning rate (xgb's \"eta\")\n",
      "     |  n_estimators : int\n",
      "     |      Number of trees to fit.\n",
      "     |  verbosity : int\n",
      "     |      The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
      "     |  silent : boolean\n",
      "     |      Whether to print messages while running boosting. Deprecated. Use verbosity instead.\n",
      "     |  objective : string or callable\n",
      "     |      Specify the learning task and the corresponding learning objective or\n",
      "     |      a custom objective function to be used (see note below).\n",
      "     |  booster: string\n",
      "     |      Specify which booster to use: gbtree, gblinear or dart.\n",
      "     |  nthread : int\n",
      "     |      Number of parallel threads used to run xgboost.  (Deprecated, please use ``n_jobs``)\n",
      "     |  n_jobs : int\n",
      "     |      Number of parallel threads used to run xgboost.  (replaces ``nthread``)\n",
      "     |  gamma : float\n",
      "     |      Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
      "     |  min_child_weight : int\n",
      "     |      Minimum sum of instance weight(hessian) needed in a child.\n",
      "     |  max_delta_step : int\n",
      "     |      Maximum delta step we allow each tree's weight estimation to be.\n",
      "     |  subsample : float\n",
      "     |      Subsample ratio of the training instance.\n",
      "     |  colsample_bytree : float\n",
      "     |      Subsample ratio of columns when constructing each tree.\n",
      "     |  colsample_bylevel : float\n",
      "     |      Subsample ratio of columns for each level.\n",
      "     |  colsample_bynode : float\n",
      "     |      Subsample ratio of columns for each split.\n",
      "     |  reg_alpha : float (xgb's alpha)\n",
      "     |      L1 regularization term on weights\n",
      "     |  reg_lambda : float (xgb's lambda)\n",
      "     |      L2 regularization term on weights\n",
      "     |  scale_pos_weight : float\n",
      "     |      Balancing of positive and negative weights.\n",
      "     |  base_score:\n",
      "     |      The initial prediction score of all instances, global bias.\n",
      "     |  seed : int\n",
      "     |      Random number seed.  (Deprecated, please use random_state)\n",
      "     |  random_state : int\n",
      "     |      Random number seed.  (replaces seed)\n",
      "     |  missing : float, optional\n",
      "     |      Value in the data which needs to be present as a missing value. If\n",
      "     |      None, defaults to np.nan.\n",
      "     |  importance_type: string, default \"gain\"\n",
      "     |      The feature importance type for the feature_importances_ property: either \"gain\",\n",
      "     |      \"weight\", \"cover\", \"total_gain\" or \"total_cover\".\n",
      "     |  \\*\\*kwargs : dict, optional\n",
      "     |      Keyword arguments for XGBoost Booster object.  Full documentation of parameters can\n",
      "     |      be found here: https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
      "     |      Attempting to set a parameter via the constructor args and \\*\\*kwargs dict simultaneously\n",
      "     |      will result in a TypeError.\n",
      "     |  \n",
      "     |      .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      "     |  \n",
      "     |          \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee that parameters\n",
      "     |          passed via this argument will interact properly with scikit-learn.\n",
      "     |  \n",
      "     |  Note\n",
      "     |  ----\n",
      "     |  A custom objective function can be provided for the ``objective``\n",
      "     |  parameter. In this case, it should have the signature\n",
      "     |  ``objective(y_true, y_pred) -> grad, hess``:\n",
      "     |  \n",
      "     |  y_true: array_like of shape [n_samples]\n",
      "     |      The target values\n",
      "     |  y_pred: array_like of shape [n_samples]\n",
      "     |      The predicted values\n",
      "     |  \n",
      "     |  grad: array_like of shape [n_samples]\n",
      "     |      The value of the gradient for each sample point.\n",
      "     |  hess: array_like of shape [n_samples]\n",
      "     |      The value of the second derivative for each sample point\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      XGBRegressor\n",
      "     |      XGBModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods inherited from XGBModel:\n",
      "     |  \n",
      "     |  __init__(self, max_depth=3, learning_rate=0.1, n_estimators=100, verbosity=1, silent=None, objective='reg:linear', booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, importance_type='gain', **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  apply(self, X, ntree_limit=0)\n",
      "     |      Return the predicted leaf every tree for each sample.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like, shape=[n_samples, n_features]\n",
      "     |          Input features matrix.\n",
      "     |      \n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to 0 (use all trees).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      "     |          For each datapoint x in X and for each tree, return the index of the\n",
      "     |          leaf x ends up in. Leaves are numbered within\n",
      "     |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      "     |  \n",
      "     |  evals_result(self)\n",
      "     |      Return the evaluation results.\n",
      "     |      \n",
      "     |      If **eval_set** is passed to the `fit` function, you can call\n",
      "     |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.\n",
      "     |      When **eval_metric** is also passed to the `fit` function, the\n",
      "     |      **evals_result** will contain the **eval_metrics** passed to the `fit` function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      evals_result : dictionary\n",
      "     |      \n",
      "     |      Example\n",
      "     |      -------\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n",
      "     |      \n",
      "     |          clf = xgb.XGBModel(**param_dist)\n",
      "     |      \n",
      "     |          clf.fit(X_train, y_train,\n",
      "     |                  eval_set=[(X_train, y_train), (X_test, y_test)],\n",
      "     |                  eval_metric='logloss',\n",
      "     |                  verbose=True)\n",
      "     |      \n",
      "     |          evals_result = clf.evals_result()\n",
      "     |      \n",
      "     |      The variable **evals_result** will contain:\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      "     |          'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None, sample_weight_eval_set=None, callbacks=None)\n",
      "     |      Fit the gradient boosting model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like\n",
      "     |          Feature matrix\n",
      "     |      y : array_like\n",
      "     |          Labels\n",
      "     |      sample_weight : array_like\n",
      "     |          instance weights\n",
      "     |      eval_set : list, optional\n",
      "     |          A list of (X, y) tuple pairs to use as a validation set for\n",
      "     |          early-stopping\n",
      "     |      sample_weight_eval_set : list, optional\n",
      "     |          A list of the form [L_1, L_2, ..., L_n], where each L_i is a list of\n",
      "     |          instance weights on the i-th validation set.\n",
      "     |      eval_metric : str, callable, optional\n",
      "     |          If a str, should be a built-in evaluation metric to use. See\n",
      "     |          doc/parameter.rst. If callable, a custom evaluation metric. The call\n",
      "     |          signature is func(y_predicted, y_true) where y_true will be a\n",
      "     |          DMatrix object such that you may need to call the get_label\n",
      "     |          method. It must return a str, value pair where the str is a name\n",
      "     |          for the evaluation and value is the value of the evaluation\n",
      "     |          function. This objective is always minimized.\n",
      "     |      early_stopping_rounds : int\n",
      "     |          Activates early stopping. Validation error needs to decrease at\n",
      "     |          least every <early_stopping_rounds> round(s) to continue training.\n",
      "     |          Requires at least one item in evals.  If there's more than one,\n",
      "     |          will use the last. Returns the model from the last iteration\n",
      "     |          (not the best one). If early stopping occurs, the model will\n",
      "     |          have three additional fields: bst.best_score, bst.best_iteration\n",
      "     |          and bst.best_ntree_limit.\n",
      "     |          (Use bst.best_ntree_limit to get the correct value if num_parallel_tree\n",
      "     |          and/or num_class appears in the parameters)\n",
      "     |      verbose : bool\n",
      "     |          If `verbose` and an evaluation set is used, writes the evaluation\n",
      "     |          metric measured on the validation set to stderr.\n",
      "     |      xgb_model : str\n",
      "     |          file name of stored xgb model or 'Booster' instance Xgb model to be\n",
      "     |          loaded before training (allows training continuation).\n",
      "     |      callbacks : list of callback functions\n",
      "     |          List of callback functions that are applied at end of each iteration.\n",
      "     |          It is possible to use predefined callbacks by using :ref:`callback_api`.\n",
      "     |          Example:\n",
      "     |      \n",
      "     |          .. code-block:: python\n",
      "     |      \n",
      "     |              [xgb.callback.reset_learning_rate(custom_rates)]\n",
      "     |  \n",
      "     |  get_booster(self)\n",
      "     |      Get the underlying xgboost Booster of this model.\n",
      "     |      \n",
      "     |      This will raise an exception when fit was not called\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      booster : a xgboost booster of underlying model\n",
      "     |  \n",
      "     |  get_num_boosting_rounds(self)\n",
      "     |      Gets the number of xgboost boosting rounds.\n",
      "     |  \n",
      "     |  get_params(self, deep=False)\n",
      "     |      Get parameters.\n",
      "     |  \n",
      "     |  get_xgb_params(self)\n",
      "     |      Get xgboost type parameters.\n",
      "     |  \n",
      "     |  load_model(self, fname)\n",
      "     |      Load the model from a file.\n",
      "     |      \n",
      "     |      The model is loaded from an XGBoost internal binary format which is\n",
      "     |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      "     |      the Python Booster object (such as feature names) will not be loaded.\n",
      "     |      Label encodings (text labels to numeric labels) will be also lost.\n",
      "     |      **If you are using only the Python interface, we recommend pickling the\n",
      "     |      model object for best results.**\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string or a memory buffer\n",
      "     |          Input file name or memory buffer(see also save_raw)\n",
      "     |  \n",
      "     |  predict(self, data, output_margin=False, ntree_limit=None, validate_features=True)\n",
      "     |      Predict with `data`.\n",
      "     |      \n",
      "     |      .. note:: This function is not thread safe.\n",
      "     |      \n",
      "     |        For each booster object, predict can only be called from one thread.\n",
      "     |        If you want to run prediction using multiple thread, call ``xgb.copy()`` to make copies\n",
      "     |        of model object and then call ``predict()``.\n",
      "     |      \n",
      "     |      .. note:: Using ``predict()`` with DART booster\n",
      "     |      \n",
      "     |        If the booster object is DART type, ``predict()`` will perform dropouts, i.e. only\n",
      "     |        some of the trees will be evaluated. This will produce incorrect results if ``data`` is\n",
      "     |        not the training data. To obtain correct results on test sets, set ``ntree_limit`` to\n",
      "     |        a nonzero value, e.g.\n",
      "     |      \n",
      "     |        .. code-block:: python\n",
      "     |      \n",
      "     |          preds = bst.predict(dtest, ntree_limit=num_round)\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : DMatrix\n",
      "     |          The dmatrix storing the input.\n",
      "     |      output_margin : bool\n",
      "     |          Whether to output the raw untransformed margin value.\n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to best_ntree_limit if defined\n",
      "     |          (i.e. it has been trained with early stopping), otherwise 0 (use all trees).\n",
      "     |      validate_features : bool\n",
      "     |          When this is True, validate that the Booster's and data's feature_names are identical.\n",
      "     |          Otherwise, it is assumed that the feature_names are the same.\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      prediction : numpy array\n",
      "     |  \n",
      "     |  save_model(self, fname)\n",
      "     |      Save the model to a file.\n",
      "     |      \n",
      "     |      The model is saved in an XGBoost internal binary format which is\n",
      "     |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      "     |      the Python Booster object (such as feature names) will not be loaded.\n",
      "     |      Label encodings (text labels to numeric labels) will be also lost.\n",
      "     |      **If you are using only the Python interface, we recommend pickling the\n",
      "     |      model object for best results.**\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string\n",
      "     |          Output file name\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      Modification of the sklearn method to allow unknown kwargs. This allows using\n",
      "     |      the full range of xgboost parameters that are not defined as member variables\n",
      "     |      in sklearn grid search.\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from XGBModel:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |      Coefficients property\n",
      "     |      \n",
      "     |      .. note:: Coefficients are defined only for linear learners\n",
      "     |      \n",
      "     |          Coefficients are only defined when the linear model is chosen as base\n",
      "     |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      "     |          as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Feature importances property\n",
      "     |      \n",
      "     |      .. note:: Feature importance is defined only for tree boosters\n",
      "     |      \n",
      "     |          Feature importance is only defined when the decision tree model is chosen as base\n",
      "     |          learner (`booster=gbtree`). It is not defined for other base learner types, such\n",
      "     |          as linear learners (`booster=gblinear`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array of shape ``[n_features]``\n",
      "     |  \n",
      "     |  intercept_\n",
      "     |      Intercept (bias) property\n",
      "     |      \n",
      "     |      .. note:: Intercept is defined only for linear learners\n",
      "     |      \n",
      "     |          Intercept (bias) is only defined when the linear model is chosen as base\n",
      "     |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      "     |          as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a\n",
      "     |          precomputed kernel matrix instead, shape = (n_samples,\n",
      "     |          n_samples_fitted], where n_samples_fitted is the number of\n",
      "     |          samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The R2 score used when calling ``score`` on a regressor will use\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with `metrics.r2_score`. This will influence the ``score`` method of\n",
      "     |      all the multioutput regressors (except for\n",
      "     |      `multioutput.MultiOutputRegressor`). To specify the default value\n",
      "     |      manually and avoid the warning, please either call `metrics.r2_score`\n",
      "     |      directly or make a custom scorer with `metrics.make_scorer` (the\n",
      "     |      built-in scorer ``'r2'`` uses ``multioutput='uniform_average'``).\n",
      "\n",
      "FUNCTIONS\n",
      "    cv(params, dtrain, num_boost_round=10, nfold=3, stratified=False, folds=None, metrics=(), obj=None, feval=None, maximize=False, early_stopping_rounds=None, fpreproc=None, as_pandas=True, verbose_eval=None, show_stdv=True, seed=0, callbacks=None, shuffle=True)\n",
      "        Cross-validation with given parameters.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        params : dict\n",
      "            Booster params.\n",
      "        dtrain : DMatrix\n",
      "            Data to be trained.\n",
      "        num_boost_round : int\n",
      "            Number of boosting iterations.\n",
      "        nfold : int\n",
      "            Number of folds in CV.\n",
      "        stratified : bool\n",
      "            Perform stratified sampling.\n",
      "        folds : a KFold or StratifiedKFold instance or list of fold indices\n",
      "            Sklearn KFolds or StratifiedKFolds object.\n",
      "            Alternatively may explicitly pass sample indices for each fold.\n",
      "            For ``n`` folds, **folds** should be a length ``n`` list of tuples.\n",
      "            Each tuple is ``(in,out)`` where ``in`` is a list of indices to be used\n",
      "            as the training samples for the ``n`` th fold and ``out`` is a list of\n",
      "            indices to be used as the testing samples for the ``n`` th fold.\n",
      "        metrics : string or list of strings\n",
      "            Evaluation metrics to be watched in CV.\n",
      "        obj : function\n",
      "            Custom objective function.\n",
      "        feval : function\n",
      "            Custom evaluation function.\n",
      "        maximize : bool\n",
      "            Whether to maximize feval.\n",
      "        early_stopping_rounds: int\n",
      "            Activates early stopping. CV error needs to decrease at least\n",
      "            every <early_stopping_rounds> round(s) to continue.\n",
      "            Last entry in evaluation history is the one from best iteration.\n",
      "        fpreproc : function\n",
      "            Preprocessing function that takes (dtrain, dtest, param) and returns\n",
      "            transformed versions of those.\n",
      "        as_pandas : bool, default True\n",
      "            Return pd.DataFrame when pandas is installed.\n",
      "            If False or pandas is not installed, return np.ndarray\n",
      "        verbose_eval : bool, int, or None, default None\n",
      "            Whether to display the progress. If None, progress will be displayed\n",
      "            when np.ndarray is returned. If True, progress will be displayed at\n",
      "            boosting stage. If an integer is given, progress will be displayed\n",
      "            at every given `verbose_eval` boosting stage.\n",
      "        show_stdv : bool, default True\n",
      "            Whether to display the standard deviation in progress.\n",
      "            Results are not affected, and always contains std.\n",
      "        seed : int\n",
      "            Seed used to generate the folds (passed to numpy.random.seed).\n",
      "        callbacks : list of callback functions\n",
      "            List of callback functions that are applied at end of each iteration.\n",
      "            It is possible to use predefined callbacks by using\n",
      "            :ref:`Callback API <callback_api>`.\n",
      "            Example:\n",
      "        \n",
      "            .. code-block:: python\n",
      "        \n",
      "                [xgb.callback.reset_learning_rate(custom_rates)]\n",
      "        shuffle : bool\n",
      "            Shuffle data before creating folds.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        evaluation history : list(string)\n",
      "    \n",
      "    plot_importance(booster, ax=None, height=0.2, xlim=None, ylim=None, title='Feature importance', xlabel='F score', ylabel='Features', importance_type='weight', max_num_features=None, grid=True, show_values=True, **kwargs)\n",
      "        Plot importance based on fitted trees.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        booster : Booster, XGBModel or dict\n",
      "            Booster or XGBModel instance, or dict taken by Booster.get_fscore()\n",
      "        ax : matplotlib Axes, default None\n",
      "            Target axes instance. If None, new figure and axes will be created.\n",
      "        grid : bool, Turn the axes grids on or off.  Default is True (On).\n",
      "        importance_type : str, default \"weight\"\n",
      "            How the importance is calculated: either \"weight\", \"gain\", or \"cover\"\n",
      "        \n",
      "            * \"weight\" is the number of times a feature appears in a tree\n",
      "            * \"gain\" is the average gain of splits which use the feature\n",
      "            * \"cover\" is the average coverage of splits which use the feature\n",
      "              where coverage is defined as the number of samples affected by the split\n",
      "        max_num_features : int, default None\n",
      "            Maximum number of top features displayed on plot. If None, all features will be displayed.\n",
      "        height : float, default 0.2\n",
      "            Bar height, passed to ax.barh()\n",
      "        xlim : tuple, default None\n",
      "            Tuple passed to axes.xlim()\n",
      "        ylim : tuple, default None\n",
      "            Tuple passed to axes.ylim()\n",
      "        title : str, default \"Feature importance\"\n",
      "            Axes title. To disable, pass None.\n",
      "        xlabel : str, default \"F score\"\n",
      "            X axis title label. To disable, pass None.\n",
      "        ylabel : str, default \"Features\"\n",
      "            Y axis title label. To disable, pass None.\n",
      "        show_values : bool, default True\n",
      "            Show values on plot. To disable, pass False.\n",
      "        kwargs :\n",
      "            Other keywords passed to ax.barh()\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ax : matplotlib Axes\n",
      "    \n",
      "    plot_tree(booster, fmap='', num_trees=0, rankdir='UT', ax=None, **kwargs)\n",
      "        Plot specified tree.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        booster : Booster, XGBModel\n",
      "            Booster or XGBModel instance\n",
      "        fmap: str (optional)\n",
      "           The name of feature map file\n",
      "        num_trees : int, default 0\n",
      "            Specify the ordinal number of target tree\n",
      "        rankdir : str, default \"UT\"\n",
      "            Passed to graphiz via graph_attr\n",
      "        ax : matplotlib Axes, default None\n",
      "            Target axes instance. If None, new figure and axes will be created.\n",
      "        kwargs :\n",
      "            Other keywords passed to to_graphviz\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ax : matplotlib Axes\n",
      "    \n",
      "    to_graphviz(booster, fmap='', num_trees=0, rankdir='UT', yes_color='#0000FF', no_color='#FF0000', condition_node_params=None, leaf_node_params=None, **kwargs)\n",
      "        Convert specified tree to graphviz instance. IPython can automatically plot the\n",
      "        returned graphiz instance. Otherwise, you should call .render() method\n",
      "        of the returned graphiz instance.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        booster : Booster, XGBModel\n",
      "            Booster or XGBModel instance\n",
      "        fmap: str (optional)\n",
      "           The name of feature map file\n",
      "        num_trees : int, default 0\n",
      "            Specify the ordinal number of target tree\n",
      "        rankdir : str, default \"UT\"\n",
      "            Passed to graphiz via graph_attr\n",
      "        yes_color : str, default '#0000FF'\n",
      "            Edge color when meets the node condition.\n",
      "        no_color : str, default '#FF0000'\n",
      "            Edge color when doesn't meet the node condition.\n",
      "        condition_node_params : dict (optional)\n",
      "            condition node configuration,\n",
      "            {'shape':'box',\n",
      "                   'style':'filled,rounded',\n",
      "                   'fillcolor':'#78bceb'\n",
      "            }\n",
      "        leaf_node_params : dict (optional)\n",
      "            leaf node configuration\n",
      "            {'shape':'box',\n",
      "                   'style':'filled',\n",
      "                   'fillcolor':'#e48038'\n",
      "            }\n",
      "        kwargs :\n",
      "            Other keywords passed to graphviz graph_attr\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ax : matplotlib Axes\n",
      "    \n",
      "    train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None, maximize=False, early_stopping_rounds=None, evals_result=None, verbose_eval=True, xgb_model=None, callbacks=None, learning_rates=None)\n",
      "        Train a booster with given parameters.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        params : dict\n",
      "            Booster params.\n",
      "        dtrain : DMatrix\n",
      "            Data to be trained.\n",
      "        num_boost_round: int\n",
      "            Number of boosting iterations.\n",
      "        evals: list of pairs (DMatrix, string)\n",
      "            List of items to be evaluated during training, this allows user to watch\n",
      "            performance on the validation set.\n",
      "        obj : function\n",
      "            Customized objective function.\n",
      "        feval : function\n",
      "            Customized evaluation function.\n",
      "        maximize : bool\n",
      "            Whether to maximize feval.\n",
      "        early_stopping_rounds: int\n",
      "            Activates early stopping. Validation error needs to decrease at least\n",
      "            every **early_stopping_rounds** round(s) to continue training.\n",
      "            Requires at least one item in **evals**.\n",
      "            If there's more than one, will use the last.\n",
      "            Returns the model from the last iteration (not the best one).\n",
      "            If early stopping occurs, the model will have three additional fields:\n",
      "            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.\n",
      "            (Use ``bst.best_ntree_limit`` to get the correct value if\n",
      "            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)\n",
      "        evals_result: dict\n",
      "            This dictionary stores the evaluation results of all the items in watchlist.\n",
      "        \n",
      "            Example: with a watchlist containing\n",
      "            ``[(dtest,'eval'), (dtrain,'train')]`` and\n",
      "            a parameter containing ``('eval_metric': 'logloss')``,\n",
      "            the **evals_result** returns\n",
      "        \n",
      "            .. code-block:: python\n",
      "        \n",
      "                {'train': {'logloss': ['0.48253', '0.35953']},\n",
      "                 'eval': {'logloss': ['0.480385', '0.357756']}}\n",
      "        \n",
      "        verbose_eval : bool or int\n",
      "            Requires at least one item in **evals**.\n",
      "            If **verbose_eval** is True then the evaluation metric on the validation set is\n",
      "            printed at each boosting stage.\n",
      "            If **verbose_eval** is an integer then the evaluation metric on the validation set\n",
      "            is printed at every given **verbose_eval** boosting stage. The last boosting stage\n",
      "            / the boosting stage found by using **early_stopping_rounds** is also printed.\n",
      "            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric\n",
      "            is printed every 4 boosting stages, instead of every boosting stage.\n",
      "        learning_rates: list or function (deprecated - use callback API instead)\n",
      "            List of learning rate for each boosting round\n",
      "            or a customized function that calculates eta in terms of\n",
      "            current number of round and the total number of boosting round (e.g. yields\n",
      "            learning rate decay)\n",
      "        xgb_model : file name of stored xgb model or 'Booster' instance\n",
      "            Xgb model to be loaded before training (allows training continuation).\n",
      "        callbacks : list of callback functions\n",
      "            List of callback functions that are applied at end of each iteration.\n",
      "            It is possible to use predefined callbacks by using\n",
      "            :ref:`Callback API <callback_api>`.\n",
      "            Example:\n",
      "        \n",
      "            .. code-block:: python\n",
      "        \n",
      "                [xgb.callback.reset_learning_rate(custom_rates)]\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Booster : a trained booster model\n",
      "\n",
      "DATA\n",
      "    __all__ = ['DMatrix', 'Booster', 'train', 'cv', 'XGBModel', 'XGBClassi...\n",
      "\n",
      "VERSION\n",
      "    0.90\n",
      "\n",
      "FILE\n",
      "    /home/yazi/anaconda3/envs/tf/lib/python3.7/site-packages/xgboost/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_true=pd.read_csv('/home/yazi/Documents/stage/data/data1_true.csv',delimiter=';',encoding='iso 8859-1')\n",
    "data2_true=pd.read_csv('/home/yazi/Documents/stage/data/data2_true.csv',delimiter=';',encoding='iso 8859-1')\n",
    "data1_false=pd.read_csv('/home/yazi/Documents/stage/data/data1_false.csv',delimiter=';',encoding='iso 8859-1')\n",
    "data2_false=pd.read_csv('/home/yazi/Documents/stage/data/data2_false.csv',delimiter=';',encoding='iso 8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.42148014440433"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(data1_false)+len(data2_false))/(len(data1_true)+len(data2_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ot</th>\n",
       "      <th>Dt</th>\n",
       "      <th>DUREE</th>\n",
       "      <th>NBTRAJTC</th>\n",
       "      <th>dist</th>\n",
       "      <th>parking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18300.0</td>\n",
       "      <td>19800.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20362.956563</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34200.0</td>\n",
       "      <td>35340.0</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1941.648784</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36000.0</td>\n",
       "      <td>37800.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>23648.467181</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27000.0</td>\n",
       "      <td>31800.0</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>35474.638828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27000.0</td>\n",
       "      <td>32400.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>35713.582850</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30000.0</td>\n",
       "      <td>32400.0</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13400.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>27900.0</td>\n",
       "      <td>33600.0</td>\n",
       "      <td>5700.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>32614.260685</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>48600.0</td>\n",
       "      <td>51300.0</td>\n",
       "      <td>2700.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16319.620094</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>19800.0</td>\n",
       "      <td>22500.0</td>\n",
       "      <td>2700.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19009.471324</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>30000.0</td>\n",
       "      <td>30300.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>66000.0</td>\n",
       "      <td>66600.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1421.267040</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>21600.0</td>\n",
       "      <td>27000.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>41824.872982</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>25200.0</td>\n",
       "      <td>32400.0</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24935.917870</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>27000.0</td>\n",
       "      <td>31500.0</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30084.215130</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>24300.0</td>\n",
       "      <td>29100.0</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20260.799589</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>25200.0</td>\n",
       "      <td>27060.0</td>\n",
       "      <td>1860.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13061.393494</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>25200.0</td>\n",
       "      <td>29040.0</td>\n",
       "      <td>3840.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>28540.847920</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>26100.0</td>\n",
       "      <td>30600.0</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>39805.778475</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>47700.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6484.597135</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>26100.0</td>\n",
       "      <td>30600.0</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>35467.449866</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>26700.0</td>\n",
       "      <td>31500.0</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30620.581314</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>27000.0</td>\n",
       "      <td>33300.0</td>\n",
       "      <td>6300.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28338.136848</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>25800.0</td>\n",
       "      <td>31500.0</td>\n",
       "      <td>5700.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>41388.162559</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>22800.0</td>\n",
       "      <td>29400.0</td>\n",
       "      <td>6600.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>62309.068361</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>29400.0</td>\n",
       "      <td>35700.0</td>\n",
       "      <td>6300.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>41629.316593</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26100.0</td>\n",
       "      <td>30600.0</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>45632.554169</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>18600.0</td>\n",
       "      <td>27000.0</td>\n",
       "      <td>8400.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>54800.091241</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>18000.0</td>\n",
       "      <td>24300.0</td>\n",
       "      <td>6300.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>65060.971404</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>22800.0</td>\n",
       "      <td>27000.0</td>\n",
       "      <td>4200.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24087.548651</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>24300.0</td>\n",
       "      <td>29400.0</td>\n",
       "      <td>5100.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>46044.000695</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21489</th>\n",
       "      <td>57600.0</td>\n",
       "      <td>64800.0</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>45449.312426</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21490</th>\n",
       "      <td>62100.0</td>\n",
       "      <td>63900.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16530.275255</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21491</th>\n",
       "      <td>60300.0</td>\n",
       "      <td>62100.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5122.499390</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21492</th>\n",
       "      <td>71100.0</td>\n",
       "      <td>71700.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6365.532185</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21493</th>\n",
       "      <td>70200.0</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6484.597135</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21494</th>\n",
       "      <td>62400.0</td>\n",
       "      <td>62700.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2418.677324</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21495</th>\n",
       "      <td>72000.0</td>\n",
       "      <td>72120.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>806.225775</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21496</th>\n",
       "      <td>45000.0</td>\n",
       "      <td>46800.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7317.103252</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21497</th>\n",
       "      <td>63900.0</td>\n",
       "      <td>65400.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8905.054744</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21498</th>\n",
       "      <td>66600.0</td>\n",
       "      <td>68700.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>412.310563</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21499</th>\n",
       "      <td>61200.0</td>\n",
       "      <td>65400.0</td>\n",
       "      <td>4200.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>31196.474160</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21500</th>\n",
       "      <td>58500.0</td>\n",
       "      <td>59400.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4045.985665</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21501</th>\n",
       "      <td>69300.0</td>\n",
       "      <td>70200.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5385.164807</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21502</th>\n",
       "      <td>60900.0</td>\n",
       "      <td>61200.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>412.310563</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21503</th>\n",
       "      <td>72900.0</td>\n",
       "      <td>73500.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2400.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21504</th>\n",
       "      <td>47700.0</td>\n",
       "      <td>49800.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5685.068161</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21505</th>\n",
       "      <td>71100.0</td>\n",
       "      <td>72900.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18917.980865</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21506</th>\n",
       "      <td>54000.0</td>\n",
       "      <td>63000.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>47071.222631</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21507</th>\n",
       "      <td>73800.0</td>\n",
       "      <td>74700.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21508</th>\n",
       "      <td>64800.0</td>\n",
       "      <td>65700.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>447.213595</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21509</th>\n",
       "      <td>63000.0</td>\n",
       "      <td>65100.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4024.922359</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21510</th>\n",
       "      <td>73800.0</td>\n",
       "      <td>75600.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10748.023074</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21511</th>\n",
       "      <td>67800.0</td>\n",
       "      <td>72600.0</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>29138.634148</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21512</th>\n",
       "      <td>90000.0</td>\n",
       "      <td>90300.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21513</th>\n",
       "      <td>64500.0</td>\n",
       "      <td>67800.0</td>\n",
       "      <td>3300.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16737.084573</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21514</th>\n",
       "      <td>61800.0</td>\n",
       "      <td>63000.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2195.449840</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21515</th>\n",
       "      <td>62400.0</td>\n",
       "      <td>68280.0</td>\n",
       "      <td>5880.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>48206.327386</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21516</th>\n",
       "      <td>63900.0</td>\n",
       "      <td>69000.0</td>\n",
       "      <td>5100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>36748.469356</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21517</th>\n",
       "      <td>79500.0</td>\n",
       "      <td>82500.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>36531.219525</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21518</th>\n",
       "      <td>84600.0</td>\n",
       "      <td>86400.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5825.804665</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21519 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Ot       Dt   DUREE  NBTRAJTC          dist  parking\n",
       "0      18300.0  19800.0  1500.0       1.0  20362.956563        1\n",
       "1      34200.0  35340.0  1140.0       1.0   1941.648784        1\n",
       "2      36000.0  37800.0  1800.0       2.0  23648.467181        1\n",
       "3      27000.0  31800.0  4800.0       3.0  35474.638828        1\n",
       "4      27000.0  32400.0  5400.0       3.0  35713.582850        1\n",
       "...        ...      ...     ...       ...           ...      ...\n",
       "21514  61800.0  63000.0  1200.0       1.0   2195.449840        0\n",
       "21515  62400.0  68280.0  5880.0       2.0  48206.327386        0\n",
       "21516  63900.0  69000.0  5100.0       2.0  36748.469356        0\n",
       "21517  79500.0  82500.0  3000.0       2.0  36531.219525        0\n",
       "21518  84600.0  86400.0  1800.0       1.0   5825.804665        0\n",
       "\n",
       "[21519 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data_Train=pd.concat([data1_true,data2_true,data1_false,data2_false],ignore_index=True)\n",
    "Data_Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=pd.DataFrame()\n",
    "for i in range(10):\n",
    "    f1=pd.read_csv('/home/yazi/Documents/stage/data/car_afc_train/train'+str(i)+'_first.csv',delimiter=';',encoding='iso 8859-1',dtype=str)\n",
    "    f2=pd.read_csv('/home/yazi/Documents/stage/data/car_afc_train/train'+str(i)+'_last.csv',delimiter=';',encoding='iso 8859-1',dtype=str)\n",
    "    file=pd.concat([f1,f2,file],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Data_Train.drop('parking', axis=1)\n",
    "y = Data_Train['parking']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.5958114 , -1.66829122, -0.57514726, -0.89708632,  0.93431633],\n",
       "       [-0.77721666, -0.86264201, -0.75408122, -0.89708632, -0.61836558],\n",
       "       [-0.68454556, -0.73510681, -0.42603562,  0.23129809,  1.21124307],\n",
       "       ...,\n",
       "       [ 0.75185653,  0.88241283,  1.21419238,  0.23129809,  2.31540661],\n",
       "       [ 1.55500608,  1.58230113,  0.17041092,  0.23129809,  2.29709521],\n",
       "       [ 1.81757421,  1.78449109, -0.42603562, -0.89708632, -0.29098065]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        1\n",
       "2        1\n",
       "3        1\n",
       "4        1\n",
       "        ..\n",
       "21514    0\n",
       "21515    0\n",
       "21516    0\n",
       "21517    0\n",
       "21518    0\n",
       "Name: parking, Length: 21519, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yazi/anaconda3/envs/tf/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(data=X_train, label=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtest = xgb.DMatrix(data=X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# first way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'max_depth': 2, 'eta': 1, 'objective': 'binary:logistic'}\n",
    "param['nthread'] = 4\n",
    "param['eval_metric'] = 'auc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 2,\n",
       " 'eta': 1,\n",
       " 'objective': 'binary:logistic',\n",
       " 'nthread': 4,\n",
       " 'eval_metric': 'auc'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "evallist = [(dtest, 'eval'), (dtrain, 'train')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_round=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-auc:0.748417\ttrain-auc:0.741415\n",
      "[1]\teval-auc:0.758654\ttrain-auc:0.756009\n",
      "[2]\teval-auc:0.799882\ttrain-auc:0.793166\n",
      "[3]\teval-auc:0.801223\ttrain-auc:0.803686\n",
      "[4]\teval-auc:0.81374\ttrain-auc:0.819883\n",
      "[5]\teval-auc:0.817953\ttrain-auc:0.826423\n",
      "[6]\teval-auc:0.823372\ttrain-auc:0.831112\n",
      "[7]\teval-auc:0.824294\ttrain-auc:0.831703\n",
      "[8]\teval-auc:0.822437\ttrain-auc:0.833985\n",
      "[9]\teval-auc:0.823432\ttrain-auc:0.834932\n"
     ]
    }
   ],
   "source": [
    "bst = xgb.train(param, dtrain, num_round, evallist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = bst.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.18677746, 0.06689963, 0.08307943, ..., 0.01442825, 0.01240223,\n",
       "       0.03323568], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred[ypred> thresh] = 1\n",
    "ypred [ypred <= thresh] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "def show_data(cm, print_res = 0):\n",
    "    tp = cm[1,1]\n",
    "    fn = cm[1,0]\n",
    "    fp = cm[0,1]\n",
    "    tn = cm[0,0]\n",
    "    if print_res == 1:\n",
    "        print('Precision =     {:.3f}'.format(tp/(tp+fp)))\n",
    "        print('Recall (TPR) =  {:.3f}'.format(tp/(tp+fn)))\n",
    "        print('Fallout (FPR) = {:.3e}'.format(fp/(fp+tn)))\n",
    "    return tp/(tp+fp), tp/(tp+fn), fp/(fp+tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision =     0.126\n",
      "Recall (TPR) =  0.806\n",
      "Fallout (FPR) = 2.821e-01\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, ypred)\n",
    "plot_confusion_matrix(cm, ['0', '1'], )\n",
    "pr, tpr, fpr = show_data(cm, print_res = 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# second way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 2,\n",
       " 'eta': 1,\n",
       " 'objective': 'binary:logistic',\n",
       " 'nthread': 4,\n",
       " 'eval_metric': 'auc'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = xgb.XGBModel(**param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.615799\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.615115\n",
      "[1]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.552508\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.551285\n",
      "[2]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.499956\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.498126\n",
      "[3]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.455784\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.453453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yazi/anaconda3/envs/tf/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.418368\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.415528\n",
      "[5]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.386386\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.383061\n",
      "[6]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.358967\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.355184\n",
      "[7]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.335321\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.331073\n",
      "[8]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.314867\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.310268\n",
      "[9]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.297135\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.292105\n",
      "[10]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.281703\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.276297\n",
      "[11]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.268267\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.262469\n",
      "[12]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.256555\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.250362\n",
      "[13]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.246334\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.239818\n",
      "[14]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.237308\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.230445\n",
      "[15]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.229415\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.222183\n",
      "[16]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.222519\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.214904\n",
      "[17]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.216474\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.208555\n",
      "[18]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.211179\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.202981\n",
      "[19]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.206602\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.19815\n",
      "[20]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.202526\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.193768\n",
      "[21]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.198918\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.189928\n",
      "[22]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.195732\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.186559\n",
      "[23]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.192939\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.183555\n",
      "[24]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.190532\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.180951\n",
      "[25]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.188364\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.178634\n",
      "[26]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.18646\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.176622\n",
      "[27]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.184793\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.174763\n",
      "[28]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.183399\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.173265\n",
      "[29]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.182076\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.171796\n",
      "[30]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.180917\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.170568\n",
      "[31]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.179977\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.169605\n",
      "[32]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.179049\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.168608\n",
      "[33]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.178227\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.167721\n",
      "[34]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.177584\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.167065\n",
      "[35]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.176911\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.166406\n",
      "[36]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.176301\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.165742\n",
      "[37]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.175831\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.165192\n",
      "[38]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.175292\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.164651\n",
      "[39]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.174764\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.164072\n",
      "[40]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.174368\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.163788\n",
      "[41]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.173916\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.163372\n",
      "[42]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.173418\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.162873\n",
      "[43]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.173127\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.162606\n",
      "[44]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.172723\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.162208\n",
      "[45]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.172339\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.161857\n",
      "[46]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.17212\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.161645\n",
      "[47]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.171823\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.161327\n",
      "[48]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.171593\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.161069\n",
      "[49]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.171225\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.160804\n",
      "[50]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.170877\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.160484\n",
      "[51]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.170615\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.160221\n",
      "[52]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.170407\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.160135\n",
      "[53]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.170258\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.160009\n",
      "[54]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.169968\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.159783\n",
      "[55]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.169809\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.159768\n",
      "[56]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.169528\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.159588\n",
      "[57]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.169254\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.159372\n",
      "[58]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.169077\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.159351\n",
      "[59]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.168897\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.159167\n",
      "[60]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.168682\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.159061\n",
      "[61]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.168449\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.158885\n",
      "[62]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.168325\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.158701\n",
      "[63]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.168182\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.158624\n",
      "[64]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.168039\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.158471\n",
      "[65]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.16795\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.158393\n",
      "[66]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.167868\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.15833\n",
      "[67]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.167741\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.158229\n",
      "[68]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.167611\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.158217\n",
      "[69]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.167537\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.15815\n",
      "[70]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.167474\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.158099\n",
      "[71]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.167307\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.157906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.16719\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.157863\n",
      "[73]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.166987\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.157723\n",
      "[74]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.166875\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.157636\n",
      "[75]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.166815\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.157635\n",
      "[76]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.166616\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.157603\n",
      "[77]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.166441\tvalidation_1-error:0.047862\tvalidation_1-logloss:0.157545\n",
      "[78]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.166388\tvalidation_1-error:0.04763\tvalidation_1-logloss:0.157507\n",
      "[79]\tvalidation_0-error:0.052396\tvalidation_0-logloss:0.166294\tvalidation_1-error:0.04763\tvalidation_1-logloss:0.157477\n",
      "[80]\tvalidation_0-error:0.052338\tvalidation_0-logloss:0.166214\tvalidation_1-error:0.04763\tvalidation_1-logloss:0.157417\n",
      "[81]\tvalidation_0-error:0.052338\tvalidation_0-logloss:0.166172\tvalidation_1-error:0.04763\tvalidation_1-logloss:0.157424\n",
      "[82]\tvalidation_0-error:0.052338\tvalidation_0-logloss:0.166118\tvalidation_1-error:0.04763\tvalidation_1-logloss:0.1574\n",
      "[83]\tvalidation_0-error:0.052338\tvalidation_0-logloss:0.165961\tvalidation_1-error:0.04763\tvalidation_1-logloss:0.15726\n",
      "[84]\tvalidation_0-error:0.052338\tvalidation_0-logloss:0.165902\tvalidation_1-error:0.04763\tvalidation_1-logloss:0.157213\n",
      "[85]\tvalidation_0-error:0.052338\tvalidation_0-logloss:0.165762\tvalidation_1-error:0.04763\tvalidation_1-logloss:0.157132\n",
      "[86]\tvalidation_0-error:0.052338\tvalidation_0-logloss:0.16561\tvalidation_1-error:0.04763\tvalidation_1-logloss:0.157127\n",
      "[87]\tvalidation_0-error:0.052338\tvalidation_0-logloss:0.165453\tvalidation_1-error:0.04763\tvalidation_1-logloss:0.157068\n",
      "[88]\tvalidation_0-error:0.052338\tvalidation_0-logloss:0.16541\tvalidation_1-error:0.04763\tvalidation_1-logloss:0.157029\n",
      "[89]\tvalidation_0-error:0.052338\tvalidation_0-logloss:0.165315\tvalidation_1-error:0.04763\tvalidation_1-logloss:0.157087\n",
      "[90]\tvalidation_0-error:0.052338\tvalidation_0-logloss:0.165234\tvalidation_1-error:0.04763\tvalidation_1-logloss:0.157025\n",
      "[91]\tvalidation_0-error:0.052338\tvalidation_0-logloss:0.165161\tvalidation_1-error:0.04763\tvalidation_1-logloss:0.157028\n",
      "[92]\tvalidation_0-error:0.052338\tvalidation_0-logloss:0.165127\tvalidation_1-error:0.04763\tvalidation_1-logloss:0.157028\n",
      "[93]\tvalidation_0-error:0.052338\tvalidation_0-logloss:0.165032\tvalidation_1-error:0.04763\tvalidation_1-logloss:0.157023\n",
      "[94]\tvalidation_0-error:0.052338\tvalidation_0-logloss:0.164905\tvalidation_1-error:0.04763\tvalidation_1-logloss:0.156981\n",
      "[95]\tvalidation_0-error:0.052338\tvalidation_0-logloss:0.164868\tvalidation_1-error:0.04763\tvalidation_1-logloss:0.156961\n",
      "[96]\tvalidation_0-error:0.052338\tvalidation_0-logloss:0.164837\tvalidation_1-error:0.04763\tvalidation_1-logloss:0.15695\n",
      "[97]\tvalidation_0-error:0.052338\tvalidation_0-logloss:0.164798\tvalidation_1-error:0.04763\tvalidation_1-logloss:0.156941\n",
      "[98]\tvalidation_0-error:0.052338\tvalidation_0-logloss:0.16467\tvalidation_1-error:0.04763\tvalidation_1-logloss:0.156944\n",
      "[99]\tvalidation_0-error:0.052338\tvalidation_0-logloss:0.164558\tvalidation_1-error:0.04763\tvalidation_1-logloss:0.156953\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBModel(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "         colsample_bynode=1, colsample_bytree=1, eta=1, eval_metric='auc',\n",
       "         gamma=0, importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "         max_depth=2, min_child_weight=1, missing=None, n_estimators=100,\n",
       "         n_jobs=1, nthread=4, objective='binary:logistic', random_state=0,\n",
       "         reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
       "         subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train,\n",
    "        eval_set=[(X_train, y_train), (X_test, y_test)], \n",
    "        eval_metric=['error','logloss'],\n",
    "        verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_result = clf.evals_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Access logloss metric directly from validation_0:\n",
      "[0.615223, 0.551532, 0.498582, 0.45408, 0.416376, 0.384153, 0.35651, 0.332672, 0.312002, 0.294098, 0.27848, 0.264889, 0.253024, 0.242625, 0.233522, 0.225603, 0.21861, 0.212495, 0.207159, 0.202455, 0.198338, 0.194691, 0.191553, 0.188718, 0.186237, 0.184092, 0.182159, 0.180543, 0.179033, 0.177701, 0.176523, 0.175494, 0.174645, 0.173802, 0.173149, 0.172474, 0.171958, 0.171372, 0.170922, 0.170423, 0.169968, 0.169493, 0.169201, 0.168818, 0.168439, 0.168164, 0.16784, 0.1674, 0.167062, 0.166884, 0.166617, 0.166415, 0.166135, 0.165797, 0.165608, 0.165398, 0.165242, 0.164984, 0.164737, 0.164623, 0.164394, 0.164227, 0.164077, 0.16387, 0.163735, 0.163542, 0.163437, 0.163215, 0.163041, 0.162969, 0.162831, 0.162752, 0.162639, 0.162561, 0.162454, 0.162395, 0.16228, 0.162136, 0.161985, 0.161904, 0.161813, 0.161676, 0.161526, 0.16147, 0.161298, 0.161213, 0.161104, 0.161058, 0.160982, 0.160837, 0.160782, 0.160699, 0.160562, 0.160489, 0.160432, 0.160394, 0.160279, 0.160239, 0.160182, 0.160106]\n",
      "\n",
      "Access metrics through a loop:\n",
      "- validation_0\n",
      "   - error\n",
      "      - [0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050595, 0.050595, 0.050595, 0.050595, 0.050595, 0.050595, 0.050537, 0.050537, 0.050537, 0.050537, 0.050537, 0.050537, 0.050537, 0.050537, 0.050537, 0.050537, 0.050537, 0.050537, 0.050537, 0.050479, 0.050479, 0.050479, 0.050479, 0.050479, 0.050479, 0.050479, 0.050479, 0.050479, 0.050479, 0.050421, 0.050421]\n",
      "   - logloss\n",
      "      - [0.615223, 0.551532, 0.498582, 0.45408, 0.416376, 0.384153, 0.35651, 0.332672, 0.312002, 0.294098, 0.27848, 0.264889, 0.253024, 0.242625, 0.233522, 0.225603, 0.21861, 0.212495, 0.207159, 0.202455, 0.198338, 0.194691, 0.191553, 0.188718, 0.186237, 0.184092, 0.182159, 0.180543, 0.179033, 0.177701, 0.176523, 0.175494, 0.174645, 0.173802, 0.173149, 0.172474, 0.171958, 0.171372, 0.170922, 0.170423, 0.169968, 0.169493, 0.169201, 0.168818, 0.168439, 0.168164, 0.16784, 0.1674, 0.167062, 0.166884, 0.166617, 0.166415, 0.166135, 0.165797, 0.165608, 0.165398, 0.165242, 0.164984, 0.164737, 0.164623, 0.164394, 0.164227, 0.164077, 0.16387, 0.163735, 0.163542, 0.163437, 0.163215, 0.163041, 0.162969, 0.162831, 0.162752, 0.162639, 0.162561, 0.162454, 0.162395, 0.16228, 0.162136, 0.161985, 0.161904, 0.161813, 0.161676, 0.161526, 0.16147, 0.161298, 0.161213, 0.161104, 0.161058, 0.160982, 0.160837, 0.160782, 0.160699, 0.160562, 0.160489, 0.160432, 0.160394, 0.160279, 0.160239, 0.160182, 0.160106]\n",
      "- validation_1\n",
      "   - error\n",
      "      - [0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833]\n",
      "   - logloss\n",
      "      - [0.61587, 0.552777, 0.500337, 0.456363, 0.419122, 0.387319, 0.360119, 0.336629, 0.316405, 0.298839, 0.283633, 0.270425, 0.258913, 0.24886, 0.240072, 0.232379, 0.225699, 0.219856, 0.214816, 0.210302, 0.20641, 0.202999, 0.200081, 0.197376, 0.19507, 0.193123, 0.191344, 0.189898, 0.188517, 0.187275, 0.186266, 0.185309, 0.184582, 0.183863, 0.183307, 0.182679, 0.182213, 0.181748, 0.181546, 0.181107, 0.180762, 0.180251, 0.180035, 0.179695, 0.1793, 0.179146, 0.178789, 0.17853, 0.17825, 0.178097, 0.177949, 0.177836, 0.177636, 0.177455, 0.177261, 0.177145, 0.177074, 0.176897, 0.176736, 0.176622, 0.176356, 0.176257, 0.176092, 0.175981, 0.175956, 0.175839, 0.175807, 0.17579, 0.175583, 0.175486, 0.175457, 0.17549, 0.17554, 0.175522, 0.175488, 0.175413, 0.175309, 0.175262, 0.175178, 0.17519, 0.175189, 0.174977, 0.175, 0.17498, 0.174888, 0.174861, 0.174795, 0.174782, 0.174768, 0.17477, 0.174744, 0.174808, 0.174846, 0.174824, 0.174849, 0.174842, 0.174758, 0.174747, 0.174745, 0.174717]\n",
      "\n",
      "Access complete dict:\n",
      "{'validation_0': {'error': [0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050653, 0.050595, 0.050595, 0.050595, 0.050595, 0.050595, 0.050595, 0.050537, 0.050537, 0.050537, 0.050537, 0.050537, 0.050537, 0.050537, 0.050537, 0.050537, 0.050537, 0.050537, 0.050537, 0.050537, 0.050479, 0.050479, 0.050479, 0.050479, 0.050479, 0.050479, 0.050479, 0.050479, 0.050479, 0.050479, 0.050421, 0.050421], 'logloss': [0.615223, 0.551532, 0.498582, 0.45408, 0.416376, 0.384153, 0.35651, 0.332672, 0.312002, 0.294098, 0.27848, 0.264889, 0.253024, 0.242625, 0.233522, 0.225603, 0.21861, 0.212495, 0.207159, 0.202455, 0.198338, 0.194691, 0.191553, 0.188718, 0.186237, 0.184092, 0.182159, 0.180543, 0.179033, 0.177701, 0.176523, 0.175494, 0.174645, 0.173802, 0.173149, 0.172474, 0.171958, 0.171372, 0.170922, 0.170423, 0.169968, 0.169493, 0.169201, 0.168818, 0.168439, 0.168164, 0.16784, 0.1674, 0.167062, 0.166884, 0.166617, 0.166415, 0.166135, 0.165797, 0.165608, 0.165398, 0.165242, 0.164984, 0.164737, 0.164623, 0.164394, 0.164227, 0.164077, 0.16387, 0.163735, 0.163542, 0.163437, 0.163215, 0.163041, 0.162969, 0.162831, 0.162752, 0.162639, 0.162561, 0.162454, 0.162395, 0.16228, 0.162136, 0.161985, 0.161904, 0.161813, 0.161676, 0.161526, 0.16147, 0.161298, 0.161213, 0.161104, 0.161058, 0.160982, 0.160837, 0.160782, 0.160699, 0.160562, 0.160489, 0.160432, 0.160394, 0.160279, 0.160239, 0.160182, 0.160106]}, 'validation_1': {'error': [0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833, 0.054833], 'logloss': [0.61587, 0.552777, 0.500337, 0.456363, 0.419122, 0.387319, 0.360119, 0.336629, 0.316405, 0.298839, 0.283633, 0.270425, 0.258913, 0.24886, 0.240072, 0.232379, 0.225699, 0.219856, 0.214816, 0.210302, 0.20641, 0.202999, 0.200081, 0.197376, 0.19507, 0.193123, 0.191344, 0.189898, 0.188517, 0.187275, 0.186266, 0.185309, 0.184582, 0.183863, 0.183307, 0.182679, 0.182213, 0.181748, 0.181546, 0.181107, 0.180762, 0.180251, 0.180035, 0.179695, 0.1793, 0.179146, 0.178789, 0.17853, 0.17825, 0.178097, 0.177949, 0.177836, 0.177636, 0.177455, 0.177261, 0.177145, 0.177074, 0.176897, 0.176736, 0.176622, 0.176356, 0.176257, 0.176092, 0.175981, 0.175956, 0.175839, 0.175807, 0.17579, 0.175583, 0.175486, 0.175457, 0.17549, 0.17554, 0.175522, 0.175488, 0.175413, 0.175309, 0.175262, 0.175178, 0.17519, 0.175189, 0.174977, 0.175, 0.17498, 0.174888, 0.174861, 0.174795, 0.174782, 0.174768, 0.17477, 0.174744, 0.174808, 0.174846, 0.174824, 0.174849, 0.174842, 0.174758, 0.174747, 0.174745, 0.174717]}}\n"
     ]
    }
   ],
   "source": [
    "print('Access logloss metric directly from validation_0:')\n",
    "print(evals_result['validation_0']['logloss'])\n",
    "\n",
    "print('')\n",
    "print('Access metrics through a loop:')\n",
    "\n",
    "for e_name, e_mtrs in evals_result.items():\n",
    "    print('- {}'.format(e_name))\n",
    "    for e_mtr_name, e_mtr_vals in e_mtrs.items():\n",
    "        print('   - {}'.format(e_mtr_name))\n",
    "        print('      - {}'.format(e_mtr_vals))\n",
    "        \n",
    " \n",
    "print('')\n",
    "print('Access complete dict:')\n",
    "print(evals_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.13545404, 0.0698577 , 0.04456442, ..., 0.01691139, 0.01098339,\n",
       "       0.04543292], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 95.24%\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results =clf.evals_result()\n",
    "epochs = len(results['validation_0']['error'])\n",
    "x_axis = range(0, epochs)\n",
    "# plot log loss\n",
    "fig, ax = pyplot.subplots()\n",
    "ax.plot(x_axis, results['validation_0']['logloss'], label='Train')\n",
    "ax.plot(x_axis, results['validation_1']['logloss'], label='Test')\n",
    "ax.legend()\n",
    "pyplot.ylabel('Log Loss')\n",
    "pyplot.title('XGBoost Log Loss')\n",
    "pyplot.show()\n",
    "# plot classification error\n",
    "fig, ax = pyplot.subplots()\n",
    "ax.plot(x_axis, results['validation_0']['error'], label='Train')\n",
    "ax.plot(x_axis, results['validation_1']['error'], label='Test')\n",
    "ax.legend()\n",
    "pyplot.ylabel('Classification Error')\n",
    "pyplot.title('XGBoost Classification Error')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " ...]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5118     0\n",
       "14913    0\n",
       "13236    0\n",
       "13543    0\n",
       "6088     0\n",
       "        ..\n",
       "20234    0\n",
       "18141    0\n",
       "8946     0\n",
       "15363    0\n",
       "21213    0\n",
       "Name: parking, Length: 4304, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4098    0]\n",
      " [ 205    1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98      4098\n",
      "           1       1.00      0.00      0.01       206\n",
      "\n",
      "    accuracy                           0.95      4304\n",
      "   macro avg       0.98      0.50      0.49      4304\n",
      "weighted avg       0.95      0.95      0.93      4304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# third way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "param3={'max_depth': 12,\n",
    "         'subsample': 0.33,\n",
    "         'objective': 'binary:logistic',\n",
    "         'n_estimators':300,\n",
    "         'nthread': 4,\n",
    "         'learning_rate':0.06,\n",
    "         'scale_pos_weight':18}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=xgb.XGBClassifier(**param3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "989      1\n",
       "4189     0\n",
       "10139    0\n",
       "11770    0\n",
       "12669    0\n",
       "        ..\n",
       "12523    0\n",
       "11052    0\n",
       "2950     0\n",
       "4939     0\n",
       "7351     0\n",
       "Name: parking, Length: 17215, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-error:0.146384\tvalidation_0-logloss:0.654622\tvalidation_1-error:0.172165\tvalidation_1-logloss:0.657196\n",
      "Multiple eval metrics have been passed: 'validation_1-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-logloss hasn't improved in 15 rounds.\n",
      "[1]\tvalidation_0-error:0.105025\tvalidation_0-logloss:0.617972\tvalidation_1-error:0.126626\tvalidation_1-logloss:0.623353\n",
      "[2]\tvalidation_0-error:0.104386\tvalidation_0-logloss:0.585523\tvalidation_1-error:0.133132\tvalidation_1-logloss:0.592816\n",
      "[3]\tvalidation_0-error:0.101481\tvalidation_0-logloss:0.558142\tvalidation_1-error:0.122909\tvalidation_1-logloss:0.567042\n",
      "[4]\tvalidation_0-error:0.100668\tvalidation_0-logloss:0.532116\tvalidation_1-error:0.131041\tvalidation_1-logloss:0.543253\n",
      "[5]\tvalidation_0-error:0.097415\tvalidation_0-logloss:0.509202\tvalidation_1-error:0.125\tvalidation_1-logloss:0.522067\n",
      "[6]\tvalidation_0-error:0.099971\tvalidation_0-logloss:0.488424\tvalidation_1-error:0.127091\tvalidation_1-logloss:0.502801\n",
      "[7]\tvalidation_0-error:0.099797\tvalidation_0-logloss:0.469068\tvalidation_1-error:0.125697\tvalidation_1-logloss:0.485079\n",
      "[8]\tvalidation_0-error:0.098635\tvalidation_0-logloss:0.451762\tvalidation_1-error:0.124768\tvalidation_1-logloss:0.468732\n",
      "[9]\tvalidation_0-error:0.102875\tvalidation_0-logloss:0.436261\tvalidation_1-error:0.128485\tvalidation_1-logloss:0.454263\n",
      "[10]\tvalidation_0-error:0.100436\tvalidation_0-logloss:0.420007\tvalidation_1-error:0.127091\tvalidation_1-logloss:0.439452\n",
      "[11]\tvalidation_0-error:0.104095\tvalidation_0-logloss:0.408411\tvalidation_1-error:0.130809\tvalidation_1-logloss:0.428902\n",
      "[12]\tvalidation_0-error:0.105896\tvalidation_0-logloss:0.396739\tvalidation_1-error:0.132435\tvalidation_1-logloss:0.418281\n",
      "[13]\tvalidation_0-error:0.105083\tvalidation_0-logloss:0.383725\tvalidation_1-error:0.132435\tvalidation_1-logloss:0.406312\n",
      "[14]\tvalidation_0-error:0.109207\tvalidation_0-logloss:0.375343\tvalidation_1-error:0.135688\tvalidation_1-logloss:0.399161\n",
      "[15]\tvalidation_0-error:0.111763\tvalidation_0-logloss:0.365674\tvalidation_1-error:0.138011\tvalidation_1-logloss:0.391386\n",
      "[16]\tvalidation_0-error:0.111763\tvalidation_0-logloss:0.356171\tvalidation_1-error:0.138011\tvalidation_1-logloss:0.383104\n",
      "[17]\tvalidation_0-error:0.108626\tvalidation_0-logloss:0.34504\tvalidation_1-error:0.135223\tvalidation_1-logloss:0.373243\n",
      "[18]\tvalidation_0-error:0.110659\tvalidation_0-logloss:0.338514\tvalidation_1-error:0.137082\tvalidation_1-logloss:0.367588\n",
      "[19]\tvalidation_0-error:0.107523\tvalidation_0-logloss:0.329534\tvalidation_1-error:0.134294\tvalidation_1-logloss:0.359721\n",
      "[20]\tvalidation_0-error:0.109091\tvalidation_0-logloss:0.324171\tvalidation_1-error:0.135223\tvalidation_1-logloss:0.35539\n",
      "[21]\tvalidation_0-error:0.1088\tvalidation_0-logloss:0.317446\tvalidation_1-error:0.135688\tvalidation_1-logloss:0.349684\n",
      "[22]\tvalidation_0-error:0.109904\tvalidation_0-logloss:0.312917\tvalidation_1-error:0.138476\tvalidation_1-logloss:0.346208\n",
      "[23]\tvalidation_0-error:0.108626\tvalidation_0-logloss:0.307724\tvalidation_1-error:0.137314\tvalidation_1-logloss:0.342047\n",
      "[24]\tvalidation_0-error:0.110195\tvalidation_0-logloss:0.304547\tvalidation_1-error:0.138011\tvalidation_1-logloss:0.339851\n",
      "[25]\tvalidation_0-error:0.112402\tvalidation_0-logloss:0.300375\tvalidation_1-error:0.140799\tvalidation_1-logloss:0.336347\n",
      "[26]\tvalidation_0-error:0.108626\tvalidation_0-logloss:0.292253\tvalidation_1-error:0.138941\tvalidation_1-logloss:0.32983\n",
      "[27]\tvalidation_0-error:0.106245\tvalidation_0-logloss:0.285007\tvalidation_1-error:0.138708\tvalidation_1-logloss:0.324471\n",
      "[28]\tvalidation_0-error:0.10729\tvalidation_0-logloss:0.282101\tvalidation_1-error:0.138941\tvalidation_1-logloss:0.322303\n",
      "[29]\tvalidation_0-error:0.10485\tvalidation_0-logloss:0.276032\tvalidation_1-error:0.137082\tvalidation_1-logloss:0.317298\n",
      "[30]\tvalidation_0-error:0.103631\tvalidation_0-logloss:0.271336\tvalidation_1-error:0.134991\tvalidation_1-logloss:0.313454\n",
      "[31]\tvalidation_0-error:0.103166\tvalidation_0-logloss:0.26769\tvalidation_1-error:0.132667\tvalidation_1-logloss:0.310947\n",
      "[32]\tvalidation_0-error:0.102701\tvalidation_0-logloss:0.265123\tvalidation_1-error:0.13592\tvalidation_1-logloss:0.309665\n",
      "[33]\tvalidation_0-error:0.103921\tvalidation_0-logloss:0.262861\tvalidation_1-error:0.136152\tvalidation_1-logloss:0.307861\n",
      "[34]\tvalidation_0-error:0.105722\tvalidation_0-logloss:0.261324\tvalidation_1-error:0.137546\tvalidation_1-logloss:0.306922\n",
      "[35]\tvalidation_0-error:0.104037\tvalidation_0-logloss:0.257899\tvalidation_1-error:0.136385\tvalidation_1-logloss:0.304274\n",
      "[36]\tvalidation_0-error:0.105489\tvalidation_0-logloss:0.256619\tvalidation_1-error:0.136849\tvalidation_1-logloss:0.303172\n",
      "[37]\tvalidation_0-error:0.106419\tvalidation_0-logloss:0.255651\tvalidation_1-error:0.137314\tvalidation_1-logloss:0.30295\n",
      "[38]\tvalidation_0-error:0.107232\tvalidation_0-logloss:0.254999\tvalidation_1-error:0.139405\tvalidation_1-logloss:0.303141\n",
      "[39]\tvalidation_0-error:0.109614\tvalidation_0-logloss:0.25536\tvalidation_1-error:0.141496\tvalidation_1-logloss:0.304002\n",
      "[40]\tvalidation_0-error:0.108626\tvalidation_0-logloss:0.252684\tvalidation_1-error:0.14289\tvalidation_1-logloss:0.302506\n",
      "[41]\tvalidation_0-error:0.109614\tvalidation_0-logloss:0.251723\tvalidation_1-error:0.14289\tvalidation_1-logloss:0.302041\n",
      "[42]\tvalidation_0-error:0.110195\tvalidation_0-logloss:0.250209\tvalidation_1-error:0.143355\tvalidation_1-logloss:0.301051\n",
      "[43]\tvalidation_0-error:0.108917\tvalidation_0-logloss:0.247879\tvalidation_1-error:0.144749\tvalidation_1-logloss:0.29908\n",
      "[44]\tvalidation_0-error:0.110717\tvalidation_0-logloss:0.250006\tvalidation_1-error:0.144981\tvalidation_1-logloss:0.30185\n",
      "[45]\tvalidation_0-error:0.110717\tvalidation_0-logloss:0.249589\tvalidation_1-error:0.144981\tvalidation_1-logloss:0.302075\n",
      "[46]\tvalidation_0-error:0.109614\tvalidation_0-logloss:0.246869\tvalidation_1-error:0.14289\tvalidation_1-logloss:0.300439\n",
      "[47]\tvalidation_0-error:0.109846\tvalidation_0-logloss:0.248139\tvalidation_1-error:0.143123\tvalidation_1-logloss:0.302611\n",
      "[48]\tvalidation_0-error:0.107987\tvalidation_0-logloss:0.245847\tvalidation_1-error:0.142426\tvalidation_1-logloss:0.301326\n",
      "[49]\tvalidation_0-error:0.107348\tvalidation_0-logloss:0.244557\tvalidation_1-error:0.142193\tvalidation_1-logloss:0.300394\n",
      "[50]\tvalidation_0-error:0.108103\tvalidation_0-logloss:0.244187\tvalidation_1-error:0.141961\tvalidation_1-logloss:0.300197\n",
      "[51]\tvalidation_0-error:0.107581\tvalidation_0-logloss:0.242857\tvalidation_1-error:0.14289\tvalidation_1-logloss:0.299659\n",
      "[52]\tvalidation_0-error:0.106825\tvalidation_0-logloss:0.240435\tvalidation_1-error:0.141264\tvalidation_1-logloss:0.29826\n",
      "[53]\tvalidation_0-error:0.105722\tvalidation_0-logloss:0.238188\tvalidation_1-error:0.140799\tvalidation_1-logloss:0.296737\n",
      "[54]\tvalidation_0-error:0.107232\tvalidation_0-logloss:0.238513\tvalidation_1-error:0.140567\tvalidation_1-logloss:0.29792\n",
      "[55]\tvalidation_0-error:0.107174\tvalidation_0-logloss:0.236602\tvalidation_1-error:0.141496\tvalidation_1-logloss:0.297443\n",
      "[56]\tvalidation_0-error:0.105257\tvalidation_0-logloss:0.233904\tvalidation_1-error:0.139173\tvalidation_1-logloss:0.295348\n",
      "[57]\tvalidation_0-error:0.104618\tvalidation_0-logloss:0.233282\tvalidation_1-error:0.138243\tvalidation_1-logloss:0.295308\n",
      "[58]\tvalidation_0-error:0.105025\tvalidation_0-logloss:0.231599\tvalidation_1-error:0.140335\tvalidation_1-logloss:0.29433\n",
      "[59]\tvalidation_0-error:0.105199\tvalidation_0-logloss:0.23192\tvalidation_1-error:0.141264\tvalidation_1-logloss:0.295194\n",
      "[60]\tvalidation_0-error:0.103747\tvalidation_0-logloss:0.22934\tvalidation_1-error:0.140799\tvalidation_1-logloss:0.293747\n",
      "[61]\tvalidation_0-error:0.102353\tvalidation_0-logloss:0.226324\tvalidation_1-error:0.137779\tvalidation_1-logloss:0.291691\n",
      "[62]\tvalidation_0-error:0.102933\tvalidation_0-logloss:0.226645\tvalidation_1-error:0.139405\tvalidation_1-logloss:0.292698\n",
      "[63]\tvalidation_0-error:0.10456\tvalidation_0-logloss:0.228083\tvalidation_1-error:0.141496\tvalidation_1-logloss:0.294624\n",
      "[64]\tvalidation_0-error:0.105489\tvalidation_0-logloss:0.229486\tvalidation_1-error:0.141961\tvalidation_1-logloss:0.296729\n",
      "[65]\tvalidation_0-error:0.104037\tvalidation_0-logloss:0.227767\tvalidation_1-error:0.141264\tvalidation_1-logloss:0.295608\n",
      "[66]\tvalidation_0-error:0.102004\tvalidation_0-logloss:0.225422\tvalidation_1-error:0.140335\tvalidation_1-logloss:0.294021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[67]\tvalidation_0-error:0.099274\tvalidation_0-logloss:0.220522\tvalidation_1-error:0.139173\tvalidation_1-logloss:0.289953\n",
      "[68]\tvalidation_0-error:0.09817\tvalidation_0-logloss:0.219348\tvalidation_1-error:0.139173\tvalidation_1-logloss:0.289596\n",
      "[69]\tvalidation_0-error:0.097705\tvalidation_0-logloss:0.2184\tvalidation_1-error:0.138941\tvalidation_1-logloss:0.288971\n",
      "[70]\tvalidation_0-error:0.097415\tvalidation_0-logloss:0.217361\tvalidation_1-error:0.138243\tvalidation_1-logloss:0.288814\n",
      "[71]\tvalidation_0-error:0.097415\tvalidation_0-logloss:0.217236\tvalidation_1-error:0.139173\tvalidation_1-logloss:0.288914\n",
      "[72]\tvalidation_0-error:0.096602\tvalidation_0-logloss:0.214483\tvalidation_1-error:0.137082\tvalidation_1-logloss:0.286658\n",
      "[73]\tvalidation_0-error:0.095905\tvalidation_0-logloss:0.212985\tvalidation_1-error:0.137779\tvalidation_1-logloss:0.285951\n",
      "[74]\tvalidation_0-error:0.09695\tvalidation_0-logloss:0.21355\tvalidation_1-error:0.137546\tvalidation_1-logloss:0.286841\n",
      "[75]\tvalidation_0-error:0.094336\tvalidation_0-logloss:0.211484\tvalidation_1-error:0.137082\tvalidation_1-logloss:0.285676\n",
      "[76]\tvalidation_0-error:0.094511\tvalidation_0-logloss:0.209703\tvalidation_1-error:0.136152\tvalidation_1-logloss:0.28439\n",
      "[77]\tvalidation_0-error:0.09544\tvalidation_0-logloss:0.209949\tvalidation_1-error:0.13592\tvalidation_1-logloss:0.284857\n",
      "[78]\tvalidation_0-error:0.092768\tvalidation_0-logloss:0.206571\tvalidation_1-error:0.133829\tvalidation_1-logloss:0.28277\n",
      "[79]\tvalidation_0-error:0.092768\tvalidation_0-logloss:0.207388\tvalidation_1-error:0.135688\tvalidation_1-logloss:0.284163\n",
      "[80]\tvalidation_0-error:0.093233\tvalidation_0-logloss:0.207222\tvalidation_1-error:0.134294\tvalidation_1-logloss:0.284432\n",
      "[81]\tvalidation_0-error:0.090502\tvalidation_0-logloss:0.202772\tvalidation_1-error:0.132435\tvalidation_1-logloss:0.280412\n",
      "[82]\tvalidation_0-error:0.08754\tvalidation_0-logloss:0.200418\tvalidation_1-error:0.130576\tvalidation_1-logloss:0.278584\n",
      "[83]\tvalidation_0-error:0.086959\tvalidation_0-logloss:0.199618\tvalidation_1-error:0.131041\tvalidation_1-logloss:0.27824\n",
      "[84]\tvalidation_0-error:0.086669\tvalidation_0-logloss:0.199853\tvalidation_1-error:0.133132\tvalidation_1-logloss:0.278342\n",
      "[85]\tvalidation_0-error:0.08603\tvalidation_0-logloss:0.198121\tvalidation_1-error:0.131738\tvalidation_1-logloss:0.277332\n",
      "[86]\tvalidation_0-error:0.08388\tvalidation_0-logloss:0.195545\tvalidation_1-error:0.129182\tvalidation_1-logloss:0.275046\n",
      "[87]\tvalidation_0-error:0.082719\tvalidation_0-logloss:0.193589\tvalidation_1-error:0.125697\tvalidation_1-logloss:0.273399\n",
      "[88]\tvalidation_0-error:0.081789\tvalidation_0-logloss:0.193028\tvalidation_1-error:0.125697\tvalidation_1-logloss:0.273704\n",
      "[89]\tvalidation_0-error:0.081905\tvalidation_0-logloss:0.192598\tvalidation_1-error:0.125929\tvalidation_1-logloss:0.273991\n",
      "[90]\tvalidation_0-error:0.079291\tvalidation_0-logloss:0.190423\tvalidation_1-error:0.124535\tvalidation_1-logloss:0.27289\n",
      "[91]\tvalidation_0-error:0.078478\tvalidation_0-logloss:0.188544\tvalidation_1-error:0.122677\tvalidation_1-logloss:0.271798\n",
      "[92]\tvalidation_0-error:0.077374\tvalidation_0-logloss:0.187856\tvalidation_1-error:0.123374\tvalidation_1-logloss:0.271801\n",
      "[93]\tvalidation_0-error:0.076619\tvalidation_0-logloss:0.185971\tvalidation_1-error:0.121747\tvalidation_1-logloss:0.270539\n",
      "[94]\tvalidation_0-error:0.076793\tvalidation_0-logloss:0.186312\tvalidation_1-error:0.123374\tvalidation_1-logloss:0.271368\n",
      "[95]\tvalidation_0-error:0.0772\tvalidation_0-logloss:0.186421\tvalidation_1-error:0.123141\tvalidation_1-logloss:0.272388\n",
      "[96]\tvalidation_0-error:0.075051\tvalidation_0-logloss:0.184722\tvalidation_1-error:0.121747\tvalidation_1-logloss:0.271607\n",
      "[97]\tvalidation_0-error:0.073599\tvalidation_0-logloss:0.182962\tvalidation_1-error:0.121515\tvalidation_1-logloss:0.270336\n",
      "[98]\tvalidation_0-error:0.075109\tvalidation_0-logloss:0.183312\tvalidation_1-error:0.12198\tvalidation_1-logloss:0.270851\n",
      "[99]\tvalidation_0-error:0.074121\tvalidation_0-logloss:0.182101\tvalidation_1-error:0.121283\tvalidation_1-logloss:0.270398\n",
      "[100]\tvalidation_0-error:0.074296\tvalidation_0-logloss:0.181121\tvalidation_1-error:0.121283\tvalidation_1-logloss:0.270156\n",
      "[101]\tvalidation_0-error:0.07447\tvalidation_0-logloss:0.182202\tvalidation_1-error:0.123141\tvalidation_1-logloss:0.271475\n",
      "[102]\tvalidation_0-error:0.074818\tvalidation_0-logloss:0.181317\tvalidation_1-error:0.122677\tvalidation_1-logloss:0.270766\n",
      "[103]\tvalidation_0-error:0.073308\tvalidation_0-logloss:0.18043\tvalidation_1-error:0.121515\tvalidation_1-logloss:0.270478\n",
      "[104]\tvalidation_0-error:0.072204\tvalidation_0-logloss:0.179218\tvalidation_1-error:0.122444\tvalidation_1-logloss:0.269403\n",
      "[105]\tvalidation_0-error:0.073018\tvalidation_0-logloss:0.179061\tvalidation_1-error:0.123838\tvalidation_1-logloss:0.269609\n",
      "[106]\tvalidation_0-error:0.07081\tvalidation_0-logloss:0.177792\tvalidation_1-error:0.120121\tvalidation_1-logloss:0.26924\n",
      "[107]\tvalidation_0-error:0.071391\tvalidation_0-logloss:0.177816\tvalidation_1-error:0.120818\tvalidation_1-logloss:0.270007\n",
      "[108]\tvalidation_0-error:0.071624\tvalidation_0-logloss:0.178058\tvalidation_1-error:0.12105\tvalidation_1-logloss:0.270092\n",
      "[109]\tvalidation_0-error:0.072321\tvalidation_0-logloss:0.177835\tvalidation_1-error:0.122212\tvalidation_1-logloss:0.270652\n",
      "[110]\tvalidation_0-error:0.072553\tvalidation_0-logloss:0.178383\tvalidation_1-error:0.123606\tvalidation_1-logloss:0.272044\n",
      "[111]\tvalidation_0-error:0.071856\tvalidation_0-logloss:0.177759\tvalidation_1-error:0.122677\tvalidation_1-logloss:0.271825\n",
      "[112]\tvalidation_0-error:0.071682\tvalidation_0-logloss:0.177041\tvalidation_1-error:0.121515\tvalidation_1-logloss:0.271839\n",
      "[113]\tvalidation_0-error:0.071914\tvalidation_0-logloss:0.177902\tvalidation_1-error:0.121515\tvalidation_1-logloss:0.273239\n",
      "[114]\tvalidation_0-error:0.071159\tvalidation_0-logloss:0.177103\tvalidation_1-error:0.119656\tvalidation_1-logloss:0.272849\n",
      "[115]\tvalidation_0-error:0.07174\tvalidation_0-logloss:0.178868\tvalidation_1-error:0.121515\tvalidation_1-logloss:0.274995\n",
      "[116]\tvalidation_0-error:0.073076\tvalidation_0-logloss:0.180935\tvalidation_1-error:0.122444\tvalidation_1-logloss:0.277027\n",
      "[117]\tvalidation_0-error:0.071217\tvalidation_0-logloss:0.179846\tvalidation_1-error:0.121283\tvalidation_1-logloss:0.276766\n",
      "[118]\tvalidation_0-error:0.071507\tvalidation_0-logloss:0.179494\tvalidation_1-error:0.121283\tvalidation_1-logloss:0.27685\n",
      "[119]\tvalidation_0-error:0.071043\tvalidation_0-logloss:0.178499\tvalidation_1-error:0.120818\tvalidation_1-logloss:0.276415\n",
      "[120]\tvalidation_0-error:0.070171\tvalidation_0-logloss:0.178035\tvalidation_1-error:0.120121\tvalidation_1-logloss:0.275733\n",
      "[121]\tvalidation_0-error:0.071159\tvalidation_0-logloss:0.179198\tvalidation_1-error:0.120586\tvalidation_1-logloss:0.277344\n",
      "Stopping. Best iteration:\n",
      "[106]\tvalidation_0-error:0.07081\tvalidation_0-logloss:0.177792\tvalidation_1-error:0.120121\tvalidation_1-logloss:0.26924\n",
      "\n",
      "CPU times: user 12.8 s, sys: 1.03 s, total: 13.8 s\n",
      "Wall time: 6.76 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.06, max_delta_step=0, max_depth=12,\n",
       "              min_child_weight=1, missing=None, n_estimators=300, n_jobs=1,\n",
       "              nthread=4, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=18, seed=None,\n",
       "              silent=None, subsample=0.33, verbosity=1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "clf.fit(X_train, y_train,\n",
    "        early_stopping_rounds=15,\n",
    "        eval_set=[(X_train, y_train), (X_test, y_test)], \n",
    "        eval_metric=['error','logloss'],\n",
    "        verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [round(value) for value in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.73%\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results =clf.evals_result()\n",
    "epochs = len(results['validation_0']['error'])\n",
    "x_axis = range(0, epochs)\n",
    "# plot log loss\n",
    "fig, ax = pyplot.subplots()\n",
    "ax.plot(x_axis, results['validation_0']['logloss'], label='Train')\n",
    "ax.plot(x_axis, results['validation_1']['logloss'], label='Test')\n",
    "ax.legend()\n",
    "pyplot.ylabel('Log Loss')\n",
    "pyplot.title('XGBoost Log Loss')\n",
    "pyplot.show()\n",
    "# plot classification error\n",
    "fig, ax = pyplot.subplots()\n",
    "ax.plot(x_axis, results['validation_0']['error'], label='Train')\n",
    "ax.plot(x_axis, results['validation_1']['error'], label='Test')\n",
    "ax.legend()\n",
    "pyplot.ylabel('Classification Error')\n",
    "pyplot.title('XGBoost Classification Error')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3754  314]\n",
      " [ 171   65]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94      4068\n",
      "           1       0.17      0.28      0.21       236\n",
      "\n",
      "    accuracy                           0.89      4304\n",
      "   macro avg       0.56      0.60      0.58      4304\n",
      "weighted avg       0.91      0.89      0.90      4304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 3925, 1: 379})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections.Counter(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sm=SMOTE(random_state=42)\n",
    "X_res,y_res=sm.fit_resample(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X_res,y_res,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "param4={'max_depth': 12,\n",
    "         'subsample': 0.33,\n",
    "         'objective': 'binary:logistic',\n",
    "         'n_estimators':282,\n",
    "         'nthread': 4,\n",
    "         'learning_rate':0.13,\n",
    "         'scale_pos_weight':18}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=xgb.XGBClassifier(**param4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-error:0.222219\tvalidation_0-logloss:0.632471\tvalidation_1-error:0.225107\tvalidation_1-logloss:0.632812\n",
      "Multiple eval metrics have been passed: 'validation_1-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-logloss hasn't improved in 15 rounds.\n",
      "[1]\tvalidation_0-error:0.211961\tvalidation_0-logloss:0.583306\tvalidation_1-error:0.21286\tvalidation_1-logloss:0.58436\n",
      "[2]\tvalidation_0-error:0.213951\tvalidation_0-logloss:0.551369\tvalidation_1-error:0.218494\tvalidation_1-logloss:0.553881\n",
      "[3]\tvalidation_0-error:0.205346\tvalidation_0-logloss:0.524186\tvalidation_1-error:0.209553\tvalidation_1-logloss:0.52724\n",
      "[4]\tvalidation_0-error:0.203264\tvalidation_0-logloss:0.502062\tvalidation_1-error:0.209553\tvalidation_1-logloss:0.505415\n",
      "[5]\tvalidation_0-error:0.203264\tvalidation_0-logloss:0.489378\tvalidation_1-error:0.206124\tvalidation_1-logloss:0.492653\n",
      "[6]\tvalidation_0-error:0.201366\tvalidation_0-logloss:0.477537\tvalidation_1-error:0.204042\tvalidation_1-logloss:0.481242\n",
      "[7]\tvalidation_0-error:0.20011\tvalidation_0-logloss:0.463179\tvalidation_1-error:0.203797\tvalidation_1-logloss:0.468039\n",
      "[8]\tvalidation_0-error:0.197477\tvalidation_0-logloss:0.452509\tvalidation_1-error:0.202082\tvalidation_1-logloss:0.459117\n",
      "[9]\tvalidation_0-error:0.193955\tvalidation_0-logloss:0.444775\tvalidation_1-error:0.197795\tvalidation_1-logloss:0.451657\n",
      "[10]\tvalidation_0-error:0.193159\tvalidation_0-logloss:0.440763\tvalidation_1-error:0.197918\tvalidation_1-logloss:0.448087\n",
      "[11]\tvalidation_0-error:0.190985\tvalidation_0-logloss:0.433025\tvalidation_1-error:0.195713\tvalidation_1-logloss:0.442656\n",
      "[12]\tvalidation_0-error:0.189485\tvalidation_0-logloss:0.429524\tvalidation_1-error:0.193754\tvalidation_1-logloss:0.440422\n",
      "[13]\tvalidation_0-error:0.188719\tvalidation_0-logloss:0.427715\tvalidation_1-error:0.193999\tvalidation_1-logloss:0.440433\n",
      "[14]\tvalidation_0-error:0.185718\tvalidation_0-logloss:0.419771\tvalidation_1-error:0.19008\tvalidation_1-logloss:0.433947\n",
      "[15]\tvalidation_0-error:0.183422\tvalidation_0-logloss:0.419296\tvalidation_1-error:0.188242\tvalidation_1-logloss:0.433812\n",
      "[16]\tvalidation_0-error:0.181523\tvalidation_0-logloss:0.414503\tvalidation_1-error:0.188242\tvalidation_1-logloss:0.429102\n",
      "[17]\tvalidation_0-error:0.181799\tvalidation_0-logloss:0.413551\tvalidation_1-error:0.18665\tvalidation_1-logloss:0.429649\n",
      "[18]\tvalidation_0-error:0.181554\tvalidation_0-logloss:0.414756\tvalidation_1-error:0.187385\tvalidation_1-logloss:0.431551\n",
      "[19]\tvalidation_0-error:0.17843\tvalidation_0-logloss:0.40997\tvalidation_1-error:0.185181\tvalidation_1-logloss:0.427491\n",
      "[20]\tvalidation_0-error:0.177206\tvalidation_0-logloss:0.40763\tvalidation_1-error:0.183588\tvalidation_1-logloss:0.427342\n",
      "[21]\tvalidation_0-error:0.174909\tvalidation_0-logloss:0.40481\tvalidation_1-error:0.181629\tvalidation_1-logloss:0.425905\n",
      "[22]\tvalidation_0-error:0.175307\tvalidation_0-logloss:0.40272\tvalidation_1-error:0.182609\tvalidation_1-logloss:0.4246\n",
      "[23]\tvalidation_0-error:0.175184\tvalidation_0-logloss:0.403485\tvalidation_1-error:0.182364\tvalidation_1-logloss:0.427505\n",
      "[24]\tvalidation_0-error:0.175093\tvalidation_0-logloss:0.401459\tvalidation_1-error:0.182609\tvalidation_1-logloss:0.427401\n",
      "[25]\tvalidation_0-error:0.17151\tvalidation_0-logloss:0.393022\tvalidation_1-error:0.178934\tvalidation_1-logloss:0.421333\n",
      "[26]\tvalidation_0-error:0.169091\tvalidation_0-logloss:0.387009\tvalidation_1-error:0.177097\tvalidation_1-logloss:0.416307\n",
      "[27]\tvalidation_0-error:0.168019\tvalidation_0-logloss:0.383417\tvalidation_1-error:0.175383\tvalidation_1-logloss:0.413815\n",
      "[28]\tvalidation_0-error:0.168448\tvalidation_0-logloss:0.385254\tvalidation_1-error:0.175873\tvalidation_1-logloss:0.416586\n",
      "[29]\tvalidation_0-error:0.1656\tvalidation_0-logloss:0.380272\tvalidation_1-error:0.17428\tvalidation_1-logloss:0.414397\n",
      "[30]\tvalidation_0-error:0.162569\tvalidation_0-logloss:0.37442\tvalidation_1-error:0.171831\tvalidation_1-logloss:0.408746\n",
      "[31]\tvalidation_0-error:0.159874\tvalidation_0-logloss:0.367695\tvalidation_1-error:0.168769\tvalidation_1-logloss:0.402343\n",
      "[32]\tvalidation_0-error:0.158741\tvalidation_0-logloss:0.36698\tvalidation_1-error:0.168157\tvalidation_1-logloss:0.401906\n",
      "[33]\tvalidation_0-error:0.156046\tvalidation_0-logloss:0.358031\tvalidation_1-error:0.16583\tvalidation_1-logloss:0.393874\n",
      "[34]\tvalidation_0-error:0.153566\tvalidation_0-logloss:0.352446\tvalidation_1-error:0.161911\tvalidation_1-logloss:0.38968\n",
      "[35]\tvalidation_0-error:0.151851\tvalidation_0-logloss:0.349208\tvalidation_1-error:0.161543\tvalidation_1-logloss:0.38775\n",
      "[36]\tvalidation_0-error:0.1513\tvalidation_0-logloss:0.347917\tvalidation_1-error:0.160808\tvalidation_1-logloss:0.387286\n",
      "[37]\tvalidation_0-error:0.148391\tvalidation_0-logloss:0.341987\tvalidation_1-error:0.158604\tvalidation_1-logloss:0.382855\n",
      "[38]\tvalidation_0-error:0.14738\tvalidation_0-logloss:0.339789\tvalidation_1-error:0.157502\tvalidation_1-logloss:0.381694\n",
      "[39]\tvalidation_0-error:0.147044\tvalidation_0-logloss:0.339328\tvalidation_1-error:0.157134\tvalidation_1-logloss:0.381627\n",
      "[40]\tvalidation_0-error:0.142022\tvalidation_0-logloss:0.32933\tvalidation_1-error:0.156399\tvalidation_1-logloss:0.372646\n",
      "[41]\tvalidation_0-error:0.141287\tvalidation_0-logloss:0.329294\tvalidation_1-error:0.156277\tvalidation_1-logloss:0.372913\n",
      "[42]\tvalidation_0-error:0.140368\tvalidation_0-logloss:0.327424\tvalidation_1-error:0.154317\tvalidation_1-logloss:0.371792\n",
      "[43]\tvalidation_0-error:0.140276\tvalidation_0-logloss:0.327509\tvalidation_1-error:0.15493\tvalidation_1-logloss:0.37239\n",
      "[44]\tvalidation_0-error:0.138715\tvalidation_0-logloss:0.32335\tvalidation_1-error:0.15395\tvalidation_1-logloss:0.368815\n",
      "[45]\tvalidation_0-error:0.137612\tvalidation_0-logloss:0.320651\tvalidation_1-error:0.15248\tvalidation_1-logloss:0.366205\n",
      "[46]\tvalidation_0-error:0.135499\tvalidation_0-logloss:0.315224\tvalidation_1-error:0.151133\tvalidation_1-logloss:0.362032\n",
      "[47]\tvalidation_0-error:0.13455\tvalidation_0-logloss:0.312757\tvalidation_1-error:0.15101\tvalidation_1-logloss:0.361\n",
      "[48]\tvalidation_0-error:0.134734\tvalidation_0-logloss:0.312907\tvalidation_1-error:0.151133\tvalidation_1-logloss:0.361092\n",
      "[49]\tvalidation_0-error:0.133386\tvalidation_0-logloss:0.309927\tvalidation_1-error:0.150153\tvalidation_1-logloss:0.358417\n",
      "[50]\tvalidation_0-error:0.132162\tvalidation_0-logloss:0.305384\tvalidation_1-error:0.147826\tvalidation_1-logloss:0.354756\n",
      "[51]\tvalidation_0-error:0.132253\tvalidation_0-logloss:0.304202\tvalidation_1-error:0.148683\tvalidation_1-logloss:0.354372\n",
      "[52]\tvalidation_0-error:0.13109\tvalidation_0-logloss:0.301899\tvalidation_1-error:0.146969\tvalidation_1-logloss:0.352427\n",
      "[53]\tvalidation_0-error:0.129589\tvalidation_0-logloss:0.299447\tvalidation_1-error:0.145377\tvalidation_1-logloss:0.350886\n",
      "[54]\tvalidation_0-error:0.127752\tvalidation_0-logloss:0.295784\tvalidation_1-error:0.144397\tvalidation_1-logloss:0.34826\n",
      "[55]\tvalidation_0-error:0.127507\tvalidation_0-logloss:0.293965\tvalidation_1-error:0.143784\tvalidation_1-logloss:0.347421\n",
      "[56]\tvalidation_0-error:0.126925\tvalidation_0-logloss:0.291783\tvalidation_1-error:0.142927\tvalidation_1-logloss:0.345626\n",
      "[57]\tvalidation_0-error:0.125915\tvalidation_0-logloss:0.289633\tvalidation_1-error:0.14256\tvalidation_1-logloss:0.344287\n",
      "[58]\tvalidation_0-error:0.123281\tvalidation_0-logloss:0.285264\tvalidation_1-error:0.141457\tvalidation_1-logloss:0.340782\n",
      "[59]\tvalidation_0-error:0.121628\tvalidation_0-logloss:0.282234\tvalidation_1-error:0.14011\tvalidation_1-logloss:0.338816\n",
      "[60]\tvalidation_0-error:0.120434\tvalidation_0-logloss:0.279729\tvalidation_1-error:0.139498\tvalidation_1-logloss:0.337441\n",
      "[61]\tvalidation_0-error:0.118749\tvalidation_0-logloss:0.275423\tvalidation_1-error:0.138641\tvalidation_1-logloss:0.333746\n",
      "[62]\tvalidation_0-error:0.116453\tvalidation_0-logloss:0.27083\tvalidation_1-error:0.135579\tvalidation_1-logloss:0.329537\n",
      "[63]\tvalidation_0-error:0.116483\tvalidation_0-logloss:0.27033\tvalidation_1-error:0.135824\tvalidation_1-logloss:0.32923\n",
      "[64]\tvalidation_0-error:0.115381\tvalidation_0-logloss:0.266555\tvalidation_1-error:0.135089\tvalidation_1-logloss:0.32708\n",
      "[65]\tvalidation_0-error:0.112901\tvalidation_0-logloss:0.263018\tvalidation_1-error:0.132639\tvalidation_1-logloss:0.323871\n",
      "[66]\tvalidation_0-error:0.111676\tvalidation_0-logloss:0.259653\tvalidation_1-error:0.131047\tvalidation_1-logloss:0.320608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[67]\tvalidation_0-error:0.110727\tvalidation_0-logloss:0.257023\tvalidation_1-error:0.1297\tvalidation_1-logloss:0.319055\n",
      "[68]\tvalidation_0-error:0.109747\tvalidation_0-logloss:0.254191\tvalidation_1-error:0.128598\tvalidation_1-logloss:0.317258\n",
      "[69]\tvalidation_0-error:0.108522\tvalidation_0-logloss:0.25209\tvalidation_1-error:0.127985\tvalidation_1-logloss:0.315662\n",
      "[70]\tvalidation_0-error:0.107665\tvalidation_0-logloss:0.251336\tvalidation_1-error:0.128475\tvalidation_1-logloss:0.315483\n",
      "[71]\tvalidation_0-error:0.107297\tvalidation_0-logloss:0.250352\tvalidation_1-error:0.12823\tvalidation_1-logloss:0.315137\n",
      "[72]\tvalidation_0-error:0.106409\tvalidation_0-logloss:0.248093\tvalidation_1-error:0.126883\tvalidation_1-logloss:0.313731\n",
      "[73]\tvalidation_0-error:0.105245\tvalidation_0-logloss:0.246244\tvalidation_1-error:0.125658\tvalidation_1-logloss:0.312105\n",
      "[74]\tvalidation_0-error:0.104021\tvalidation_0-logloss:0.243742\tvalidation_1-error:0.124923\tvalidation_1-logloss:0.309561\n",
      "[75]\tvalidation_0-error:0.103776\tvalidation_0-logloss:0.243081\tvalidation_1-error:0.125046\tvalidation_1-logloss:0.309795\n",
      "[76]\tvalidation_0-error:0.103133\tvalidation_0-logloss:0.241301\tvalidation_1-error:0.124801\tvalidation_1-logloss:0.308446\n",
      "[77]\tvalidation_0-error:0.102153\tvalidation_0-logloss:0.239814\tvalidation_1-error:0.124679\tvalidation_1-logloss:0.307061\n",
      "[78]\tvalidation_0-error:0.101295\tvalidation_0-logloss:0.238609\tvalidation_1-error:0.123454\tvalidation_1-logloss:0.305954\n",
      "[79]\tvalidation_0-error:0.100683\tvalidation_0-logloss:0.23766\tvalidation_1-error:0.122474\tvalidation_1-logloss:0.305374\n",
      "[80]\tvalidation_0-error:0.099611\tvalidation_0-logloss:0.235276\tvalidation_1-error:0.122474\tvalidation_1-logloss:0.303841\n",
      "[81]\tvalidation_0-error:0.098539\tvalidation_0-logloss:0.231591\tvalidation_1-error:0.121617\tvalidation_1-logloss:0.301024\n",
      "[82]\tvalidation_0-error:0.097866\tvalidation_0-logloss:0.230055\tvalidation_1-error:0.121004\tvalidation_1-logloss:0.299614\n",
      "[83]\tvalidation_0-error:0.097039\tvalidation_0-logloss:0.229338\tvalidation_1-error:0.121862\tvalidation_1-logloss:0.299428\n",
      "[84]\tvalidation_0-error:0.096702\tvalidation_0-logloss:0.22821\tvalidation_1-error:0.120882\tvalidation_1-logloss:0.298725\n",
      "[85]\tvalidation_0-error:0.096702\tvalidation_0-logloss:0.227171\tvalidation_1-error:0.120392\tvalidation_1-logloss:0.298002\n",
      "[86]\tvalidation_0-error:0.096335\tvalidation_0-logloss:0.224819\tvalidation_1-error:0.119535\tvalidation_1-logloss:0.296185\n",
      "[87]\tvalidation_0-error:0.095692\tvalidation_0-logloss:0.222936\tvalidation_1-error:0.119045\tvalidation_1-logloss:0.295051\n",
      "[88]\tvalidation_0-error:0.094804\tvalidation_0-logloss:0.220888\tvalidation_1-error:0.117575\tvalidation_1-logloss:0.293335\n",
      "[89]\tvalidation_0-error:0.094007\tvalidation_0-logloss:0.219284\tvalidation_1-error:0.117208\tvalidation_1-logloss:0.29231\n",
      "[90]\tvalidation_0-error:0.093609\tvalidation_0-logloss:0.217428\tvalidation_1-error:0.116228\tvalidation_1-logloss:0.291342\n",
      "[91]\tvalidation_0-error:0.093456\tvalidation_0-logloss:0.216196\tvalidation_1-error:0.115615\tvalidation_1-logloss:0.29048\n",
      "[92]\tvalidation_0-error:0.092476\tvalidation_0-logloss:0.214752\tvalidation_1-error:0.114758\tvalidation_1-logloss:0.289526\n",
      "[93]\tvalidation_0-error:0.092078\tvalidation_0-logloss:0.213693\tvalidation_1-error:0.114513\tvalidation_1-logloss:0.28889\n",
      "[94]\tvalidation_0-error:0.090119\tvalidation_0-logloss:0.21054\tvalidation_1-error:0.113656\tvalidation_1-logloss:0.286702\n",
      "[95]\tvalidation_0-error:0.08871\tvalidation_0-logloss:0.208142\tvalidation_1-error:0.113166\tvalidation_1-logloss:0.284969\n",
      "[96]\tvalidation_0-error:0.087271\tvalidation_0-logloss:0.20474\tvalidation_1-error:0.112921\tvalidation_1-logloss:0.282516\n",
      "[97]\tvalidation_0-error:0.086168\tvalidation_0-logloss:0.202412\tvalidation_1-error:0.112186\tvalidation_1-logloss:0.280741\n",
      "[98]\tvalidation_0-error:0.084545\tvalidation_0-logloss:0.199178\tvalidation_1-error:0.111819\tvalidation_1-logloss:0.277832\n",
      "[99]\tvalidation_0-error:0.083964\tvalidation_0-logloss:0.197449\tvalidation_1-error:0.111574\tvalidation_1-logloss:0.27705\n",
      "[100]\tvalidation_0-error:0.082034\tvalidation_0-logloss:0.193696\tvalidation_1-error:0.111574\tvalidation_1-logloss:0.275383\n",
      "[101]\tvalidation_0-error:0.080534\tvalidation_0-logloss:0.190853\tvalidation_1-error:0.111084\tvalidation_1-logloss:0.273064\n",
      "[102]\tvalidation_0-error:0.080228\tvalidation_0-logloss:0.189166\tvalidation_1-error:0.110472\tvalidation_1-logloss:0.272224\n",
      "[103]\tvalidation_0-error:0.07888\tvalidation_0-logloss:0.186036\tvalidation_1-error:0.109614\tvalidation_1-logloss:0.269741\n",
      "[104]\tvalidation_0-error:0.078636\tvalidation_0-logloss:0.185653\tvalidation_1-error:0.108879\tvalidation_1-logloss:0.270059\n",
      "[105]\tvalidation_0-error:0.077747\tvalidation_0-logloss:0.183641\tvalidation_1-error:0.108267\tvalidation_1-logloss:0.268694\n",
      "[106]\tvalidation_0-error:0.077319\tvalidation_0-logloss:0.182784\tvalidation_1-error:0.108145\tvalidation_1-logloss:0.268476\n",
      "[107]\tvalidation_0-error:0.076676\tvalidation_0-logloss:0.181333\tvalidation_1-error:0.108389\tvalidation_1-logloss:0.267288\n",
      "[108]\tvalidation_0-error:0.076247\tvalidation_0-logloss:0.180177\tvalidation_1-error:0.1079\tvalidation_1-logloss:0.266366\n",
      "[109]\tvalidation_0-error:0.07539\tvalidation_0-logloss:0.179217\tvalidation_1-error:0.108145\tvalidation_1-logloss:0.265597\n",
      "[110]\tvalidation_0-error:0.075114\tvalidation_0-logloss:0.178689\tvalidation_1-error:0.107532\tvalidation_1-logloss:0.265403\n",
      "[111]\tvalidation_0-error:0.074593\tvalidation_0-logloss:0.177953\tvalidation_1-error:0.106797\tvalidation_1-logloss:0.265471\n",
      "[112]\tvalidation_0-error:0.073461\tvalidation_0-logloss:0.177024\tvalidation_1-error:0.107042\tvalidation_1-logloss:0.265427\n",
      "[113]\tvalidation_0-error:0.072909\tvalidation_0-logloss:0.174851\tvalidation_1-error:0.106185\tvalidation_1-logloss:0.264202\n",
      "[114]\tvalidation_0-error:0.072511\tvalidation_0-logloss:0.173668\tvalidation_1-error:0.10594\tvalidation_1-logloss:0.263199\n",
      "[115]\tvalidation_0-error:0.071501\tvalidation_0-logloss:0.171627\tvalidation_1-error:0.104715\tvalidation_1-logloss:0.261286\n",
      "[116]\tvalidation_0-error:0.070705\tvalidation_0-logloss:0.17008\tvalidation_1-error:0.105083\tvalidation_1-logloss:0.260433\n",
      "[117]\tvalidation_0-error:0.071715\tvalidation_0-logloss:0.171131\tvalidation_1-error:0.105083\tvalidation_1-logloss:0.261992\n",
      "[118]\tvalidation_0-error:0.071041\tvalidation_0-logloss:0.170047\tvalidation_1-error:0.105328\tvalidation_1-logloss:0.26153\n",
      "[119]\tvalidation_0-error:0.070429\tvalidation_0-logloss:0.169026\tvalidation_1-error:0.10545\tvalidation_1-logloss:0.261205\n",
      "[120]\tvalidation_0-error:0.069694\tvalidation_0-logloss:0.167086\tvalidation_1-error:0.105205\tvalidation_1-logloss:0.259051\n",
      "[121]\tvalidation_0-error:0.068408\tvalidation_0-logloss:0.164031\tvalidation_1-error:0.103123\tvalidation_1-logloss:0.257004\n",
      "[122]\tvalidation_0-error:0.067336\tvalidation_0-logloss:0.162465\tvalidation_1-error:0.102756\tvalidation_1-logloss:0.256014\n",
      "[123]\tvalidation_0-error:0.066571\tvalidation_0-logloss:0.161603\tvalidation_1-error:0.102266\tvalidation_1-logloss:0.255257\n",
      "[124]\tvalidation_0-error:0.066265\tvalidation_0-logloss:0.161049\tvalidation_1-error:0.101653\tvalidation_1-logloss:0.254539\n",
      "[125]\tvalidation_0-error:0.066111\tvalidation_0-logloss:0.160264\tvalidation_1-error:0.102021\tvalidation_1-logloss:0.254321\n",
      "[126]\tvalidation_0-error:0.065989\tvalidation_0-logloss:0.159321\tvalidation_1-error:0.101653\tvalidation_1-logloss:0.253885\n",
      "[127]\tvalidation_0-error:0.065132\tvalidation_0-logloss:0.158407\tvalidation_1-error:0.100919\tvalidation_1-logloss:0.253649\n",
      "[128]\tvalidation_0-error:0.065468\tvalidation_0-logloss:0.158381\tvalidation_1-error:0.100919\tvalidation_1-logloss:0.253831\n",
      "[129]\tvalidation_0-error:0.064825\tvalidation_0-logloss:0.157035\tvalidation_1-error:0.100061\tvalidation_1-logloss:0.252732\n",
      "[130]\tvalidation_0-error:0.064703\tvalidation_0-logloss:0.157032\tvalidation_1-error:0.099939\tvalidation_1-logloss:0.253754\n",
      "[131]\tvalidation_0-error:0.063662\tvalidation_0-logloss:0.155173\tvalidation_1-error:0.100061\tvalidation_1-logloss:0.251971\n",
      "[132]\tvalidation_0-error:0.062896\tvalidation_0-logloss:0.154451\tvalidation_1-error:0.099571\tvalidation_1-logloss:0.250915\n",
      "[133]\tvalidation_0-error:0.062529\tvalidation_0-logloss:0.153564\tvalidation_1-error:0.099081\tvalidation_1-logloss:0.250518\n",
      "[134]\tvalidation_0-error:0.061886\tvalidation_0-logloss:0.15179\tvalidation_1-error:0.098347\tvalidation_1-logloss:0.249042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[135]\tvalidation_0-error:0.061794\tvalidation_0-logloss:0.151459\tvalidation_1-error:0.098836\tvalidation_1-logloss:0.249181\n",
      "[136]\tvalidation_0-error:0.060906\tvalidation_0-logloss:0.149787\tvalidation_1-error:0.098592\tvalidation_1-logloss:0.248389\n",
      "[137]\tvalidation_0-error:0.060508\tvalidation_0-logloss:0.148305\tvalidation_1-error:0.097612\tvalidation_1-logloss:0.247552\n",
      "[138]\tvalidation_0-error:0.060508\tvalidation_0-logloss:0.148587\tvalidation_1-error:0.097367\tvalidation_1-logloss:0.24813\n",
      "[139]\tvalidation_0-error:0.059987\tvalidation_0-logloss:0.146914\tvalidation_1-error:0.096632\tvalidation_1-logloss:0.246853\n",
      "[140]\tvalidation_0-error:0.060232\tvalidation_0-logloss:0.146678\tvalidation_1-error:0.09602\tvalidation_1-logloss:0.246632\n",
      "[141]\tvalidation_0-error:0.059528\tvalidation_0-logloss:0.145913\tvalidation_1-error:0.096387\tvalidation_1-logloss:0.245697\n",
      "[142]\tvalidation_0-error:0.059589\tvalidation_0-logloss:0.144474\tvalidation_1-error:0.095775\tvalidation_1-logloss:0.245515\n",
      "[143]\tvalidation_0-error:0.058762\tvalidation_0-logloss:0.143123\tvalidation_1-error:0.095775\tvalidation_1-logloss:0.244907\n",
      "[144]\tvalidation_0-error:0.058701\tvalidation_0-logloss:0.143006\tvalidation_1-error:0.095897\tvalidation_1-logloss:0.245177\n",
      "[145]\tvalidation_0-error:0.05818\tvalidation_0-logloss:0.142459\tvalidation_1-error:0.095407\tvalidation_1-logloss:0.245168\n",
      "[146]\tvalidation_0-error:0.057476\tvalidation_0-logloss:0.140127\tvalidation_1-error:0.09553\tvalidation_1-logloss:0.243422\n",
      "[147]\tvalidation_0-error:0.057231\tvalidation_0-logloss:0.139005\tvalidation_1-error:0.09553\tvalidation_1-logloss:0.242953\n",
      "[148]\tvalidation_0-error:0.056496\tvalidation_0-logloss:0.13861\tvalidation_1-error:0.095775\tvalidation_1-logloss:0.243206\n",
      "[149]\tvalidation_0-error:0.056282\tvalidation_0-logloss:0.137727\tvalidation_1-error:0.094672\tvalidation_1-logloss:0.242711\n",
      "[150]\tvalidation_0-error:0.055516\tvalidation_0-logloss:0.136196\tvalidation_1-error:0.094427\tvalidation_1-logloss:0.242247\n",
      "[151]\tvalidation_0-error:0.055425\tvalidation_0-logloss:0.135441\tvalidation_1-error:0.094427\tvalidation_1-logloss:0.241514\n",
      "[152]\tvalidation_0-error:0.055149\tvalidation_0-logloss:0.134886\tvalidation_1-error:0.095162\tvalidation_1-logloss:0.241439\n",
      "[153]\tvalidation_0-error:0.055241\tvalidation_0-logloss:0.134667\tvalidation_1-error:0.09504\tvalidation_1-logloss:0.24154\n",
      "[154]\tvalidation_0-error:0.054628\tvalidation_0-logloss:0.134301\tvalidation_1-error:0.095162\tvalidation_1-logloss:0.241759\n",
      "[155]\tvalidation_0-error:0.054077\tvalidation_0-logloss:0.133375\tvalidation_1-error:0.094795\tvalidation_1-logloss:0.241308\n",
      "[156]\tvalidation_0-error:0.053832\tvalidation_0-logloss:0.132683\tvalidation_1-error:0.094182\tvalidation_1-logloss:0.240799\n",
      "[157]\tvalidation_0-error:0.053312\tvalidation_0-logloss:0.13153\tvalidation_1-error:0.092958\tvalidation_1-logloss:0.239876\n",
      "[158]\tvalidation_0-error:0.053005\tvalidation_0-logloss:0.130906\tvalidation_1-error:0.092835\tvalidation_1-logloss:0.240077\n",
      "[159]\tvalidation_0-error:0.052393\tvalidation_0-logloss:0.130119\tvalidation_1-error:0.092468\tvalidation_1-logloss:0.240072\n",
      "[160]\tvalidation_0-error:0.052791\tvalidation_0-logloss:0.130403\tvalidation_1-error:0.09308\tvalidation_1-logloss:0.240752\n",
      "[161]\tvalidation_0-error:0.053312\tvalidation_0-logloss:0.130403\tvalidation_1-error:0.093448\tvalidation_1-logloss:0.241457\n",
      "[162]\tvalidation_0-error:0.052179\tvalidation_0-logloss:0.128949\tvalidation_1-error:0.092468\tvalidation_1-logloss:0.240232\n",
      "[163]\tvalidation_0-error:0.051872\tvalidation_0-logloss:0.128896\tvalidation_1-error:0.09357\tvalidation_1-logloss:0.240963\n",
      "[164]\tvalidation_0-error:0.051964\tvalidation_0-logloss:0.12886\tvalidation_1-error:0.09357\tvalidation_1-logloss:0.240908\n",
      "[165]\tvalidation_0-error:0.050709\tvalidation_0-logloss:0.127194\tvalidation_1-error:0.09259\tvalidation_1-logloss:0.239157\n",
      "[166]\tvalidation_0-error:0.050831\tvalidation_0-logloss:0.127276\tvalidation_1-error:0.09308\tvalidation_1-logloss:0.23947\n",
      "[167]\tvalidation_0-error:0.050403\tvalidation_0-logloss:0.125763\tvalidation_1-error:0.091611\tvalidation_1-logloss:0.238425\n",
      "[168]\tvalidation_0-error:0.04976\tvalidation_0-logloss:0.124864\tvalidation_1-error:0.091366\tvalidation_1-logloss:0.238209\n",
      "[169]\tvalidation_0-error:0.049362\tvalidation_0-logloss:0.123823\tvalidation_1-error:0.090263\tvalidation_1-logloss:0.236967\n",
      "[170]\tvalidation_0-error:0.048902\tvalidation_0-logloss:0.122608\tvalidation_1-error:0.089528\tvalidation_1-logloss:0.236384\n",
      "[171]\tvalidation_0-error:0.048443\tvalidation_0-logloss:0.122118\tvalidation_1-error:0.089896\tvalidation_1-logloss:0.236415\n",
      "[172]\tvalidation_0-error:0.048167\tvalidation_0-logloss:0.120915\tvalidation_1-error:0.090263\tvalidation_1-logloss:0.235784\n",
      "[173]\tvalidation_0-error:0.04829\tvalidation_0-logloss:0.120968\tvalidation_1-error:0.089651\tvalidation_1-logloss:0.236555\n",
      "[174]\tvalidation_0-error:0.04731\tvalidation_0-logloss:0.119475\tvalidation_1-error:0.088916\tvalidation_1-logloss:0.235055\n",
      "[175]\tvalidation_0-error:0.046851\tvalidation_0-logloss:0.118809\tvalidation_1-error:0.088671\tvalidation_1-logloss:0.234519\n",
      "[176]\tvalidation_0-error:0.046575\tvalidation_0-logloss:0.118124\tvalidation_1-error:0.088426\tvalidation_1-logloss:0.234191\n",
      "[177]\tvalidation_0-error:0.046453\tvalidation_0-logloss:0.117233\tvalidation_1-error:0.087569\tvalidation_1-logloss:0.233119\n",
      "[178]\tvalidation_0-error:0.045993\tvalidation_0-logloss:0.116086\tvalidation_1-error:0.087079\tvalidation_1-logloss:0.232269\n",
      "[179]\tvalidation_0-error:0.045166\tvalidation_0-logloss:0.115598\tvalidation_1-error:0.086712\tvalidation_1-logloss:0.23185\n",
      "[180]\tvalidation_0-error:0.045075\tvalidation_0-logloss:0.115587\tvalidation_1-error:0.086834\tvalidation_1-logloss:0.232439\n",
      "[181]\tvalidation_0-error:0.045136\tvalidation_0-logloss:0.115384\tvalidation_1-error:0.086957\tvalidation_1-logloss:0.23258\n",
      "[182]\tvalidation_0-error:0.044768\tvalidation_0-logloss:0.115168\tvalidation_1-error:0.087201\tvalidation_1-logloss:0.232945\n",
      "[183]\tvalidation_0-error:0.04434\tvalidation_0-logloss:0.114205\tvalidation_1-error:0.087446\tvalidation_1-logloss:0.232522\n",
      "[184]\tvalidation_0-error:0.044095\tvalidation_0-logloss:0.113335\tvalidation_1-error:0.087446\tvalidation_1-logloss:0.232335\n",
      "[185]\tvalidation_0-error:0.044095\tvalidation_0-logloss:0.113362\tvalidation_1-error:0.087446\tvalidation_1-logloss:0.232573\n",
      "[186]\tvalidation_0-error:0.04437\tvalidation_0-logloss:0.113203\tvalidation_1-error:0.087936\tvalidation_1-logloss:0.232797\n",
      "[187]\tvalidation_0-error:0.044554\tvalidation_0-logloss:0.113145\tvalidation_1-error:0.087691\tvalidation_1-logloss:0.232801\n",
      "[188]\tvalidation_0-error:0.043788\tvalidation_0-logloss:0.111989\tvalidation_1-error:0.087079\tvalidation_1-logloss:0.23161\n",
      "[189]\tvalidation_0-error:0.043543\tvalidation_0-logloss:0.112251\tvalidation_1-error:0.087324\tvalidation_1-logloss:0.232226\n",
      "[190]\tvalidation_0-error:0.043452\tvalidation_0-logloss:0.111545\tvalidation_1-error:0.086467\tvalidation_1-logloss:0.231601\n",
      "[191]\tvalidation_0-error:0.043054\tvalidation_0-logloss:0.110305\tvalidation_1-error:0.086344\tvalidation_1-logloss:0.230624\n",
      "[192]\tvalidation_0-error:0.043145\tvalidation_0-logloss:0.110831\tvalidation_1-error:0.087691\tvalidation_1-logloss:0.231743\n",
      "[193]\tvalidation_0-error:0.043207\tvalidation_0-logloss:0.110573\tvalidation_1-error:0.086467\tvalidation_1-logloss:0.231693\n",
      "[194]\tvalidation_0-error:0.043329\tvalidation_0-logloss:0.110675\tvalidation_1-error:0.086834\tvalidation_1-logloss:0.23202\n",
      "[195]\tvalidation_0-error:0.042686\tvalidation_0-logloss:0.109629\tvalidation_1-error:0.086834\tvalidation_1-logloss:0.231557\n",
      "[196]\tvalidation_0-error:0.042227\tvalidation_0-logloss:0.108862\tvalidation_1-error:0.086467\tvalidation_1-logloss:0.231425\n",
      "[197]\tvalidation_0-error:0.042135\tvalidation_0-logloss:0.108382\tvalidation_1-error:0.086222\tvalidation_1-logloss:0.231136\n",
      "[198]\tvalidation_0-error:0.042043\tvalidation_0-logloss:0.108046\tvalidation_1-error:0.085977\tvalidation_1-logloss:0.230645\n",
      "[199]\tvalidation_0-error:0.041369\tvalidation_0-logloss:0.106887\tvalidation_1-error:0.084017\tvalidation_1-logloss:0.229557\n",
      "[200]\tvalidation_0-error:0.041278\tvalidation_0-logloss:0.106148\tvalidation_1-error:0.084385\tvalidation_1-logloss:0.229491\n",
      "[201]\tvalidation_0-error:0.041186\tvalidation_0-logloss:0.10602\tvalidation_1-error:0.084874\tvalidation_1-logloss:0.22985\n",
      "[202]\tvalidation_0-error:0.040359\tvalidation_0-logloss:0.105182\tvalidation_1-error:0.08463\tvalidation_1-logloss:0.22985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[203]\tvalidation_0-error:0.040236\tvalidation_0-logloss:0.104511\tvalidation_1-error:0.084752\tvalidation_1-logloss:0.229425\n",
      "[204]\tvalidation_0-error:0.040236\tvalidation_0-logloss:0.10428\tvalidation_1-error:0.085854\tvalidation_1-logloss:0.22984\n",
      "[205]\tvalidation_0-error:0.040236\tvalidation_0-logloss:0.103915\tvalidation_1-error:0.085242\tvalidation_1-logloss:0.229584\n",
      "[206]\tvalidation_0-error:0.040175\tvalidation_0-logloss:0.10408\tvalidation_1-error:0.085854\tvalidation_1-logloss:0.229918\n",
      "[207]\tvalidation_0-error:0.0399\tvalidation_0-logloss:0.103408\tvalidation_1-error:0.085119\tvalidation_1-logloss:0.229424\n",
      "[208]\tvalidation_0-error:0.0399\tvalidation_0-logloss:0.103303\tvalidation_1-error:0.084752\tvalidation_1-logloss:0.229919\n",
      "[209]\tvalidation_0-error:0.039746\tvalidation_0-logloss:0.103318\tvalidation_1-error:0.085854\tvalidation_1-logloss:0.230149\n",
      "[210]\tvalidation_0-error:0.039624\tvalidation_0-logloss:0.102973\tvalidation_1-error:0.085977\tvalidation_1-logloss:0.230009\n",
      "[211]\tvalidation_0-error:0.039379\tvalidation_0-logloss:0.102828\tvalidation_1-error:0.085854\tvalidation_1-logloss:0.230236\n",
      "[212]\tvalidation_0-error:0.039073\tvalidation_0-logloss:0.102246\tvalidation_1-error:0.084997\tvalidation_1-logloss:0.230267\n",
      "[213]\tvalidation_0-error:0.03895\tvalidation_0-logloss:0.101881\tvalidation_1-error:0.085119\tvalidation_1-logloss:0.230349\n",
      "[214]\tvalidation_0-error:0.038767\tvalidation_0-logloss:0.101128\tvalidation_1-error:0.084262\tvalidation_1-logloss:0.229478\n",
      "[215]\tvalidation_0-error:0.038767\tvalidation_0-logloss:0.101211\tvalidation_1-error:0.084997\tvalidation_1-logloss:0.229658\n",
      "[216]\tvalidation_0-error:0.038583\tvalidation_0-logloss:0.101029\tvalidation_1-error:0.084385\tvalidation_1-logloss:0.229582\n",
      "[217]\tvalidation_0-error:0.038124\tvalidation_0-logloss:0.100329\tvalidation_1-error:0.084874\tvalidation_1-logloss:0.228757\n",
      "[218]\tvalidation_0-error:0.037695\tvalidation_0-logloss:0.09938\tvalidation_1-error:0.084874\tvalidation_1-logloss:0.228703\n",
      "[219]\tvalidation_0-error:0.037205\tvalidation_0-logloss:0.098602\tvalidation_1-error:0.084017\tvalidation_1-logloss:0.228072\n",
      "[220]\tvalidation_0-error:0.037052\tvalidation_0-logloss:0.098566\tvalidation_1-error:0.083895\tvalidation_1-logloss:0.228597\n",
      "[221]\tvalidation_0-error:0.036868\tvalidation_0-logloss:0.09825\tvalidation_1-error:0.084262\tvalidation_1-logloss:0.228983\n",
      "[222]\tvalidation_0-error:0.036623\tvalidation_0-logloss:0.098173\tvalidation_1-error:0.08365\tvalidation_1-logloss:0.22877\n",
      "[223]\tvalidation_0-error:0.036531\tvalidation_0-logloss:0.09707\tvalidation_1-error:0.08365\tvalidation_1-logloss:0.22802\n",
      "[224]\tvalidation_0-error:0.035949\tvalidation_0-logloss:0.096747\tvalidation_1-error:0.08463\tvalidation_1-logloss:0.227723\n",
      "[225]\tvalidation_0-error:0.035888\tvalidation_0-logloss:0.096334\tvalidation_1-error:0.083772\tvalidation_1-logloss:0.227994\n",
      "[226]\tvalidation_0-error:0.035674\tvalidation_0-logloss:0.095775\tvalidation_1-error:0.083405\tvalidation_1-logloss:0.22731\n",
      "[227]\tvalidation_0-error:0.035521\tvalidation_0-logloss:0.095485\tvalidation_1-error:0.083037\tvalidation_1-logloss:0.226975\n",
      "[228]\tvalidation_0-error:0.035306\tvalidation_0-logloss:0.095529\tvalidation_1-error:0.082425\tvalidation_1-logloss:0.227313\n",
      "[229]\tvalidation_0-error:0.035337\tvalidation_0-logloss:0.094677\tvalidation_1-error:0.083282\tvalidation_1-logloss:0.226854\n",
      "[230]\tvalidation_0-error:0.034878\tvalidation_0-logloss:0.094322\tvalidation_1-error:0.082915\tvalidation_1-logloss:0.226419\n",
      "[231]\tvalidation_0-error:0.035123\tvalidation_0-logloss:0.094446\tvalidation_1-error:0.082792\tvalidation_1-logloss:0.226737\n",
      "[232]\tvalidation_0-error:0.035368\tvalidation_0-logloss:0.094146\tvalidation_1-error:0.08218\tvalidation_1-logloss:0.226786\n",
      "[233]\tvalidation_0-error:0.034326\tvalidation_0-logloss:0.093085\tvalidation_1-error:0.08316\tvalidation_1-logloss:0.225819\n",
      "[234]\tvalidation_0-error:0.034173\tvalidation_0-logloss:0.092635\tvalidation_1-error:0.083772\tvalidation_1-logloss:0.226029\n",
      "[235]\tvalidation_0-error:0.033928\tvalidation_0-logloss:0.092028\tvalidation_1-error:0.083895\tvalidation_1-logloss:0.226259\n",
      "[236]\tvalidation_0-error:0.033837\tvalidation_0-logloss:0.092149\tvalidation_1-error:0.08316\tvalidation_1-logloss:0.226634\n",
      "[237]\tvalidation_0-error:0.033683\tvalidation_0-logloss:0.091217\tvalidation_1-error:0.082915\tvalidation_1-logloss:0.226187\n",
      "[238]\tvalidation_0-error:0.033377\tvalidation_0-logloss:0.091026\tvalidation_1-error:0.082058\tvalidation_1-logloss:0.225651\n",
      "[239]\tvalidation_0-error:0.033102\tvalidation_0-logloss:0.090303\tvalidation_1-error:0.08169\tvalidation_1-logloss:0.225176\n",
      "[240]\tvalidation_0-error:0.033132\tvalidation_0-logloss:0.089322\tvalidation_1-error:0.081445\tvalidation_1-logloss:0.224389\n",
      "[241]\tvalidation_0-error:0.032795\tvalidation_0-logloss:0.089033\tvalidation_1-error:0.08218\tvalidation_1-logloss:0.224313\n",
      "[242]\tvalidation_0-error:0.032704\tvalidation_0-logloss:0.088659\tvalidation_1-error:0.082058\tvalidation_1-logloss:0.224043\n",
      "[243]\tvalidation_0-error:0.032428\tvalidation_0-logloss:0.088524\tvalidation_1-error:0.08218\tvalidation_1-logloss:0.224393\n",
      "[244]\tvalidation_0-error:0.032122\tvalidation_0-logloss:0.087145\tvalidation_1-error:0.081813\tvalidation_1-logloss:0.222638\n",
      "[245]\tvalidation_0-error:0.031938\tvalidation_0-logloss:0.086852\tvalidation_1-error:0.0812\tvalidation_1-logloss:0.222725\n",
      "[246]\tvalidation_0-error:0.031938\tvalidation_0-logloss:0.086957\tvalidation_1-error:0.081445\tvalidation_1-logloss:0.222981\n",
      "[247]\tvalidation_0-error:0.032061\tvalidation_0-logloss:0.087093\tvalidation_1-error:0.082058\tvalidation_1-logloss:0.223516\n",
      "[248]\tvalidation_0-error:0.031907\tvalidation_0-logloss:0.086926\tvalidation_1-error:0.081935\tvalidation_1-logloss:0.223456\n",
      "[249]\tvalidation_0-error:0.031907\tvalidation_0-logloss:0.086961\tvalidation_1-error:0.08169\tvalidation_1-logloss:0.224092\n",
      "[250]\tvalidation_0-error:0.031907\tvalidation_0-logloss:0.086294\tvalidation_1-error:0.082547\tvalidation_1-logloss:0.223767\n",
      "[251]\tvalidation_0-error:0.03154\tvalidation_0-logloss:0.085797\tvalidation_1-error:0.082547\tvalidation_1-logloss:0.223849\n",
      "[252]\tvalidation_0-error:0.031203\tvalidation_0-logloss:0.085432\tvalidation_1-error:0.08267\tvalidation_1-logloss:0.22343\n",
      "[253]\tvalidation_0-error:0.030928\tvalidation_0-logloss:0.085239\tvalidation_1-error:0.082425\tvalidation_1-logloss:0.223399\n",
      "[254]\tvalidation_0-error:0.030683\tvalidation_0-logloss:0.084568\tvalidation_1-error:0.08169\tvalidation_1-logloss:0.222917\n",
      "[255]\tvalidation_0-error:0.030897\tvalidation_0-logloss:0.084427\tvalidation_1-error:0.082303\tvalidation_1-logloss:0.223369\n",
      "[256]\tvalidation_0-error:0.030621\tvalidation_0-logloss:0.083884\tvalidation_1-error:0.082303\tvalidation_1-logloss:0.223161\n",
      "[257]\tvalidation_0-error:0.030744\tvalidation_0-logloss:0.084143\tvalidation_1-error:0.082058\tvalidation_1-logloss:0.223786\n",
      "[258]\tvalidation_0-error:0.030346\tvalidation_0-logloss:0.083746\tvalidation_1-error:0.081935\tvalidation_1-logloss:0.223368\n",
      "[259]\tvalidation_0-error:0.030162\tvalidation_0-logloss:0.083205\tvalidation_1-error:0.081813\tvalidation_1-logloss:0.223303\n",
      "Stopping. Best iteration:\n",
      "[244]\tvalidation_0-error:0.032122\tvalidation_0-logloss:0.087145\tvalidation_1-error:0.081813\tvalidation_1-logloss:0.222638\n",
      "\n",
      "CPU times: user 40.1 s, sys: 2.65 s, total: 42.7 s\n",
      "Wall time: 20.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.13, max_delta_step=0, max_depth=12,\n",
       "              min_child_weight=1, missing=None, n_estimators=282, n_jobs=1,\n",
       "              nthread=4, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=18, seed=None,\n",
       "              silent=None, subsample=0.33, verbosity=1)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "clf.fit(X_train, y_train,\n",
    "        early_stopping_rounds=15,\n",
    "        eval_set=[(X_train, y_train), (X_test, y_test)], \n",
    "        eval_metric=['error','logloss'],\n",
    "        verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.82%\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEICAYAAABS0fM3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3dd3zV5dn48c91TvbeIYMQphCQGUAQ98CNtm7t4yy1j6u11p+djj62amurVutoC3XUQW1V3HtUUZYCspdZJJC9yTzX74/vQSMmEEJOTpJzvV+v8yLnO6+bA+fKPb73LaqKMcaYwOXydwDGGGP8yxKBMcYEOEsExhgT4CwRGGNMgLNEYIwxAc4SgTHGBDhLBMYYE+AsERi/EJEoEckTkQs7bIsWkQIRObvDtlwReVlEqkSkWkTWi8gdIhLv3X+piLSLSL33tV1Efujj2I8WkaL9HPMPEfk/f8dhTHdYIjB+oar1wHzgPhFJ9m6+G1ihqs8BiMhs4H3gY2CsqsYBJwFtwKQOl/tEVaNUNQo4G7hbRKb0TUmMGfgsERi/UdU3gVeA+0XkaOBc4OoOh9wNLFTV36nqLu85Bap6i6q+38U1PwM2AOP2bBORM0RknbdG8b6IdNw3zrut2nvMGR32neKtgdSJyA4RuVFEIoHXgPQOtZD0Aym3iMwWkeUiUuP9c3aHfcNF5EPvPd8WkQdF5MkDub73OrEi8riIlIlIvoj8UkRc3n2jROQD7/3LReRZ73YRkT+JSKl33xoRmXCg9zYDjyUC428/Bo4GngNuVNUSAO8X7izg3wdyMRGZDowBVnjfjwGeBn4EJAOvAi+JSIiIBAMvAW8CKcC1wD9F5BDv5f4O/EBVo4EJwLuq2gCcDBTvqYWoavEBxJeAN/kBicAfgVdEJNF7yFPAMu++W4HvHUj5O/gzEAuMAI4C/ge4zLvvNzhljgcyvccCnAgcifP3FwecB1T08P5mALFEYPxKVauAdUAE8J8Ou+Jx/n3u3LNBRO72/ubeICK/7HDsYd7t9Thfok8AW7z7zgNeUdW3VLUV+AMQDswGDgOigDtVtUVV3wVeBi7wntsK5IhIjKpWeWsbB+tUYIuqPqGqbar6NLAROF1EsoDpwK+98XwELD7QG4iIG6fcP1PVOlXNA+7h66TSCgwD0lW1yXufPdujgbGAqOqGPYnZDG6WCIxficjFQDbwNnBXh11VgAdI27NBVW/y9hM8DwR1OPZTVY3z9hEMAcYDv/XuSwfyO1zDAxQCGd59hd5te+R79wF8FzgFyPc2pcw6uNJ+O5697pkOVKpqY4d9hT24RxIQstd9OpbrJkCAZd7msMsBvInwAeBBYJeIPCoiMT24vxlgLBEYvxGRFOBPwPeBHwDnisiRAN4mmKXAdw7kmt6+hH8Dp3s3FeP89rvnngIMBXZ49w3d03buleXdh6ouV9V5OM1GLwCL9tzmQGLayzfi2eueJUCCiER02De0B/co5+vf+ve+B6q6U1W/r6rpOH/vfxGRUd5996vqNJxkOgb4aQ/ubwYYSwTGnx4AXlDV97xNEDcBfxWRUO/+m4DLReRmb9JARDKB4V1d0NvWfhZOcxM4X96nishx3j6BnwDNwBKcRNMA3CQiwd4O69OBZ7x9CBeJSKy3SakWaPdecxeQKCKx+ymfW0TCOrxCcPooxojIhSISJCLnATnAy6qaj9O3cav3/rP4OqF1aa97hOHUpBYBd4gzJHcYcAPwpPf4c7x/j+DUvBRoF5HpIjLT+/fUADR1KLMZzFTVXvbq8xdwJs5vx3F7bX8HuKPD+5k4X57V3tda4A4g0bv/Upwvq3rvqxSnczilwzXOAtYDNcAHwPgO+8Z7t9V4jznLuz0EeB3ni7IWWA7M6XDeApyO1Gqctva9y/cPnC/Yjq+PvPvmACu991y513VHAv8F6rx/F48Cf+/i7/DoTu6hwCicPpYngTKc5qVfAy7veXfj1A7qgW3AfO/244A13u3lwD+BKH//W7GX71/i/QdgjOmHvEM7N6rqLf6OxQxe1jRkTD/ibZ4ZKSIuETkJmIfTP2GMzwTt/xBjTB8agjOMNhEoAn6oqp/7NyQz2FnTkDHGBDhrGjLGmAA34JqGkpKSNDs7299hGGPMgLJy5cpyVU3ubN+ASwTZ2dmsWLHC32EYY8yAIiJ7P9H+FWsaMsaYAGeJwBhjApwlAmOMCXADro/AGGO6q7W1laKiIpqamvwdSp8JCwsjMzOT4ODgbp9jicAYM2gVFRURHR1NdnY2zsSzg5uqUlFRQVFREcOHdzk347dY05AxZtBqamoiMTExIJIAgIiQmJh4wDUgSwTGmEEtUJLAHj0pb8AkgvzP32bt4zegHpte3RhjOgqYRFC87mMmbP879bVV/g7FGBMgKioqmDx5MpMnT2bIkCFkZGR89b6lpaVb17jsssvYtGmTT+MMmM5iV0Q8APXV5UTHJfk5GmNMIEhMTGTVqlUA3HrrrURFRXHjjTd+45g9i8O4XJ3/Xr5w4UKfxxkwNYLgqAQAGmvK/ByJMSbQbd26lQkTJnDVVVcxdepUSkpKmD9/Prm5uYwfP57bb7/9q2PnzJnDqlWraGtrIy4ujptvvplJkyYxa9YsSktLeyWegKkRBEclAtBUV+nnSIwx/nDbS+tYX1zbq9fMSY/hltPH9+jc9evXs3DhQh5++GEA7rzzThISEmhra+OYY47h7LPPJicn5xvn1NTUcNRRR3HnnXdyww03sGDBAm6++eaDLkfA1AjCY5xE0FJvicAY438jR45k+vTpX71/+umnmTp1KlOnTmXDhg2sX7/+W+eEh4dz8sknAzBt2jTy8vJ6JZaAqRFEevsF2hosERgTiHr6m7uvREZGfvXzli1buO+++1i2bBlxcXFcfPHFnT4LEBIS8tXPbrebtra2XoklYGoEezqIPQ02asgY07/U1tYSHR1NTEwMJSUlvPHGG316/8CpEURE0azB0FTt71CMMeYbpk6dSk5ODhMmTGDEiBEcfvjhfXr/AbdmcW5urvZ0YZqyW4eRl3gE0699spejMsb0Rxs2bGDcuHH+DqPPdVZuEVmpqrmdHe/TpiEROUlENonIVhHptGtbRM4VkfUisk5EnvJlPA0SRVBzjS9vYYwxA47PmoZExA08CJwAFAHLRWSxqq7vcMxo4GfA4apaJSIpvooHoNEdQ0hr7w4fM8aYgc6XNYIZwFZV3a6qLcAzwLy9jvk+8KCqVgGoau88HdGF5qAYwtrrfHkLY4wZcHyZCDKAwg7vi7zbOhoDjBGRj0XkUxE5qbMLich8EVkhIivKynr+ZHBrSAwRlgiMMeYbfJkIOpsLde+e6SBgNHA0cAHwNxGJ+9ZJqo+qaq6q5iYnJ/c4oLaQWKK0vsfnG2PMYOTLRFAEDO3wPhMo7uSYF1W1VVW/BDbhJAaf0LA4omlE23vnIQxjjBkMfJkIlgOjRWS4iIQA5wOL9zrmBeAYABFJwmkq2u6ziMKdykZDrT1dbIzxvd6YhhpgwYIF7Ny502dx+mzUkKq2icg1wBuAG1igqutE5HZghaou9u47UUTWA+3AT1W1wlcxufdMRV1TTlS8TwcoGWNMt6ah7o4FCxYwdepUhgwZ0tshAj5+slhVXwVe3Wvbrzv8rMAN3pfP7ZmKene1TUVtjPGvxx57jAcffJCWlhZmz57NAw88gMfj4bLLLmPVqlWoKvPnzyc1NZVVq1Zx3nnnER4ezrJly74x51BvCJgpJgBC451BS7srd/g5EmNMn3vtZtj5Re9ec8ihcPKdB3za2rVref7551myZAlBQUHMnz+fZ555hpEjR1JeXs4XXzhxVldXExcXx5///GceeOABJk+e3LvxewVUIohNzQKgubJwP0caY4zvvP322yxfvpzcXGfGh927dzN06FDmzp3Lpk2buP766znllFM48cQT+ySegEoEyakZtKgbT83eg5eMMYNeD35z9xVV5fLLL+c3v/nNt/atWbOG1157jfvvv59///vfPProoz6PJ2CmoQYICwmmTBJw15f4OxRjTAA7/vjjWbRoEeXl5YAzuqigoICysjJUlXPOOYfbbruNzz77DIDo6Gjq6nz3MGxA1QgAqtxJhO3e5e8wjDEB7NBDD+WWW27h+OOPx+PxEBwczMMPP4zb7eaKK65AVRER7rrrLgAuu+wyrrzySp91FgfUNNQAn951BkNbtpDxqw29GJUxpj+yaai/5rdpqPujpvBUEtorYIAlQGOM8ZWASwQanUY4zXgabaUyY4yBAEwErjjnWYLqXXn+DcQY0ycGWvP3wepJeQMuEYTGZwJQW1rg50iMMb4WFhZGRUVFwCQDVaWiooKwsLADOi/gRg1FpTgPle0uz/dzJMYYX8vMzKSoqIiDWcdkoAkLCyMzM/OAzgm4RJA4ZBjtKrRV2dPFxgx2wcHBDB8+3N9h9HsB1zSUHBvFThJx1VgiMMYYCMBEEOR2UeZKIazBJp4zxhgIwEQAUBOWRkyz7xZ5MMaYgSQgE0FzRAYJnnJob/V3KMYY43cBmQg0bihuPLRWW/OQMcYEZCIISRwGQHXxVj9HYowx/heQiSBqyAgAandu93MkxhjjfwGZCBLTnUTQXJ7n30CMMaYfCMhEkJ4YR4kmIFV5/g7FGGP8LiATQViwmyJXOhF1ef4OxRhj/C4gEwFAVdgwEpps4jljjAnYRNAcO5xorYPGSn+HYowxfhWwicCdMgaAhmJbstIYE9gCNhFEZ4wFoLJgvZ8jMcYY/wrYRDAkawwt6mZ3yUZ/h2KMMX7l00QgIieJyCYR2SoiN3ey/1IRKRORVd7Xlb6Mp6Os5BgKNBVXpT1dbIwJbD5bmEZE3MCDwAlAEbBcRBar6t5tMc+q6jW+iqMroUFu8oOHM7VmPaiCSF+HYIwx/YIvawQzgK2qul1VW4BngHk+vN8BK4qdRnxbKVTaVBPGmMDly0SQAXRcBqzIu21v3xWRNSLynIgM7exCIjJfRFaIyIreXHs0bPTRANRueKfXrmmMMQONLxNBZ20tutf7l4BsVZ0IvA081tmFVPVRVc1V1dzk5OReC3D8odPYqfHUbHi3165pjDEDjS8TQRHQ8Tf8TKC44wGqWqGqzd63fwWm+TCeb8lJj2WFTCBu56fg8fTlrY0xpt/wZSJYDowWkeEiEgKcDyzueICIpHV4ewbQp093uVxC+ZAjiG6vQktW9+WtjTGm3/BZIlDVNuAa4A2cL/hFqrpORG4XkTO8h10nIutEZDVwHXCpr+LpSnTOXDwqVK5+ua9vbYwx/YKo7t1s37/l5ubqihUreu16BRWNVN43h/T4KFJ+/GGvXdcYY/oTEVmpqrmd7QvYJ4v3yEqM4LOQXJJq1tgEdMaYgBTwiQCgZdiRuFDav/zI36EYY0yfs0QAZE6Yw24NoWKdDSM1xgQeSwTArNFprPSMRvI/9ncoxhjT5ywRAIlRoXwZOZnEhi3WT2CMCTiWCLw0ew4ulJbN1jxkjAkslgi8siYdwy6No3b5U/4OxRhj+pQlAq8ZI5NZ7JlDfPH70FDu73CMMabPWCLwiggJYlPqabi1HdYs8nc4xhjTZywRdJA1LpeVntG0L30EPO3+DscYY/qEJYIODh+VxIK2k3FX58HmN/wdjjHG9AlLBB1MyoxledjhlAelwus3Q91Of4dkjDE+Z4mggyC3i3NmZnNl4zV4Gsrg6fOticgYM+hZItjLRTOH8QWjeC7j/0Hx57Bigb9DMsYYnwrydwD9TXpcOBfOyOKmTz0cM3QWyW/dAi43pORA5nTnZ2OMGUSsRtCJX52Ww9SseC4qvwTPkEPh5R/Dgrmw5M/+Ds0YY3qdJYJOhAS5uHHuIWzeHcO/Jz4Cl78Jw4+Ej++Fplp/h2eMMb3KEkEXZo1IZExqFAuXFNKeOQOOvxV2V8H9U+CtW74+0Ba9N8YMcJYIuiAi/PDokawvqeUXz39B25ApcMYDkDbRqRkseQBe/Sn8LgPWPe/vcI0xpsess3gfzpqSybbSBh54bysr86v42yXfYdik8+HvJ8KbvwBxQUwGvPC/ULgcMqfB2NOhrgSWPgJBoTD7WohI8HdRjDGmSwG/eP3+qCpvrNvFj59dxemT0rj77EnOpHQlqyBlvJMMnj4PSjdC226ISgX1QFMNeNpg0gVw5l/6LF5jjOmMLV5/EESEkyYM4cwpGSxeXUxNYytEJsGo4yEmDaJTYf778PNiuOg5SJsMEUnwg//CrKth1VOwa52/i2GMMV2yRNBNFx+WRVOrh/9ZuIyX1xR/+wCXC0afABctgqs/hZSxMOcGCItx+hKsU9kY009ZIuim8emx3HDCGKobW/jxs6tYV1yz/5MiEuDEOyD/Y/j0QShcBq/dDLUlvg/YGGO6yfoIDlBVQwsn3vshAOPTY/i8oJoHL5zKnNFJnZ+gCk9fAJtfA3GDtkNYHFz+hlNrMMaYPnBQfQQiMlJEQr0/Hy0i14lIXG8HOVDER4bw6PemkZMWw/ayBiJC3Nz4r9VO30FnROC8J+CIn8C40+CKt53tr93kJAljjPGz/dYIRGQVkAtkA28Ai4FDVPUUn0fXCX/XCPa2pqia7/xlCVkJEfz5wimMT4/d/0lLH3ESwflPwdhTfR+kMSbgHeyoIY+qtgFnAfeq6o+BtG7e+CQR2SQiW0Xk5n0cd7aIqIh0GmR/NjEzjsevmEFjSztnP/QJr6/tRvt/7hWQOAre/T/rRDbG+F13EkGriFwAXAK87N0WvL+TRMQNPAicDOQAF4hITifHRQPXAUu7G3R/M3tkEouvPZzRqVFc9eRnnHTvhxxx97s88UkeHk8nNS53EBz9MyhdD2/8HCq29XnMxhizR3cSwWXALOAOVf1SRIYDT3bjvBnAVlXdrqotwDPAvE6O+w1wN9DUzZj7pZToMP511SxuOGEMMWHBpESH8asX1/HcyqLOTxh/Fgw/CpY+BP84Ddrb+jZgY4zx2m8iUNX1qnqdqj4tIvFAtKre2Y1rZwCFHd4Xebd9RUSmAENV9WX2QUTmi8gKEVlRVlbWjVv7R2iQm+uOG82iq2bx3FWzGJMaxT+XFXR+sMsNlyyGcx+HumLY/HrfBmuMMV7dGTX0vojEiEgCsBpYKCJ/7Ma1pZNtX7WTiIgL+BPwk/1dSFUfVdVcVc1NTk7uxq39T0Q4b3oWqwur2bhzH1NXH3IqRKfD8r/1XXDGGNNBd5qGYlW1FvgOsFBVpwHHd+O8ImBoh/eZQMdHcqOBCcD7IpIHHAYsHogdxl05a0oGIW4XCz/K6/ogdxDM+D5sfw9WP9tnsRljzB7dSQRBIpIGnMvXncXdsRwYLSLDRSQEOB9n6CkAqlqjqkmqmq2q2cCnwBmq2n/Ghh6khMgQLjosi3+tLGTLrrquD5x9HQybAy9dBx/fB2Wb7RkDY0yf6U4iuB3n+YFtqrpcREYAW/Z3knfI6TXeczcAi1R1nYjcLiJnHEzQA8m1x44mMiSIXzy/luLq3SzZWv7tg9xBcO5jTufxW7+GB6c78xMZY0wfsCkm+sALn+/gR8+uQsT5Rf+R701j7vghnR9cssbpL/jsMTjhN3Do2RAeD8HhfRu0MWZQ2dcDZd15sjgT+DNwOE5n70fA9araxbhI3xqIiQDgqaUFLPuygo0766hoaOHtHx9FbEQXj2O0t8ITZ0Hef533YbFw/WonIRhjTA8c7JPFC3Ha9tNxhn++5N1mDsCFM7O49/wp/OGcSVTUN/Pnd/fRuuYOhktegivegmN/6SxyY8thGmN8pDuJIFlVF6pqm/f1D2BgjOHshyZkxHLOtKE89kke+RUNXR8oAkNnwBE3QvJYG1FkjPGZ7iSCchG5WETc3tfFQIWvAxvMfnLiGNwu4b539tvn7iSEiedB4adw90h46jxY9D/w0Z9sZJExpld0JxFcjjN0dCdQApyNM+2E6aGUmDAumjmMF1cVU1DRuP8Tpl0KU74HY05y5icqXAZv3wrv/daSgTHmoPVo1JCI/EhV7/VBPPs1UDuL91Za28Scu98jNMjFD44cwTXHju7+yaqw+Br4/EmntjDvL84QVGOM6YIvFq+/4SDiMTi1gscum0FOWgz3vbOF6saW7p8sAqf/GY7+Oax5Ft6+xXeBGmMGvZ7+GtnZPELmAM0amUhMeA6n3v8RL60p4XuHDev+yS4XHP3/oKEMPnkAipZDVCqMOg6mXuIkC2OM6Yae1gisYbqX5KTFMHZINP/uarrq/Zn7Wzj2V86zB8Wr4KXr4ZUbrO/AGNNtXSYCEakTkdpOXnU4zxSYXiAinD99KKsKq3lvY+mBXyAoBI68Eea/5zx0Nvs6WLHASQZ5H1tCMMbsV5eJQFWjVTWmk1e0qlrPZC+6cOYwRiRHcvvL62lsOYgFalwuOOF2mH6lkwz+cQosOAnyl/ResMaYQaenTUOmF4UEubj9jAnkVzRw2cLlFFfv7vnFRODUe+D6NXDavVC5DRaeDIsugeb63gvaGDNoWCLoJ+aMTuJP501mRX4Vc+56l6e7Wtmsu+KHQe5lTkI49pewYTHcMxb+dSmUbeqVmI0xg4Mlgn5k3uQM3r/xaHKzE/jdqxsObEhpV0Ii4MifwmWvOTOZbnkbHpwJ/zwHanYc/PWNMQOeJYJ+ZmhCBLfPG099cxs/f/4LGpp7aVH7rMPg9Hvh+lVw1E2Q/wk8Pg/qdvbO9Y0xA1Z31izubPRQoYg8712kxvSysUNiuOGEMby2didTf/MW5z3yCS1tnt65eGQSHPNzuGgR1BTBAzPgw99Dg00fZUyg6k6N4I/AT3GmoM4EbgT+CjwDLPBdaIHtmmNH89xVszl1YhpLv6zk422drGx2MIbNhqv+C1kz4d3/g4fnOInBGBNwupMITlLVR1S1TlVrVfVR4BRVfRawlVJ8aNqweH73nUOJDg3itS9Kev8GSaPhon/B99+Flnp48mxo3sfaysaYQak7icAjIueKiMv7OrfDPntaycdCg9wcn5PKm+t3sTK/sveaiDrKmAbnPQHlm+D5q6BoBXjae/8+xph+qTuJ4CLge0Cp9/U94GIRCcdZnN742GkT06hubOW7D33C3Hs/5FcvrOWNdb3cyTviaDj+Ntj4MvztOOdBtMrtvXsPY0y/ZIvXDwCqyueF1RRWNvLwB9spqGhgd2s7T14xk9mjknr3ZmWboOATeOsWCApzlsxMHtO79zDG9DlbvH6QqW9u48wHP6a0tomHLp7G4b2dDABKN8BjpwPiJIOUsb1/D2NMn7HF6weZqNAgFl46nbTYcC5ZsIy1O2p6/yYp4+DSV5wpK/56LLx9GzRW9v59jDF+Z4vXD1BDEyJY9INZxEUE86sX1+Lx+KCJL/kQuOJNGDPXWSP53onwzu2w7T0oWWMjjIwZJGzx+gEsNiKYn508js8Lqrn+2VW8vnYnO2uaevcm8dlwzkL44RJn0Zv/3gNPnAmPHOHMXfTGL2yqCmMGuO70EWQBDwCzcPoIlgDXqepBzorWM9ZH8E2qyl/e38Y9b27Co5AQGcLCS6czaWicb25YXQg1hc7KaBtegrX/AZfbWSBn6AxIHAUhkb65tzGmxw6qs7iLC9ri9f1MUVUjRVW7uem5NdQ2tfLa9UeQFhvu+xtX5TuL4Gx923kfEg2Hfhem/g+kT7UlM43pJ3yRCApUNasbx50E3Ae4gb+p6p177b8KuBpoB+qB+aq6fl/XtESwb1+WN3Dq/f/lkCHR/PLUcUwbluD7m3o8sOVNaG2ELW/BuuehbTckjobM6TDtEmfSO2OM3/giERSq6tD9HOMGNgMnAEXAcuCCjl/0IhKjqrXen88A/ldVT9rXdS0R7N9Lq4u56bk17G5t5+xpmdx2xngiQ/twUbmmGvjiOdj0qvOUclM1TLsUTrkH3La4nTH+sK9E0NP/ld3JHjOAraq63RvEM8A84KtEsCcJeEV287pmP06flM6xY1N45INtPPDeVlbmV/HQxVMZOySmbwIIi4XpVzivlgb44C74+D5nDeWxp0J0GmRMhSETITisb2IyxnSpyxqBd5H6znYKEL6/dYtF5GycCeuu9L7/HjBTVa/Z67irgRuAEOBYVd3SybXmA/MBsrKypuXn5++vXMbr0+0VXP/M57R74D8/nE1WYoR/AtnwkjMEtWQ1eLxrLLhDIfdymH0txGb4Jy5jAkSvNw1186bnAHP3SgQzVPXaLo6/0Hv8Jfu6rjUNHbitpfWc/fASPB7l+uPHcOnsbNwuP3XiqkL9LqfJaNNrsPopUA+MOQnm3OD0KbhsvSRjepu/EsEs4FZVnet9/zMAVf1dF8e7gCpVjd3XdS0R9MzW0jpuf3kDH24uY/LQOP5wzkRGpUT7OyxnYrvVz8DSR5y+hIgkGH0CjD4RRh4L4T4aBmtMgPFXIgjC6Sw+DtiB01l8oaqu63DM6D1NQSJyOnBLV4HuYYmg51SVl9aUcMuLa2loaWfhpdN9M09RTzTVwOY3nNFHW9+G3VUgbkgdD+NOh8Ovh6BQf0dpzIDll0TgvfEpwL04w0cXqOodInI7sEJVF4vIfcDxQCtQBVzTMVF0xhLBwSura+bCv35KVWMLr153BCkx/azD1tPuNB1tfQsKPoW8/0LSIXDCbXDIyf6OzpgByW+JwBcsEfSOLbvqOOOBj0mLDePxK2aQGe+nTuTu2PwGvP4zqNwGky+Gk37rjEwyxnTbwc4+agah0anRPH7FDMrqmznjgY/5eGsvr4ncm8bMhauXwpE/hVX/hPsmwfoX/R2VMYOGJYIANj07gReuPpykqBCufGwFW0vr/R1S19zBcOwv4QcfQMJIWHQJvHi1U1sYYLVaY/obSwQBbmRyFE9cMZPwEDfXPPUZTa39fK3itElw6csw5SJY9yI8da6ztGbZJn9HZsyAZYnAkBoTxj3nTGLjzjpue2k9jS1t/g5p34LDYd6D8P/ynD+r8uChw+Efpzmdy8aYA2KdxeYrd7yynr/+90uCXMLt8yZw4cz9zivYP9Ttgk8ecCa7qy2G2EyIy4KceRA7FLLnQGiUv6M0xq9s1JDplnaP8s6GXTz+ST5LtpXz0MXTmDt+iL/D6r6mGs1aihgAABWhSURBVHjvt9BQDjtWODUFgKAwGHkcjDzGSRAuN7iCneSQNtl5b8wgZ4nAHJDGljYu/OtSNpTU8uSVM5me3QdTWfc2jwfqd0L5Ftj4ivOqLfr2cZEpTs1hxnxIHtP3cRrTRywRmANW2dDC2Q8toayumSevnOm7Fc/6iurXK6u1t4GnFep2wobFzsgjTxvknAkt9c6azNGpEDfMmRRv/Fm2wI4Z8CwRmB4prt7NeY9+QmV9C78/ZxKnHJrm75B8o6Ec3v8drHvBmQBvwnecZqaS1VC+Gcac7DzEljDC35Ea02OWCEyPldTs5odPfsaqwmqumDOcm08eS7B7kA422/N/Yc9v/+1tsPRhePf/vCuujYLhRzkzpEalQGj016/wBAiJcK5htQfTD1kiMAelpc3Db1/dwD+W5DFtWDw/OXEM04bFExoUIJ2s1YXOegrb34P8JU7z0d5cwTBkApRucDqmD7/eptQ2/YolAtMrXlpdzM3/XkNDSzthwS5OnpDGr07LISEyxN+h9R1PO1Rsc2ZHba6D5lrnz/LNULTcqTWsXwwtdc5opdQJMOl8CImEIYdCyninxmC1BtPHLBGYXlPX1MrS7ZV8sLmMZ5YXEBMWzI1zD+G83KG4/LXYTX/TVAOb34SSVc6U2mUbv94XFuckgaAwZ3rt2EwYdrjzvENkstUgjM9YIjA+sb64llsWr2V5XhXHjU3h0sOzmZ6dQFhwgDQZdYfHA9X5Tk1ixwrI+wjE5YxY+vJDp+9hj+BIiEiA1kYICofDrnIehvO0OzWKuGFOP4QxPWCJwPiMqvLYkjx+++pGWto9pMaE8r9Hj+K86UMtIXRHQznkfwz1pVCxFZpqnS/7sk3OOgx7ixoC8dkQN9QZ4dTSAMERTpNU8iGQNQvam6G+DBpKneu01DsJpSrfOWfksZA4ss+LavzLEoHxuZrdrazIq+SRD7azLK+SuIhgTsxJ5bSJ6RwxOgmxNvEDV7oRKraAO8Tph6j6EirznCemawqcDuqQSKeforrA+ZLvjCvIeU6i4/v4bCeBjDwGYjKcBX/iBsiUIqZHLBGYPqOqfLK9gkXLC3l7Qyn1zW2ckJPKH86ZRGx4sL/DG7zammHXOmdlt9Ao54npyCSIH+YMaS3+zFnlTdth6aPOU9b1pVC4zNkGTnIIj3deIVFQVwKHngtZM519thjQgGaJwPhFc1s7T3ySz12vb2RSZhxPXjnTmov6mz19GOtfcB6ga2lwEkBTrfN8xK61Xx8bP9xZJGjE0RCVCpXbnWTSWOHM1xSV4nSGtzY612mph6QxzquxElLGOX0gTbXOvuZ6p6YSl+XUbKzW6FOWCIxfvbymmGuf/pyRyVFcOCOLw0YkMi4t2pqL+jtV57mIiq3Ol37+EvjyA2hr+uZx4vI+jLe/7xJx+jHKt3xdC9nDFezUOOKyYNhsGDoD2ludPhQRJ/G01Dsd5+Jykk10mpOEaoudZjERaN3t7FOPM6FgZLKTbMLjIX2y0yHvDnLu2VQDrU1OcnIP/tqqJQLjd2+u28kf39rMxp11AGQlRHDKoWnMP3JEYD2HMNC17nZqDg1lzpQbMekQGut88TZWQFO10/cQEumsG1HwiVNriEiCHSud92mTnKamkEjnS72m0PlSbqx0Ek7hUmhvOYCgvM9lqDr3Dg5zEkZTdefHxmc799td6WxyhzoPA0amOMkmdqgzAaEryEkUcVnOy9Pm9NVEpTrHN1ZCY7nzTElkCiQMh6DQg/879hFLBKZfUFVKapr4cHMZr67dycdby0mMDOE3Z07gxJxUqyEYR5O389vldr5gUae5KiTS6Tj3tDlf+HUlToKJSv16KvE9/4ZUnaTS0uB8odfucPpQWuqdh//CE5wv7uAIp/N95xporHJGbFVudxLdARNnNFfyWEif4jwrskf6ZBhxjBOfpx1qipxyVOU5MYbFOvui051EFpnslKGlwak9VeU5TW8xGT2uvVgiMP3SuuIafvzsKjbvqmfWiER+ceo4JmRYh6TpB/b0XwSFOTWW6nwnCYVGO01bVXlOk1JEktPsVF8KlducZrSdX3zzIcI9YjKcTv3m2v3XeMTV+Siwk+6Ew37YoyJZIjD9Vlu7h6eXFfCnt7dQ1djC2VMzuXHuIaTGhO3/ZGP6q7aWr7/IPW2w+mlnhFZIJITFOM1qQWEQPcRJJE21zvF1JU6yqN0B4obwOGd7/HDnuZChM51+lh6wRGD6vZrdrfzlva0s/DgPlwtOm5jOOdMymTE8wZqMjOkFlgjMgFFQ0chDH2zlpdUl1De3MSwxgquPGcW5uUP9HZoxA5olAjPgNLa08franTzxaT6fF1TzkxPGcM2xo6x2YEwP7SsR2FSHpl+KCAniO1Mz+dcPZnHm5HTueWsz3398BY0tbfs/2RhzQHyaCETkJBHZJCJbReTmTvbfICLrRWSNiLwjIsN8GY8ZeILcLv503mR+fVoO724sZf7jKy0ZGNPLfJYIRMQNPAicDOQAF4hIzl6HfQ7kqupE4Dngbl/FYwYuEeHyOcP5/dmT+HhbOafd/xGbvA+mGWMOni9rBDOAraq6XVVbgGeAeR0PUNX3VLXR+/ZTINOH8ZgB7rvTMvnnlTOpb27j/Ec/4fOCKn+HZMyg4MtEkAEUdnhf5N3WlSuA1zrbISLzRWSFiKwoK+vJE39msJg9MolFP5hFeLCbs/6yhF++8AUez8Aa8GBMf+PLRNDZ8I5O/8eKyMVALvD7zvar6qOqmququcnJyb0YohmIspMiee1HR3Lp7Gye/LSAu17fyEAb/WZMfxLkw2sXAR0Hf2cCxXsfJCLHA78AjlLVZh/GYwaR2PBgbjk9h3aP8siH26lvbuMXp44jIsSX/6SNGZx8WSNYDowWkeEiEgKcDyzueICITAEeAc5Q1VIfxmIGIRHhtjPGc9VRI/nn0gKOvPt9PtpS7u+wjBlwfJYIVLUNuAZ4A9gALFLVdSJyu4ic4T3s90AU8C8RWSUii7u4nDGdcrmEm08ey3NXzSIxMoTL/7Gcnz//Ba+v3WnNRcZ0kz1ZbAaN6sYWbli0mhV5ldQ2tTE1K44TcoZw1pQMhsTaJHYmsNkUEyagtHuUJz/N5+llBWzcWYdL4NixqZw+KY3DRiSSEh1qU1WYgGOJwASs/IoGnlleyL9WFFFe74xFSIoKYWJmHMeMTeHc3ExCg2wdZTP4WSIwAc/jUdbsqGFVQRVri2tZkVdJXkUjY1Kj+OHRIzl5QhphwZYQzOBlicCYTry3sZRfL15LYeVuhiVG8ItTxjElK56CygYOzYgjJMjmZDSDhyUCY7rg8Sgfbinj9pfWs7284avtMWFBHD8ulSuPGEFOeowfIzSmd+wrEdjTNyaguVzC0YekcPioJF5ZU0JRVSPZSZG8v6mMN9bt5IVVO7j22NH86PjR1sFsBi2rERjThZrGVm57aR3/+XwH8yanc+2xoxiZHGUJwQxIViMwpgdiI4K559xJDE2I4KEPtvHiqmISI0OYPDSOWSMTmTM6iUNSoy0xmAHPEoEx+yAi/PiEMVw0M4s31+9iVWE1n+VX8c5GZ0aUpKhQctJjSIgI5rSJ6SRHO++D3dbRbAYOaxoypgeKq3fz0dZyPtpSTl5FA4WVjVQ1tgIwKiWKS2YNY+aIREanWFOS6R9s1JAxPtbc1s7KvCpKapp48P2tbC9zRiBlxofzw6NHMj49lokZsbhclhSMf1giMKYPqSoFlY0s3V7Jk0vzWVNUA8DolCj+95iRTMtKICY8iLiIED9HagKJJQJj/MTjUTaX1rF2Ry1//XA7m3Y5ay2HBrmYNzmdYYmRTMqMY+aIBOtXMD5lo4aM8ROXSxg7JIaxQ2L4zpQMPthSRlldM0u3V/La2p3UNbUBkBoTykUzh3HMISnERwaTHhtuzUimz1iNwBg/qmtq5ZNtFTy5tIAPN3+9HndiZAi/OXMCJ08YYp3NpldY05AxA8CX5Q1s2VVHeX0Lzy4vYHVRDSOSIjk0M5YJ6bEcn5PK8KRIf4dpBihLBMYMMK3tHp5bWcRra3eyrbSeHdW7EYEjRyeTOyyeCRmxpMeFkxQVQnxEiDUjmf2yPgJjBphgt4sLZmRxwYwswHlu4amlBbzyRQkfdGhCAkiIDGHWiERmjUzk8FFJZCdGWHOSOSBWIzBmgKltamVjSR2ldU2U1TWzdkctS7aVU1LTBEBabBgzhyeQEBnK7tY2osOCOWtKBolRIQS7XMRFBFuiCEBWIzBmEIkJC2bG8IRvbFNV8ioaWbKtnCXbKliyrYLdLe2Eh7ipbmzl0Q+3f3XsxMxYjh2bQlpsGPMmZ9iCPMZqBMYMdmV1zby3qZSWNg91TW08tSyfwsrdAMSGB3PMIckcOy6V48amEBlqvxsOVtZZbIz5iqriUVieV8miFYW8v6mMyoYWwoPdTMmKY0pWHEeMTiYmLJhxaTa76mBhicAY06V2j/JZQRUvrtrBFztqWbujhnaP870wPj2G48alMm1YPFOy4ogJC/ZztKanrI/AGNMlt0uYnp3A9Gyn36G0rokNJXUUVjby7PJCHnh3Cx4FETg0I5a544cwMjmSyNAgQoPcRIUGMTQhnGhLEgOWJQJjzDekRIeREh0GwMWHDaO+uY3VhdWsyKvinY27+P0bm751TpBLmJoVz4zhCUSEugl2uQhyO9NrTMyMRYEo63/ot6xpyBhzQGoaWymsaqSptZ2mVg91Ta18saOGD7eUsXZHbZfnjUqJYu74VFrbFREIcbuIDgvi8FFJjBsSYw/F+Zj1ERhj+kRbu4c2j9La7qGlzcMn2ysorNxNu8fDkm0VfLK9gmC3C1WlzaPs+fpJjAxhRHIkmfERTBsWz9zxQ0iODvVvYQYZvyUCETkJuA9wA39T1Tv32n8kcC8wEThfVZ/b3zUtERgzcNU1tRIREsSeX/5L65r575Zylm6voKCykYLKxq8ejEuODiU+IphDM+LIzY4nKyGC+IgQEiJDiIsItucfDpBfEoGIuIHNwAlAEbAcuEBV13c4JhuIAW4EFlsiMCawqSobSur4aGsZ20obKK9v5vPCaiobWr51bESIm/iIEOIjg79KEHv+TIsNY2JmHElRIQS5XE6/RYCv9+CvUUMzgK2qut0bxDPAPOCrRKCqed59Hh/GYYwZIESEnPQYctJjvtqmquRXNLKztomqhhYqG1uobmylsqGFqoYWqhpbqGxsJb+ikarGlq/WeOgoLNhF7rAEDhuRQG52AkNiwogKCyI82E1YsBt3gPdP+DIRZACFHd4XATN7ciERmQ/MB8jKyjr4yIwxA4aIkJ0USXY3p+Buam1nR/Vu1hRVU9fURlu7s3Top9sr+MObm791fHRoEEcdkszwpEjSYsPJjA9naEIE8RHBhAa5CQ8Z/E1QvkwEnaXYHrVDqeqjwKPgNA0dTFDGmMEtLNjNyOQoRiZHfWtfVUMLq4qqqWpwag5Nre1sKa1nydZyXv2iBM9e3y5ul5CTFkNmfDiqEBHqZtyQGI72Jo6gQdLc5MtEUAQM7fA+Eyj24f2MMWaf4iNDOOaQlE73tbV72FXXTFFlI4VVu6nZ3UpVQwufF1axeVcdLhEamtv4z2c7uOPVDQS7haHxEQCMS4thTGo0abFhHJoZy+iUqAGVJHyZCJYDo0VkOLADOB+40If3M8aYHgtyu8iICycjLnyfbdgFFY0sz6tka1k9BRWNeFRZmV/FK1+UfHVMaJCLcWkxpMWGER8ZwsSMWE6blN5vH6rz9fDRU3CGh7qBBap6h4jcDqxQ1cUiMh14HogHmoCdqjp+X9e0UUPGmP6otd1DYWUjX+yoYU1RDeuLaymvb6a8vpmqxlbASRDgtJGrKu0eJTY8mEOGRHNIajSZ8RGkx4WTHhdGelw4yVGhvfagnT1QZowxfqKqrMivYun2Cmqb2hABlwiC0wdRXt/Mxp11bNlVT33zN0c8BbuFmLBgGlvaSY4O5ScnjmHe5IwexWGTzhljjJ+IfHNSv66oKrVNbRRX73ZeNU0UV++murGViBA3pXXNJEb65mlrSwTGGNMPiAix4cHEhgczLi1m/yf0ooHTrW2MMcYnLBEYY0yAs0RgjDEBzhKBMcYEOEsExhgT4CwRGGNMgLNEYIwxAc4SgTHGBLgBN8WEiJQB+T08PQko78Vw+jMr6+BkZR2c+qKsw1Q1ubMdAy4RHAwRWdHVXBuDjZV1cLKyDk7+Lqs1DRljTICzRGCMMQEu0BLBo/4OoA9ZWQcnK+vg5NeyBlQfgTHGmG8LtBqBMcaYvVgiMMaYABcwiUBEThKRTSKyVURu9nc8vU1E8kTkCxFZJSIrvNsSROQtEdni/TPe33H2hIgsEJFSEVnbYVunZRPH/d7PeY2ITPVf5Aeui7LeKiI7vJ/tKu9a4Hv2/cxb1k0iMtc/UfeMiAwVkfdEZIOIrBOR673bB9Vnu49y9p/PVVUH/QtwA9uAEUAIsBrI8XdcvVzGPCBpr213Azd7f74ZuMvfcfawbEcCU4G1+ysbcArwGiDAYcBSf8ffC2W9Fbixk2NzvP+WQ4Hh3n/jbn+X4QDKmgZM9f4cDWz2lmlQfbb7KGe/+VwDpUYwA9iqqttVtQV4Bpjn55j6wjzgMe/PjwFn+jGWHlPVD4HKvTZ3VbZ5wOPq+BSIE5G0von04HVR1q7MA55R1WZV/RLYivNvfUBQ1RJV/cz7cx2wAchgkH22+yhnV/r8cw2URJABFHZ4X8S+P4iBSIE3RWSliMz3bktV1RJw/jECKX6Lrvd1VbbB+llf420OWdChiW/QlFVEsoEpwFIG8We7Vzmhn3yugZIIpJNtg23c7OGqOhU4GbhaRI70d0B+Mhg/64eAkcBkoAS4x7t9UJRVRKKAfwM/UtXafR3aybYBU95OytlvPtdASQRFwNAO7zOBYj/F4hOqWuz9sxR4HqcquWtP1dn7Z6n/Iux1XZVt0H3WqrpLVdtV1QP8la+bCQZ8WUUkGOfL8Z+q+h/v5kH32XZWzv70uQZKIlgOjBaR4SISApwPLPZzTL1GRCJFJHrPz8CJwFqcMl7iPewS4EX/ROgTXZVtMfA/3hEmhwE1e5oZBqq92sHPwvlswSnr+SISKiLDgdHAsr6Or6dERIC/AxtU9Y8ddg2qz7arcvarz9XfPep99cIZcbAZpwf+F/6Op5fLNgJnlMFqYN2e8gGJwDvAFu+fCf6OtYflexqn6tyK89vSFV2VDada/aD3c/4CyPV3/L1Q1ie8ZVmD8yWR1uH4X3jLugk42d/xH2BZ5+A0eawBVnlfpwy2z3Yf5ew3n6tNMWGMMQEuUJqGjDHGdMESgTHGBDhLBMYYE+AsERhjTICzRGCMMQHOEoExxgQ4SwTGGBPg/j9yvxeDucNfVwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results =clf.evals_result()\n",
    "epochs = len(results['validation_0']['error'])\n",
    "x_axis = range(0, epochs)\n",
    "# plot log loss\n",
    "fig, ax = pyplot.subplots()\n",
    "ax.plot(x_axis, results['validation_0']['logloss'], label='Train')\n",
    "ax.plot(x_axis, results['validation_1']['logloss'], label='Test')\n",
    "ax.legend()\n",
    "pyplot.ylabel('Log Loss')\n",
    "pyplot.title('XGBoost Log Loss')\n",
    "pyplot.show()\n",
    "# plot classification error\n",
    "fig, ax = pyplot.subplots()\n",
    "ax.plot(x_axis, results['validation_0']['error'], label='Train')\n",
    "ax.plot(x_axis, results['validation_1']['error'], label='Test')\n",
    "ax.legend()\n",
    "pyplot.ylabel('Classification Error')\n",
    "pyplot.title('XGBoost Classification Error')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3497  554]\n",
      " [ 114 4000]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.86      0.91      4051\n",
      "           1       0.88      0.97      0.92      4114\n",
      "\n",
      "    accuracy                           0.92      8165\n",
      "   macro avg       0.92      0.92      0.92      8165\n",
      "weighted avg       0.92      0.92      0.92      8165\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 4554, 0: 3611})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections.Counter(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction on AFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.10165604, -1.21512452, -0.17738113,  0.85520398, -0.11471561],\n",
       "       [-0.12573111,  1.1976657 ,  2.8932032 ,  3.28815296,  0.73442831],\n",
       "       [-1.09977777,  0.47995172,  3.51393893,  0.85520398, -0.38551987],\n",
       "       ...,\n",
       "       [ 0.7737926 ,  0.67654986, -0.26117054,  0.24696674,  2.43363096],\n",
       "       [ 0.78528316,  0.68837531, -0.26117054,  0.24696674,  2.43363096],\n",
       "       [ 0.74142013,  0.64323392, -0.26117054,  0.24696674,  2.43363096]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X0 = scaler.fit_transform(file)\n",
    "X0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction0=clf.predict(X0)\n",
    "prediction0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 176144, 0: 169566})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections.Counter(prediction0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5095137543027393"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "176144/(176144+169566)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
