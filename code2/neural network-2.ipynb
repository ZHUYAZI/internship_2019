{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from My_count import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ot</th>\n",
       "      <th>Dt</th>\n",
       "      <th>DUREE</th>\n",
       "      <th>NBTRAJTC</th>\n",
       "      <th>dist</th>\n",
       "      <th>parking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18300.0</td>\n",
       "      <td>19800.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20362.956563</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34200.0</td>\n",
       "      <td>35340.0</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1941.648784</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36000.0</td>\n",
       "      <td>37800.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>23648.467181</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27000.0</td>\n",
       "      <td>31800.0</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>35474.638828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27000.0</td>\n",
       "      <td>32400.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>35713.582850</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30000.0</td>\n",
       "      <td>32400.0</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13400.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>27900.0</td>\n",
       "      <td>33600.0</td>\n",
       "      <td>5700.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>32614.260685</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>48600.0</td>\n",
       "      <td>51300.0</td>\n",
       "      <td>2700.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16319.620094</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>19800.0</td>\n",
       "      <td>22500.0</td>\n",
       "      <td>2700.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19009.471324</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>30000.0</td>\n",
       "      <td>30300.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>66000.0</td>\n",
       "      <td>66600.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1421.267040</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>21600.0</td>\n",
       "      <td>27000.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>41824.872982</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>25200.0</td>\n",
       "      <td>32400.0</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24935.917870</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>27000.0</td>\n",
       "      <td>31500.0</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30084.215130</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>24300.0</td>\n",
       "      <td>29100.0</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20260.799589</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>25200.0</td>\n",
       "      <td>27060.0</td>\n",
       "      <td>1860.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13061.393494</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>25200.0</td>\n",
       "      <td>29040.0</td>\n",
       "      <td>3840.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>28540.847920</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>26100.0</td>\n",
       "      <td>30600.0</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>39805.778475</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>47700.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6484.597135</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>26100.0</td>\n",
       "      <td>30600.0</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>35467.449866</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>26700.0</td>\n",
       "      <td>31500.0</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30620.581314</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>27000.0</td>\n",
       "      <td>33300.0</td>\n",
       "      <td>6300.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28338.136848</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>25800.0</td>\n",
       "      <td>31500.0</td>\n",
       "      <td>5700.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>41388.162559</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>22800.0</td>\n",
       "      <td>29400.0</td>\n",
       "      <td>6600.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>62309.068361</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>29400.0</td>\n",
       "      <td>35700.0</td>\n",
       "      <td>6300.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>41629.316593</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26100.0</td>\n",
       "      <td>30600.0</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>45632.554169</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>18600.0</td>\n",
       "      <td>27000.0</td>\n",
       "      <td>8400.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>54800.091241</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>18000.0</td>\n",
       "      <td>24300.0</td>\n",
       "      <td>6300.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>65060.971404</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>22800.0</td>\n",
       "      <td>27000.0</td>\n",
       "      <td>4200.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24087.548651</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>24300.0</td>\n",
       "      <td>29400.0</td>\n",
       "      <td>5100.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>46044.000695</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21489</th>\n",
       "      <td>57600.0</td>\n",
       "      <td>64800.0</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>45449.312426</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21490</th>\n",
       "      <td>62100.0</td>\n",
       "      <td>63900.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16530.275255</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21491</th>\n",
       "      <td>60300.0</td>\n",
       "      <td>62100.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5122.499390</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21492</th>\n",
       "      <td>71100.0</td>\n",
       "      <td>71700.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6365.532185</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21493</th>\n",
       "      <td>70200.0</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6484.597135</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21494</th>\n",
       "      <td>62400.0</td>\n",
       "      <td>62700.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2418.677324</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21495</th>\n",
       "      <td>72000.0</td>\n",
       "      <td>72120.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>806.225775</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21496</th>\n",
       "      <td>45000.0</td>\n",
       "      <td>46800.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7317.103252</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21497</th>\n",
       "      <td>63900.0</td>\n",
       "      <td>65400.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8905.054744</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21498</th>\n",
       "      <td>66600.0</td>\n",
       "      <td>68700.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>412.310563</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21499</th>\n",
       "      <td>61200.0</td>\n",
       "      <td>65400.0</td>\n",
       "      <td>4200.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>31196.474160</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21500</th>\n",
       "      <td>58500.0</td>\n",
       "      <td>59400.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4045.985665</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21501</th>\n",
       "      <td>69300.0</td>\n",
       "      <td>70200.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5385.164807</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21502</th>\n",
       "      <td>60900.0</td>\n",
       "      <td>61200.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>412.310563</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21503</th>\n",
       "      <td>72900.0</td>\n",
       "      <td>73500.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2400.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21504</th>\n",
       "      <td>47700.0</td>\n",
       "      <td>49800.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5685.068161</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21505</th>\n",
       "      <td>71100.0</td>\n",
       "      <td>72900.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18917.980865</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21506</th>\n",
       "      <td>54000.0</td>\n",
       "      <td>63000.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>47071.222631</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21507</th>\n",
       "      <td>73800.0</td>\n",
       "      <td>74700.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21508</th>\n",
       "      <td>64800.0</td>\n",
       "      <td>65700.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>447.213595</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21509</th>\n",
       "      <td>63000.0</td>\n",
       "      <td>65100.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4024.922359</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21510</th>\n",
       "      <td>73800.0</td>\n",
       "      <td>75600.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10748.023074</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21511</th>\n",
       "      <td>67800.0</td>\n",
       "      <td>72600.0</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>29138.634148</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21512</th>\n",
       "      <td>90000.0</td>\n",
       "      <td>90300.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21513</th>\n",
       "      <td>64500.0</td>\n",
       "      <td>67800.0</td>\n",
       "      <td>3300.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16737.084573</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21514</th>\n",
       "      <td>61800.0</td>\n",
       "      <td>63000.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2195.449840</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21515</th>\n",
       "      <td>62400.0</td>\n",
       "      <td>68280.0</td>\n",
       "      <td>5880.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>48206.327386</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21516</th>\n",
       "      <td>63900.0</td>\n",
       "      <td>69000.0</td>\n",
       "      <td>5100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>36748.469356</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21517</th>\n",
       "      <td>79500.0</td>\n",
       "      <td>82500.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>36531.219525</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21518</th>\n",
       "      <td>84600.0</td>\n",
       "      <td>86400.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5825.804665</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21519 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Ot       Dt   DUREE  NBTRAJTC          dist  parking\n",
       "0      18300.0  19800.0  1500.0       1.0  20362.956563        1\n",
       "1      34200.0  35340.0  1140.0       1.0   1941.648784        1\n",
       "2      36000.0  37800.0  1800.0       2.0  23648.467181        1\n",
       "3      27000.0  31800.0  4800.0       3.0  35474.638828        1\n",
       "4      27000.0  32400.0  5400.0       3.0  35713.582850        1\n",
       "...        ...      ...     ...       ...           ...      ...\n",
       "21514  61800.0  63000.0  1200.0       1.0   2195.449840        0\n",
       "21515  62400.0  68280.0  5880.0       2.0  48206.327386        0\n",
       "21516  63900.0  69000.0  5100.0       2.0  36748.469356        0\n",
       "21517  79500.0  82500.0  3000.0       2.0  36531.219525        0\n",
       "21518  84600.0  86400.0  1800.0       1.0   5825.804665        0\n",
       "\n",
       "[21519 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1_true=pd.read_csv('/home/yazi/Documents/stage/data/data1_true.csv',delimiter=';',encoding='iso 8859-1')\n",
    "data2_true=pd.read_csv('/home/yazi/Documents/stage/data/data2_true.csv',delimiter=';',encoding='iso 8859-1')\n",
    "data1_false=pd.read_csv('/home/yazi/Documents/stage/data/data1_false.csv',delimiter=';',encoding='iso 8859-1')\n",
    "data2_false=pd.read_csv('/home/yazi/Documents/stage/data/data2_false.csv',delimiter=';',encoding='iso 8859-1')\n",
    "Data_Train=pd.concat([data1_true,data2_true,data1_false,data2_false],ignore_index=True)\n",
    "Data_Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Data_Train.drop('parking', axis=1)\n",
    "y = Data_Train['parking']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.5958114 , -1.66829122, -0.57514726, -0.89708632,  0.93431633],\n",
       "       [-0.77721666, -0.86264201, -0.75408122, -0.89708632, -0.61836558],\n",
       "       [-0.68454556, -0.73510681, -0.42603562,  0.23129809,  1.21124307],\n",
       "       ...,\n",
       "       [ 0.75185653,  0.88241283,  1.21419238,  0.23129809,  2.31540661],\n",
       "       [ 1.55500608,  1.58230113,  0.17041092,  0.23129809,  2.29709521],\n",
       "       [ 1.81757421,  1.78449109, -0.42603562, -0.89708632, -0.29098065]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=np.array(y.astype(float))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1.0: 1108, 0.0: 20411}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test=y_test.reshape([1,len(y_test)])\n",
    "y_train=y_train.reshape([1,len(y_train)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.transpose(X_train)\n",
    "X_test=np.transpose(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape is  (5, 17215)\n",
      "y_train shape is  (1, 17215)\n",
      "X_test shape is  (5, 4304)\n",
      "y_test shape is  (1, 4304)\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape is ',X_train.shape)\n",
    "print('y_train shape is ',y_train.shape)\n",
    "print('X_test shape is ',X_test.shape)\n",
    "print('y_test shape is ',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1108.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_test)+np.sum(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_x,n_y):\n",
    "    #n_x=5,n_y=1\n",
    "    X=tf.placeholder(tf.float32,[n_x,None],name=\"X\")\n",
    "    Y=tf.placeholder(tf.float32,[n_y,None],name=\"Y\")\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y=create_placeholders(5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'X:0' shape=(5, ?) dtype=float32>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Y:0' shape=(1, ?) dtype=float32>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    W1=tf.get_variable('W1',[4,5],initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b1=tf.get_variable('b1',[4,1],initializer=tf.zeros_initializer())\n",
    "    W2=tf.get_variable('W2',[1,4],initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b2=tf.get_variable('b2',[1,1],initializer=tf.zeros_initializer())\n",
    "    \n",
    "    parameters={'W1':W1,\n",
    "                'b1':b1,\n",
    "                'W2':W2,\n",
    "                'b2':b2\n",
    "               }\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0902 12:52:36.547972 139676115695424 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = <tf.Variable 'W1:0' shape=(4, 5) dtype=float32_ref>\n",
      "b1 = <tf.Variable 'b1:0' shape=(4, 1) dtype=float32_ref>\n",
      "W2 = <tf.Variable 'W2:0' shape=(1, 4) dtype=float32_ref>\n",
      "b2 = <tf.Variable 'b2:0' shape=(1, 1) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    parameters = initialize_parameters()\n",
    "    print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "    print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "    print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "    print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X,parameters):\n",
    "    \"\"\"\n",
    "    LINEAR -> RELU -> LINEAR -> Sigmoid\n",
    "\n",
    "    in：\n",
    "        X - data[5,none]\n",
    "        parameters - :W,b\n",
    "\n",
    "    return：\n",
    "        Z2 - last linear\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "\n",
    "    Z1 = tf.add(tf.matmul(W1,X),b1)        # Z1 = np.dot(W1, X) + b1\n",
    "    A1 = tf.nn.relu(Z1)                    # A1 = relu(Z1)\n",
    "    Z2 = tf.add(tf.matmul(W2, A1), b2)     # Z2 = np.dot(W2, a1) + b2\n",
    "\n",
    "\n",
    "    return Z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z2 = Tensor(\"Add_1:0\", shape=(1, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    X,Y = create_placeholders(5,1)\n",
    "    parameters = initialize_parameters()\n",
    "    Z2 = forward_propagation(X,parameters)\n",
    "    print(\"Z2 = \" + str(Z2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Z2,Y):\n",
    "    logits = tf.transpose(Z2) \n",
    "    labels = tf.transpose(Y)  \n",
    "    cost=tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(labels=Y,logits=Z2,pos_weight=18))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = Tensor(\"Mean:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    X,Y = create_placeholders(5,1)\n",
    "    parameters = initialize_parameters()\n",
    "    Z2 = forward_propagation(X,parameters)\n",
    "    cost = compute_cost(Z2,Y)\n",
    "    print(\"cost = \" + str(cost))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train,Y_train,X_test,Y_test,\n",
    "        learning_rate=0.0001,num_epochs=10000,print_cost=True,is_plot=True):\n",
    "    \"\"\"\n",
    "    实现一个2层的TensorFlow神经网络：LINEAR->RELU->LINEAR->Sigmoid\n",
    "\n",
    "    参数：\n",
    "        X_train - 训练集，维度为（输入大小（输入节点数量） = 12288, 样本数量 = 1080）\n",
    "        Y_train - 训练集分类数量，维度为（输出大小(输出节点数量) = 6, 样本数量 = 1080）\n",
    "        X_test - 测试集，维度为（输入大小（输入节点数量） = 12288, 样本数量 = 120）\n",
    "        Y_test - 测试集分类数量，维度为（输出大小(输出节点数量) = 6, 样本数量 = 120）\n",
    "        learning_rate - 学习速率\n",
    "        num_epochs - 整个训练集的遍历次数\n",
    "        mini_batch_size - 每个小批量数据集的大小\n",
    "        print_cost - 是否打印成本，每100代打印一次\n",
    "        is_plot - 是否绘制曲线图\n",
    "\n",
    "    返回：\n",
    "        parameters - 学习后的参数\n",
    "\n",
    "    \"\"\"\n",
    "    ops.reset_default_graph()                #能够重新运行模型而不覆盖tf变量\n",
    "    (n_x , m)  = X_train.shape               #获取输入节点数量和样本数\n",
    "    n_y = Y_train.shape[0]                   #获取输出节点数量\n",
    "    costs = []                               #成本集\n",
    "\n",
    "    #给X和Y创建placeholder\n",
    "    X,Y = create_placeholders(n_x,n_y)\n",
    "\n",
    "    #初始化参数\n",
    "    parameters = initialize_parameters()\n",
    "\n",
    "    #前向传播\n",
    "    Z2 = forward_propagation(X,parameters)\n",
    "\n",
    "    #计算成本\n",
    "    cost = compute_cost(Z2,Y)\n",
    "#     yazi=tf.metrics.recall(labels=Y,predictions=Z2)\n",
    "    \n",
    "    #反向传播，使用Adam优化\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    #初始化所有的变量\n",
    "    init_g = tf.global_variables_initializer()\n",
    "    init_l = tf.local_variables_initializer()\n",
    "    #开始会话并计算\n",
    "    with tf.Session() as sess:\n",
    "        #初始化\n",
    "        sess.run(init_g)\n",
    "        sess.run(init_l)\n",
    "        #正常训练的循环\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0  #每代的成本\n",
    "\n",
    "            #数据已经准备好了，开始运行session\n",
    "            _ , minibatch_cost = sess.run([optimizer,cost],feed_dict={X:X_train,Y:Y_train})\n",
    "\n",
    "            #计算这个minibatch在这一代中所占的误差\n",
    "            epoch_cost =  minibatch_cost\n",
    "\n",
    "            #记录并打印成本\n",
    "            ## 记录成本\n",
    "            if epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                #是否打印：\n",
    "                if print_cost and epoch % 100 == 0:\n",
    "                        print(\"epoch = \" + str(epoch) + \"    epoch_cost = \" + str(epoch_cost))\n",
    "\n",
    "        #是否绘制图谱\n",
    "        if is_plot:\n",
    "            plt.plot(np.squeeze(costs))\n",
    "            plt.ylabel('cost')\n",
    "            plt.xlabel('iterations (per tens)')\n",
    "            plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "            plt.show()\n",
    "\n",
    "        #保存学习后的参数\n",
    "        parameters = sess.run(parameters)\n",
    "        print(\"save the parameter to session.\")\n",
    "        \n",
    "       \n",
    "        #计算当前的预测结果\n",
    "        predicted=tf.nn.sigmoid(Z2)\n",
    "        correct_prediction = tf.equal(tf.round(predicted),Y)\n",
    "\n",
    "        #计算准确率\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction,\"float\"))\n",
    "\n",
    "        print(\"train accuracy :\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "        print(\"test accuracy :\", accuracy.eval({X: X_test, Y: Y_test}))\n",
    "        \n",
    "       \n",
    "        \n",
    "        return parameters\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0    epoch_cost = 1.3953665\n",
      "epoch = 100    epoch_cost = 1.3743997\n",
      "epoch = 200    epoch_cost = 1.3544697\n",
      "epoch = 300    epoch_cost = 1.335578\n",
      "epoch = 400    epoch_cost = 1.3175089\n",
      "epoch = 500    epoch_cost = 1.300437\n",
      "epoch = 600    epoch_cost = 1.2841803\n",
      "epoch = 700    epoch_cost = 1.2686554\n",
      "epoch = 800    epoch_cost = 1.2539538\n",
      "epoch = 900    epoch_cost = 1.2403297\n",
      "epoch = 1000    epoch_cost = 1.2274845\n",
      "epoch = 1100    epoch_cost = 1.2153327\n",
      "epoch = 1200    epoch_cost = 1.2039729\n",
      "epoch = 1300    epoch_cost = 1.1933334\n",
      "epoch = 1400    epoch_cost = 1.1832931\n",
      "epoch = 1500    epoch_cost = 1.1737685\n",
      "epoch = 1600    epoch_cost = 1.164845\n",
      "epoch = 1700    epoch_cost = 1.1564806\n",
      "epoch = 1800    epoch_cost = 1.1486137\n",
      "epoch = 1900    epoch_cost = 1.141101\n",
      "epoch = 2000    epoch_cost = 1.1339577\n",
      "epoch = 2100    epoch_cost = 1.1272578\n",
      "epoch = 2200    epoch_cost = 1.1211095\n",
      "epoch = 2300    epoch_cost = 1.1153455\n",
      "epoch = 2400    epoch_cost = 1.1099494\n",
      "epoch = 2500    epoch_cost = 1.1048853\n",
      "epoch = 2600    epoch_cost = 1.1001661\n",
      "epoch = 2700    epoch_cost = 1.0957654\n",
      "epoch = 2800    epoch_cost = 1.0917729\n",
      "epoch = 2900    epoch_cost = 1.0881058\n",
      "epoch = 3000    epoch_cost = 1.0847688\n",
      "epoch = 3100    epoch_cost = 1.0818127\n",
      "epoch = 3200    epoch_cost = 1.0790195\n",
      "epoch = 3300    epoch_cost = 1.0764245\n",
      "epoch = 3400    epoch_cost = 1.0739871\n",
      "epoch = 3500    epoch_cost = 1.0716833\n",
      "epoch = 3600    epoch_cost = 1.0695512\n",
      "epoch = 3700    epoch_cost = 1.0676038\n",
      "epoch = 3800    epoch_cost = 1.0659108\n",
      "epoch = 3900    epoch_cost = 1.0643982\n",
      "epoch = 4000    epoch_cost = 1.0630356\n",
      "epoch = 4100    epoch_cost = 1.0617682\n",
      "epoch = 4200    epoch_cost = 1.0605749\n",
      "epoch = 4300    epoch_cost = 1.0594375\n",
      "epoch = 4400    epoch_cost = 1.0583609\n",
      "epoch = 4500    epoch_cost = 1.0573963\n",
      "epoch = 4600    epoch_cost = 1.0564787\n",
      "epoch = 4700    epoch_cost = 1.0555698\n",
      "epoch = 4800    epoch_cost = 1.054738\n",
      "epoch = 4900    epoch_cost = 1.0538299\n",
      "epoch = 5000    epoch_cost = 1.0529299\n",
      "epoch = 5100    epoch_cost = 1.0520253\n",
      "epoch = 5200    epoch_cost = 1.0511087\n",
      "epoch = 5300    epoch_cost = 1.0502214\n",
      "epoch = 5400    epoch_cost = 1.0493808\n",
      "epoch = 5500    epoch_cost = 1.048568\n",
      "epoch = 5600    epoch_cost = 1.0477871\n",
      "epoch = 5700    epoch_cost = 1.0470017\n",
      "epoch = 5800    epoch_cost = 1.046228\n",
      "epoch = 5900    epoch_cost = 1.045497\n",
      "epoch = 6000    epoch_cost = 1.0447959\n",
      "epoch = 6100    epoch_cost = 1.044114\n",
      "epoch = 6200    epoch_cost = 1.0434307\n",
      "epoch = 6300    epoch_cost = 1.0427655\n",
      "epoch = 6400    epoch_cost = 1.0421278\n",
      "epoch = 6500    epoch_cost = 1.0415093\n",
      "epoch = 6600    epoch_cost = 1.0409175\n",
      "epoch = 6700    epoch_cost = 1.0403402\n",
      "epoch = 6800    epoch_cost = 1.0397683\n",
      "epoch = 6900    epoch_cost = 1.0392044\n",
      "epoch = 7000    epoch_cost = 1.0386629\n",
      "epoch = 7100    epoch_cost = 1.0381112\n",
      "epoch = 7200    epoch_cost = 1.0375814\n",
      "epoch = 7300    epoch_cost = 1.0370538\n",
      "epoch = 7400    epoch_cost = 1.0365417\n",
      "epoch = 7500    epoch_cost = 1.0360377\n",
      "epoch = 7600    epoch_cost = 1.0355477\n",
      "epoch = 7700    epoch_cost = 1.035079\n",
      "epoch = 7800    epoch_cost = 1.0346216\n",
      "epoch = 7900    epoch_cost = 1.0341858\n",
      "epoch = 8000    epoch_cost = 1.0337547\n",
      "epoch = 8100    epoch_cost = 1.0333122\n",
      "epoch = 8200    epoch_cost = 1.0328852\n",
      "epoch = 8300    epoch_cost = 1.0324546\n",
      "epoch = 8400    epoch_cost = 1.0320225\n",
      "epoch = 8500    epoch_cost = 1.0315858\n",
      "epoch = 8600    epoch_cost = 1.0311402\n",
      "epoch = 8700    epoch_cost = 1.030656\n",
      "epoch = 8800    epoch_cost = 1.0301245\n",
      "epoch = 8900    epoch_cost = 1.029441\n",
      "epoch = 9000    epoch_cost = 1.0284885\n",
      "epoch = 9100    epoch_cost = 1.0274053\n",
      "epoch = 9200    epoch_cost = 1.0260663\n",
      "epoch = 9300    epoch_cost = 1.0249995\n",
      "epoch = 9400    epoch_cost = 1.0239905\n",
      "epoch = 9500    epoch_cost = 1.0230542\n",
      "epoch = 9600    epoch_cost = 1.0221503\n",
      "epoch = 9700    epoch_cost = 1.0213262\n",
      "epoch = 9800    epoch_cost = 1.0205913\n",
      "epoch = 9900    epoch_cost = 1.0198756\n",
      "epoch = 10000    epoch_cost = 1.0191617\n",
      "epoch = 10100    epoch_cost = 1.0184631\n",
      "epoch = 10200    epoch_cost = 1.017766\n",
      "epoch = 10300    epoch_cost = 1.0170808\n",
      "epoch = 10400    epoch_cost = 1.0164204\n",
      "epoch = 10500    epoch_cost = 1.0157747\n",
      "epoch = 10600    epoch_cost = 1.0151356\n",
      "epoch = 10700    epoch_cost = 1.0145112\n",
      "epoch = 10800    epoch_cost = 1.0138996\n",
      "epoch = 10900    epoch_cost = 1.0132923\n",
      "epoch = 11000    epoch_cost = 1.0126892\n",
      "epoch = 11100    epoch_cost = 1.0120912\n",
      "epoch = 11200    epoch_cost = 1.0115116\n",
      "epoch = 11300    epoch_cost = 1.0109464\n",
      "epoch = 11400    epoch_cost = 1.010396\n",
      "epoch = 11500    epoch_cost = 1.0098411\n",
      "epoch = 11600    epoch_cost = 1.0092926\n",
      "epoch = 11700    epoch_cost = 1.0087571\n",
      "epoch = 11800    epoch_cost = 1.0082521\n",
      "epoch = 11900    epoch_cost = 1.0077575\n",
      "epoch = 12000    epoch_cost = 1.0072688\n",
      "epoch = 12100    epoch_cost = 1.0067539\n",
      "epoch = 12200    epoch_cost = 1.0062655\n",
      "epoch = 12300    epoch_cost = 1.0057544\n",
      "epoch = 12400    epoch_cost = 1.0052899\n",
      "epoch = 12500    epoch_cost = 1.0048195\n",
      "epoch = 12600    epoch_cost = 1.0043583\n",
      "epoch = 12700    epoch_cost = 1.0039275\n",
      "epoch = 12800    epoch_cost = 1.0035099\n",
      "epoch = 12900    epoch_cost = 1.0030873\n",
      "epoch = 13000    epoch_cost = 1.0025276\n",
      "epoch = 13100    epoch_cost = 1.0020198\n",
      "epoch = 13200    epoch_cost = 1.0014821\n",
      "epoch = 13300    epoch_cost = 1.0009388\n",
      "epoch = 13400    epoch_cost = 1.0004302\n",
      "epoch = 13500    epoch_cost = 0.999919\n",
      "epoch = 13600    epoch_cost = 0.9993877\n",
      "epoch = 13700    epoch_cost = 0.99890536\n",
      "epoch = 13800    epoch_cost = 0.99843854\n",
      "epoch = 13900    epoch_cost = 0.99801046\n",
      "epoch = 14000    epoch_cost = 0.9976056\n",
      "epoch = 14100    epoch_cost = 0.9971868\n",
      "epoch = 14200    epoch_cost = 0.99665195\n",
      "epoch = 14300    epoch_cost = 0.99615455\n",
      "epoch = 14400    epoch_cost = 0.9956876\n",
      "epoch = 14500    epoch_cost = 0.99519736\n",
      "epoch = 14600    epoch_cost = 0.99471235\n",
      "epoch = 14700    epoch_cost = 0.99424684\n",
      "epoch = 14800    epoch_cost = 0.9938125\n",
      "epoch = 14900    epoch_cost = 0.99340576\n",
      "epoch = 15000    epoch_cost = 0.99300325\n",
      "epoch = 15100    epoch_cost = 0.9926258\n",
      "epoch = 15200    epoch_cost = 0.9922504\n",
      "epoch = 15300    epoch_cost = 0.99182546\n",
      "epoch = 15400    epoch_cost = 0.9914455\n",
      "epoch = 15500    epoch_cost = 0.99109685\n",
      "epoch = 15600    epoch_cost = 0.99075955\n",
      "epoch = 15700    epoch_cost = 0.99042374\n",
      "epoch = 15800    epoch_cost = 0.9900237\n",
      "epoch = 15900    epoch_cost = 0.98955446\n",
      "epoch = 16000    epoch_cost = 0.98917526\n",
      "epoch = 16100    epoch_cost = 0.98878294\n",
      "epoch = 16200    epoch_cost = 0.98830587\n",
      "epoch = 16300    epoch_cost = 0.9877267\n",
      "epoch = 16400    epoch_cost = 0.98706514\n",
      "epoch = 16500    epoch_cost = 0.98613036\n",
      "epoch = 16600    epoch_cost = 0.98492074\n",
      "epoch = 16700    epoch_cost = 0.98392767\n",
      "epoch = 16800    epoch_cost = 0.9829872\n",
      "epoch = 16900    epoch_cost = 0.98209363\n",
      "epoch = 17000    epoch_cost = 0.9811894\n",
      "epoch = 17100    epoch_cost = 0.9803462\n",
      "epoch = 17200    epoch_cost = 0.9794851\n",
      "epoch = 17300    epoch_cost = 0.978592\n",
      "epoch = 17400    epoch_cost = 0.9774581\n",
      "epoch = 17500    epoch_cost = 0.97625095\n",
      "epoch = 17600    epoch_cost = 0.97532266\n",
      "epoch = 17700    epoch_cost = 0.97439665\n",
      "epoch = 17800    epoch_cost = 0.9735109\n",
      "epoch = 17900    epoch_cost = 0.97265625\n",
      "epoch = 18000    epoch_cost = 0.971841\n",
      "epoch = 18100    epoch_cost = 0.97104555\n",
      "epoch = 18200    epoch_cost = 0.97024727\n",
      "epoch = 18300    epoch_cost = 0.96951354\n",
      "epoch = 18400    epoch_cost = 0.96885526\n",
      "epoch = 18500    epoch_cost = 0.96822643\n",
      "epoch = 18600    epoch_cost = 0.9676159\n",
      "epoch = 18700    epoch_cost = 0.9669919\n",
      "epoch = 18800    epoch_cost = 0.966389\n",
      "epoch = 18900    epoch_cost = 0.96576184\n",
      "epoch = 19000    epoch_cost = 0.9651685\n",
      "epoch = 19100    epoch_cost = 0.9646475\n",
      "epoch = 19200    epoch_cost = 0.96414965\n",
      "epoch = 19300    epoch_cost = 0.9636804\n",
      "epoch = 19400    epoch_cost = 0.9632376\n",
      "epoch = 19500    epoch_cost = 0.962807\n",
      "epoch = 19600    epoch_cost = 0.96238893\n",
      "epoch = 19700    epoch_cost = 0.96198106\n",
      "epoch = 19800    epoch_cost = 0.9615806\n",
      "epoch = 19900    epoch_cost = 0.96119004\n",
      "epoch = 20000    epoch_cost = 0.96082085\n",
      "epoch = 20100    epoch_cost = 0.960467\n",
      "epoch = 20200    epoch_cost = 0.9601263\n",
      "epoch = 20300    epoch_cost = 0.9598014\n",
      "epoch = 20400    epoch_cost = 0.95946324\n",
      "epoch = 20500    epoch_cost = 0.95915693\n",
      "epoch = 20600    epoch_cost = 0.9588599\n",
      "epoch = 20700    epoch_cost = 0.9585765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 20800    epoch_cost = 0.9582996\n",
      "epoch = 20900    epoch_cost = 0.9580291\n",
      "epoch = 21000    epoch_cost = 0.95776737\n",
      "epoch = 21100    epoch_cost = 0.9575108\n",
      "epoch = 21200    epoch_cost = 0.95725226\n",
      "epoch = 21300    epoch_cost = 0.9569936\n",
      "epoch = 21400    epoch_cost = 0.9567168\n",
      "epoch = 21500    epoch_cost = 0.956476\n",
      "epoch = 21600    epoch_cost = 0.9562499\n",
      "epoch = 21700    epoch_cost = 0.9560342\n",
      "epoch = 21800    epoch_cost = 0.9558096\n",
      "epoch = 21900    epoch_cost = 0.9555802\n",
      "epoch = 22000    epoch_cost = 0.9553289\n",
      "epoch = 22100    epoch_cost = 0.9550574\n",
      "epoch = 22200    epoch_cost = 0.95482093\n",
      "epoch = 22300    epoch_cost = 0.95461196\n",
      "epoch = 22400    epoch_cost = 0.95440614\n",
      "epoch = 22500    epoch_cost = 0.9542229\n",
      "epoch = 22600    epoch_cost = 0.9540552\n",
      "epoch = 22700    epoch_cost = 0.9538983\n",
      "epoch = 22800    epoch_cost = 0.95374936\n",
      "epoch = 22900    epoch_cost = 0.953604\n",
      "epoch = 23000    epoch_cost = 0.9534665\n",
      "epoch = 23100    epoch_cost = 0.9533367\n",
      "epoch = 23200    epoch_cost = 0.95321494\n",
      "epoch = 23300    epoch_cost = 0.95310014\n",
      "epoch = 23400    epoch_cost = 0.952985\n",
      "epoch = 23500    epoch_cost = 0.9528771\n",
      "epoch = 23600    epoch_cost = 0.95277625\n",
      "epoch = 23700    epoch_cost = 0.9526782\n",
      "epoch = 23800    epoch_cost = 0.95256907\n",
      "epoch = 23900    epoch_cost = 0.9524714\n",
      "epoch = 24000    epoch_cost = 0.95238876\n",
      "epoch = 24100    epoch_cost = 0.95231223\n",
      "epoch = 24200    epoch_cost = 0.952238\n",
      "epoch = 24300    epoch_cost = 0.9521578\n",
      "epoch = 24400    epoch_cost = 0.95207393\n",
      "epoch = 24500    epoch_cost = 0.9520016\n",
      "epoch = 24600    epoch_cost = 0.95194453\n",
      "epoch = 24700    epoch_cost = 0.9518931\n",
      "epoch = 24800    epoch_cost = 0.9518461\n",
      "epoch = 24900    epoch_cost = 0.95180255\n",
      "epoch = 25000    epoch_cost = 0.9517606\n",
      "epoch = 25100    epoch_cost = 0.95171916\n",
      "epoch = 25200    epoch_cost = 0.95168287\n",
      "epoch = 25300    epoch_cost = 0.9516494\n",
      "epoch = 25400    epoch_cost = 0.9516187\n",
      "epoch = 25500    epoch_cost = 0.9515908\n",
      "epoch = 25600    epoch_cost = 0.95156574\n",
      "epoch = 25700    epoch_cost = 0.95154303\n",
      "epoch = 25800    epoch_cost = 0.95152235\n",
      "epoch = 25900    epoch_cost = 0.9515038\n",
      "epoch = 26000    epoch_cost = 0.9514854\n",
      "epoch = 26100    epoch_cost = 0.95146513\n",
      "epoch = 26200    epoch_cost = 0.9514504\n",
      "epoch = 26300    epoch_cost = 0.9514377\n",
      "epoch = 26400    epoch_cost = 0.95142627\n",
      "epoch = 26500    epoch_cost = 0.95141566\n",
      "epoch = 26600    epoch_cost = 0.9514062\n",
      "epoch = 26700    epoch_cost = 0.95139694\n",
      "epoch = 26800    epoch_cost = 0.95138896\n",
      "epoch = 26900    epoch_cost = 0.95138156\n",
      "epoch = 27000    epoch_cost = 0.95137435\n",
      "epoch = 27100    epoch_cost = 0.95136833\n",
      "epoch = 27200    epoch_cost = 0.95136136\n",
      "epoch = 27300    epoch_cost = 0.9513563\n",
      "epoch = 27400    epoch_cost = 0.9513517\n",
      "epoch = 27500    epoch_cost = 0.95134693\n",
      "epoch = 27600    epoch_cost = 0.9513419\n",
      "epoch = 27700    epoch_cost = 0.9513374\n",
      "epoch = 27800    epoch_cost = 0.9513328\n",
      "epoch = 27900    epoch_cost = 0.9513277\n",
      "epoch = 28000    epoch_cost = 0.95132345\n",
      "epoch = 28100    epoch_cost = 0.9513188\n",
      "epoch = 28200    epoch_cost = 0.9513139\n",
      "epoch = 28300    epoch_cost = 0.95131\n",
      "epoch = 28400    epoch_cost = 0.95130694\n",
      "epoch = 28500    epoch_cost = 0.9513042\n",
      "epoch = 28600    epoch_cost = 0.95130175\n",
      "epoch = 28700    epoch_cost = 0.95129955\n",
      "epoch = 28800    epoch_cost = 0.9512972\n",
      "epoch = 28900    epoch_cost = 0.9512952\n",
      "epoch = 29000    epoch_cost = 0.95129305\n",
      "epoch = 29100    epoch_cost = 0.9512911\n",
      "epoch = 29200    epoch_cost = 0.9512892\n",
      "epoch = 29300    epoch_cost = 0.95128745\n",
      "epoch = 29400    epoch_cost = 0.9512854\n",
      "epoch = 29500    epoch_cost = 0.9512837\n",
      "epoch = 29600    epoch_cost = 0.9512818\n",
      "epoch = 29700    epoch_cost = 0.95128024\n",
      "epoch = 29800    epoch_cost = 0.9512785\n",
      "epoch = 29900    epoch_cost = 0.95127684\n",
      "epoch = 30000    epoch_cost = 0.951275\n",
      "epoch = 30100    epoch_cost = 0.95127344\n",
      "epoch = 30200    epoch_cost = 0.95127183\n",
      "epoch = 30300    epoch_cost = 0.95127016\n",
      "epoch = 30400    epoch_cost = 0.95126855\n",
      "epoch = 30500    epoch_cost = 0.95126706\n",
      "epoch = 30600    epoch_cost = 0.95126545\n",
      "epoch = 30700    epoch_cost = 0.9512638\n",
      "epoch = 30800    epoch_cost = 0.9512623\n",
      "epoch = 30900    epoch_cost = 0.95126086\n",
      "epoch = 31000    epoch_cost = 0.9512592\n",
      "epoch = 31100    epoch_cost = 0.95125765\n",
      "epoch = 31200    epoch_cost = 0.95125633\n",
      "epoch = 31300    epoch_cost = 0.95125496\n",
      "epoch = 31400    epoch_cost = 0.95125324\n",
      "epoch = 31500    epoch_cost = 0.95125186\n",
      "epoch = 31600    epoch_cost = 0.9512504\n",
      "epoch = 31700    epoch_cost = 0.951249\n",
      "epoch = 31800    epoch_cost = 0.9512475\n",
      "epoch = 31900    epoch_cost = 0.9512461\n",
      "epoch = 32000    epoch_cost = 0.95124483\n",
      "epoch = 32100    epoch_cost = 0.9512433\n",
      "epoch = 32200    epoch_cost = 0.95124185\n",
      "epoch = 32300    epoch_cost = 0.9512406\n",
      "epoch = 32400    epoch_cost = 0.9512391\n",
      "epoch = 32500    epoch_cost = 0.9512378\n",
      "epoch = 32600    epoch_cost = 0.9512364\n",
      "epoch = 32700    epoch_cost = 0.95123506\n",
      "epoch = 32800    epoch_cost = 0.95123374\n",
      "epoch = 32900    epoch_cost = 0.95123243\n",
      "epoch = 33000    epoch_cost = 0.9512311\n",
      "epoch = 33100    epoch_cost = 0.9512297\n",
      "epoch = 33200    epoch_cost = 0.9512286\n",
      "epoch = 33300    epoch_cost = 0.95122725\n",
      "epoch = 33400    epoch_cost = 0.95122594\n",
      "epoch = 33500    epoch_cost = 0.95122457\n",
      "epoch = 33600    epoch_cost = 0.9512235\n",
      "epoch = 33700    epoch_cost = 0.9512223\n",
      "epoch = 33800    epoch_cost = 0.9512209\n",
      "epoch = 33900    epoch_cost = 0.9512197\n",
      "epoch = 34000    epoch_cost = 0.9512186\n",
      "epoch = 34100    epoch_cost = 0.95121735\n",
      "epoch = 34200    epoch_cost = 0.951216\n",
      "epoch = 34300    epoch_cost = 0.9512151\n",
      "epoch = 34400    epoch_cost = 0.9512138\n",
      "epoch = 34500    epoch_cost = 0.9512126\n",
      "epoch = 34600    epoch_cost = 0.9512112\n",
      "epoch = 34700    epoch_cost = 0.9512101\n",
      "epoch = 34800    epoch_cost = 0.95120907\n",
      "epoch = 34900    epoch_cost = 0.9512077\n",
      "epoch = 35000    epoch_cost = 0.9512067\n",
      "epoch = 35100    epoch_cost = 0.9512054\n",
      "epoch = 35200    epoch_cost = 0.9512043\n",
      "epoch = 35300    epoch_cost = 0.95120305\n",
      "epoch = 35400    epoch_cost = 0.95120203\n",
      "epoch = 35500    epoch_cost = 0.9512009\n",
      "epoch = 35600    epoch_cost = 0.95119977\n",
      "epoch = 35700    epoch_cost = 0.95119864\n",
      "epoch = 35800    epoch_cost = 0.95119756\n",
      "epoch = 35900    epoch_cost = 0.9511964\n",
      "epoch = 36000    epoch_cost = 0.9511951\n",
      "epoch = 36100    epoch_cost = 0.9511942\n",
      "epoch = 36200    epoch_cost = 0.9511932\n",
      "epoch = 36300    epoch_cost = 0.95119214\n",
      "epoch = 36400    epoch_cost = 0.951191\n",
      "epoch = 36500    epoch_cost = 0.95118994\n",
      "epoch = 36600    epoch_cost = 0.95118886\n",
      "epoch = 36700    epoch_cost = 0.9511877\n",
      "epoch = 36800    epoch_cost = 0.9511866\n",
      "epoch = 36900    epoch_cost = 0.95118564\n",
      "epoch = 37000    epoch_cost = 0.9511846\n",
      "epoch = 37100    epoch_cost = 0.95118374\n",
      "epoch = 37200    epoch_cost = 0.95118266\n",
      "epoch = 37300    epoch_cost = 0.9511817\n",
      "epoch = 37400    epoch_cost = 0.95118064\n",
      "epoch = 37500    epoch_cost = 0.9511796\n",
      "epoch = 37600    epoch_cost = 0.95117867\n",
      "epoch = 37700    epoch_cost = 0.9511779\n",
      "epoch = 37800    epoch_cost = 0.95117676\n",
      "epoch = 37900    epoch_cost = 0.95117605\n",
      "epoch = 38000    epoch_cost = 0.9511749\n",
      "epoch = 38100    epoch_cost = 0.9511742\n",
      "epoch = 38200    epoch_cost = 0.9511732\n",
      "epoch = 38300    epoch_cost = 0.9511722\n",
      "epoch = 38400    epoch_cost = 0.9511713\n",
      "epoch = 38500    epoch_cost = 0.95117027\n",
      "epoch = 38600    epoch_cost = 0.9511693\n",
      "epoch = 38700    epoch_cost = 0.9511685\n",
      "epoch = 38800    epoch_cost = 0.95116776\n",
      "epoch = 38900    epoch_cost = 0.9511666\n",
      "epoch = 39000    epoch_cost = 0.95116574\n",
      "epoch = 39100    epoch_cost = 0.9511649\n",
      "epoch = 39200    epoch_cost = 0.95116407\n",
      "epoch = 39300    epoch_cost = 0.9511631\n",
      "epoch = 39400    epoch_cost = 0.95116234\n",
      "epoch = 39500    epoch_cost = 0.9511614\n",
      "epoch = 39600    epoch_cost = 0.95116055\n",
      "epoch = 39700    epoch_cost = 0.9511597\n",
      "epoch = 39800    epoch_cost = 0.9511587\n",
      "epoch = 39900    epoch_cost = 0.9511579\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save the parameter to session。\n",
      "train accuracy : 0.7200116\n",
      "test accuracy : 0.7190985\n"
     ]
    }
   ],
   "source": [
    "parameters=model(X_train,y_train,X_test,y_test,\n",
    "        learning_rate=0.0001,num_epochs=40000,print_cost=True,is_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "\n",
    "    W1 = tf.convert_to_tensor(parameters[\"W1\"])\n",
    "    b1 = tf.convert_to_tensor(parameters[\"b1\"])\n",
    "    W2 = tf.convert_to_tensor(parameters[\"W2\"])\n",
    "    b2 = tf.convert_to_tensor(parameters[\"b2\"])\n",
    "\n",
    "    params = {\"W1\": W1,\n",
    "              \"b1\": b1,\n",
    "              \"W2\": W2,\n",
    "              \"b2\": b2,\n",
    "             }\n",
    "\n",
    "    x=tf.placeholder(tf.float32,[5,None],name=\"x\")\n",
    "\n",
    "    z2 = forward_propagation_for_predict(x, params)\n",
    "    p = tf.round(tf.nn.sigmoid(z2))\n",
    "\n",
    "    sess = tf.Session()\n",
    "    prediction = sess.run(p, feed_dict = {x: X})\n",
    "\n",
    "    return prediction\n",
    "\n",
    "def forward_propagation_for_predict(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "\n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\",\n",
    "                  the shapes are given in initialize_parameters\n",
    "    Returns:\n",
    "    Z2 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "                                                           # Numpy Equivalents:\n",
    "    Z1 = tf.add(tf.matmul(W1, X), b1)                      # Z1 = np.dot(W1, X) + b1\n",
    "    A1 = tf.nn.relu(Z1)                                    # A1 = relu(Z1)\n",
    "    Z2 = tf.add(tf.matmul(W2, A1), b2)                     # Z2 = np.dot(W2, a1) + b2\n",
    "  \n",
    "    return Z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=pd.DataFrame()\n",
    "for i in range(10):\n",
    "    f1=pd.read_csv('/home/yazi/Documents/stage/data/car_afc_train/train'+str(i)+'_first.csv',delimiter=';',encoding='iso 8859-1',dtype=str)\n",
    "    f2=pd.read_csv('/home/yazi/Documents/stage/data/car_afc_train/train'+str(i)+'_last.csv',delimiter=';',encoding='iso 8859-1',dtype=str)\n",
    "    file=pd.concat([f1,f2,file],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Navigo=np.array(file.astype(float))\n",
    "scaler = StandardScaler()\n",
    "X_Navigo = scaler.fit_transform(X_Navigo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Navigo=np.transpose(X_Navigo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 0.64622444, -0.66553223, -1.3364879 ,  0.45355412, -0.4978083 ],\n",
       "        [ 0.2066185 , -0.29722834, -0.31979403, -0.2655683 ,  0.2706872 ],\n",
       "        [-0.7666516 ,  0.64844173,  0.26405895, -0.06759205, -2.245466  ],\n",
       "        [-0.7160548 ,  0.7518304 ,  1.1616038 , -0.41737574,  0.0303058 ]],\n",
       "       dtype=float32), 'b1': array([[-0.51512533],\n",
       "        [ 1.3055431 ],\n",
       "        [ 0.92412204],\n",
       "        [ 0.40440565]], dtype=float32), 'W2': array([[ 1.6687042 ,  1.1493064 , -1.4108942 ,  0.56589407]],\n",
       "       dtype=float32), 'b2': array([[-1.2314926]], dtype=float32)}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=predict(X_Navigo, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116324.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 345710)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33647855138700067"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "116324/345710"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01293859014781175"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4473/345710"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "def show_result(y_test,y_pred):\n",
    "    y_test=np.squeeze(y_test)\n",
    "    y_pred=np.squeeze(y_pred)\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12058  4282]\n",
      " [  217   658]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.74      0.84     16340\n",
      "         1.0       0.13      0.75      0.23       875\n",
      "\n",
      "    accuracy                           0.74     17215\n",
      "   macro avg       0.56      0.74      0.53     17215\n",
      "weighted avg       0.94      0.74      0.81     17215\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result=predict(X_train, parameters)\n",
    "show_result(y_train,result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3012 1059]\n",
      " [  62  171]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.74      0.84      4071\n",
      "         1.0       0.14      0.73      0.23       233\n",
      "\n",
      "    accuracy                           0.74      4304\n",
      "   macro avg       0.56      0.74      0.54      4304\n",
      "weighted avg       0.93      0.74      0.81      4304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result=predict(X_test, parameters)\n",
    "show_result(y_test,result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
